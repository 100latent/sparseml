{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Transfer Learning with Adam Optimizer\n",
    "\n",
    "Neural Networks can take a long time to train. Additionally, techniques like [model pruning](https://towardsdatascience.com/pruning-deep-neural-network-56cae1ec5505) and other optimizations sometimes take many trials and errors due to a large number of hyperparameters. However, it can often be necessary to do those model optimizations to achieve both your performance and optimizing goals. Luckily, though, pruned (sparsified) Neural Networks in the computer vision and natural language space [transfer learn](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a) very well. \n",
    "\n",
    "To make it easier to use pruned models, Neural Magic's ML team is actively creating pruned versions of popular models and datasets and updating the repo with them. Also, these models are tested thoroughly with the [Neural Magic Inference Engine](https://neuralmagic.com/) to ensure performance.\n",
    "\n",
    "This notebook provides an easy step by step walkthrough for downloading a recalibrated model from the Neural Magic Model Repo and using it for transfer learning. Below we will go through the following steps:\n",
    "1. Environment Setup\n",
    "2. Model Selection\n",
    "3. Model and Dataset Setup\n",
    "4. Transfer Learning\n",
    "5. Export to ONNX\n",
    "\n",
    "Reading through this notebook will be reasonably quick to gain an intuition for what is happening. Rough time estimates for transfer learning are given, note since we are training with the Pytorch CPU implementation it will be much slower than a GPU:\n",
    "- 15 minutes on a GPU\n",
    "- 3 hours on a laptop CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Below we try to add the project folder to the PYTHONPATH environment variable for our execution. If this does not work, we will need to install neuralmagicML into the system using `pip install ./` when you are located at the root of the folder.\n",
    "\n",
    "Additionally, please be sure to install from the requirements.txt file located at the root before running: `pip install -r ./requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_name = \"transfer_learning_adam_pytorch\"\n",
    "\n",
    "# environment setup for ease of use (puts neuralmagicML into the python package path)\n",
    "if \"WORKBOOK_DIR\" not in globals():\n",
    "    WORKBOOK_DIR = os.getcwd()\n",
    "\n",
    "package_path = os.path.abspath(\n",
    "    os.path.join(os.path.expanduser(WORKBOOK_DIR), os.pardir)\n",
    ")\n",
    "sys.path.extend([package_path])\n",
    "\n",
    "print(\"added {} to PYTHONPATH\".format(package_path))\n",
    "print(\"working out of {}\".format(WORKBOOK_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "There can be a lot of models available in repositories, so a simple UI is provided to make this selection process easier. Within the UI, filters can be applied for models trained in/on specific domains or datasets. Each network architecture listed out will also include options for the dataset it was trained on and the type. The type refers to how the models were trained and/or recalibrated. Specifically:\n",
    "- base - baseline model, trained generally as in the original paper\n",
    "- recal - a recalibrated model, it is recalibrated to the point of fully recovering the baseline model's metrics\n",
    "- recal-perf - a recalibrated model, it is recalibrated for performance to the point of recovering 99% of the baseline model's metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.nbutils import ModelSelectWidgetContainer\n",
    "\n",
    "print(\"creating ui...\")\n",
    "container = ModelSelectWidgetContainer(\"pytorch\")\n",
    "display(container.create())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Dataset Setup\n",
    "\n",
    "By default, we use the [Imagewoof](https://github.com/fastai/imagenette) dataset to transfer learn to (a dataset consisting of 10 classes of dogs). This dataset is used to show how to transfer learn on a simple dataset quickly. If you would like to try out transfer learning on your own dataset, then replace the appropriate lines with your own:\n",
    "- `num_classes = 10`\n",
    "- `class_type = \"single\"`\n",
    "- `train_dataset = ImagewoofDataset(dataset_root, train=True)`\n",
    "- `val_dataset = ImagewoofDataset(dataset_root, train=False)`\n",
    "\n",
    "More information for creating and working with Pytorch datasets can be found [here](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html). Take care to keep the variable names the same, as the rest of the notebook is set up according to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from neuralmagicML.pytorch.datasets import ImagewoofDataset\n",
    "from neuralmagicML.pytorch.models import ModelRegistry\n",
    "from neuralmagicML.utils import clean_path\n",
    "\n",
    "#######################################################\n",
    "# Define the number of classes to transfer learn to below\n",
    "#######################################################\n",
    "num_classes = 10\n",
    "class_type = \"single\"  # use single for softmax output, multi for sigmoid output\n",
    "print(\n",
    "    \"transfer learning to {} classes and class_type {}\".format(num_classes, class_type)\n",
    ")\n",
    "\n",
    "repo_model = container.selected_model\n",
    "print(\"\\nloading model {} ...\".format(repo_model.root_path))\n",
    "model = ModelRegistry.create(\n",
    "    repo_model.registry_key,\n",
    "    pretrained=repo_model.desc,\n",
    "    num_classes=num_classes,\n",
    "    class_type=class_type,\n",
    ")\n",
    "model_name = model.__class__.__name__\n",
    "print(model)\n",
    "\n",
    "#######################################################\n",
    "# Define your train and validation datasets below\n",
    "#######################################################\n",
    "dataset_root = clean_path(os.path.join(\".\", notebook_name, \"datasets\"))\n",
    "\n",
    "print(\"\\nloading train dataset...\")\n",
    "train_dataset = ImagewoofDataset(dataset_root, train=True)\n",
    "print(train_dataset)\n",
    "\n",
    "print(\"\\nloading val dataset...\")\n",
    "val_dataset = ImagewoofDataset(dataset_root, train=False)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Now that the model and datasets are chosen and set up, we will begin transfer learning from the given model onto the dataset. The library to enable this is designed to be easily plugged into nearly any training setup for Pytorch. Below we provide an example of how an integration looks. Note, only a handful of these lines are needed to be able to integrate fully.\n",
    "\n",
    "1. Create a [`ConstantKSModifier`](); handles keeping the sparsity the same for any sparsified layers\n",
    "2. Create a [`ScheduledModifierManager`](); used in combination with the `ConstantKSModifier` and the `ScheduledOptimizer`\n",
    "3. Create a [`ScheduledOptimizer`](); handles updating the Pytorch objects that modify the training process. It wraps the original optimizer that was used to modify the training process/graph, and should be used in place of that. IE, optimizer.step() must be called on `ScheduledOptimizer` and not the original.\n",
    "4. Call into the `ScheduledOptimizer` for `epoch_start()` and `epoch_end()` while training. These calls mark when an epoch has started and after training for an epoch has ended, respectively.\n",
    "\n",
    "Once the training objects are created (optimizer, loss function, etc.), a `ConstantKSModifier`, `ScheduledModifierManager`, and `ScheduledOptimizer` are instantiated. Almost all logging and updates are done through `Tensorboard` for this notebook. The use of `Tensorboard` is optional, and other loggers (as well as not using a logger) are available. Finally, regular training and testing code is used to go through the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import auto\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from neuralmagicML.pytorch.utils import (\n",
    "    CrossEntropyLossWrapper,\n",
    "    TopKAccuracy,\n",
    "    ModuleTrainer,\n",
    "    ModuleTester,\n",
    "    TensorboardLogger,\n",
    ")\n",
    "from neuralmagicML.utils import create_unique_dir, clean_path\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# Necessary imports for running transfer learning\n",
    "#######################################################\n",
    "from neuralmagicML.pytorch.recal import (\n",
    "    ScheduledModifierManager,\n",
    "    ScheduledOptimizer,\n",
    "    ConstantKSModifier,\n",
    ")\n",
    "\n",
    "\n",
    "# setup device, data loaders, loss, optimizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 128\n",
    "train_data_loader = DataLoader(\n",
    "    train_dataset, batch_size, shuffle=True, pin_memory=True, num_workers=8\n",
    ")\n",
    "val_data_loader = DataLoader(\n",
    "    val_dataset, batch_size, shuffle=False, pin_memory=True, num_workers=8\n",
    ")\n",
    "loss = CrossEntropyLossWrapper(extras={\"top1acc\": TopKAccuracy(1)})\n",
    "optim = Adam(model.parameters())\n",
    "print(\"device:{} batch_size:{} loss:{}\".format(device, batch_size, loss))\n",
    "\n",
    "tensorboard_model_path = create_unique_dir(\n",
    "    os.path.join(\".\", \"tensorboard-logs\", notebook_name, model_name)\n",
    ")\n",
    "loggers = [TensorboardLogger(tensorboard_model_path)]\n",
    "print(\"logging at {}\".format(tensorboard_model_path))\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# First lines that must be substituted in training code\n",
    "# We create a ConstantKSModifier to hold the sparsity constant on all layers\n",
    "# Additionally we create a modifier manager as well as a scheduled optimizer\n",
    "# These will allow us to transfer learn with sparsity\n",
    "# The loggers can be left out if desired\n",
    "#######################################################\n",
    "manager = ScheduledModifierManager([ConstantKSModifier(layers=\"__ALL__\")])\n",
    "optim = ScheduledOptimizer(\n",
    "    optim,\n",
    "    model,\n",
    "    manager,\n",
    "    steps_per_epoch=math.ceil(len(train_dataset) / batch_size),\n",
    "    loggers=loggers,\n",
    ")\n",
    "print(\"created modifier, manager, and optimizer\")\n",
    "\n",
    "\n",
    "# we use prewritten trainers and testers to make the code more concise\n",
    "trainer = ModuleTrainer(model, device, loss, optim, loggers=loggers)\n",
    "tester = ModuleTester(model, device, loss, loggers=loggers)\n",
    "model = model.to(device)\n",
    "\n",
    "# startup tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tensorboard-logs\n",
    "\n",
    "# run initial validation for comparison\n",
    "tester.run_epoch(val_data_loader, epoch=-1, show_progress=False)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# Final lines that must be substituted in training code\n",
    "# We tell the ScheduledOptimizer when each epoch has started and ended\n",
    "#######################################################\n",
    "num_epochs = 20\n",
    "for epoch in auto.tqdm(range(num_epochs), desc=\"transfer learning\"):\n",
    "    optim.epoch_start()\n",
    "\n",
    "    trainer.run_epoch(train_data_loader, epoch, show_progress=False)\n",
    "    tester.run_epoch(val_data_loader, epoch, show_progress=False)\n",
    "\n",
    "    optim.epoch_end()\n",
    "\n",
    "# delete so all modifiers are cleaned up before exporting\n",
    "del optim\n",
    "print(\"training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX\n",
    "\n",
    "Now that the model is fully recalibrated, we need to export it to an ONNX format. The ONNX format is what is used by the Neural Magic Inference Engine. For Pytorch, exporting to ONNX is natively supported. Below we use a convenience class to handle exporting, the [`ModuleExporter`](). Once the model has saved as an ONNX file, it is ready to be used for inference with Neural Magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.utils import clean_path\n",
    "from neuralmagicML.pytorch.utils import ModuleExporter\n",
    "\n",
    "export_path = clean_path(os.path.join(\".\", notebook_name, model_name))\n",
    "exporter = ModuleExporter(model, export_path)\n",
    "for batch in val_data_loader:\n",
    "    sample_input = batch[0]\n",
    "    break\n",
    "exporter.export_onnx(sample_input)\n",
    "print(\"exported onnx to {}\".format(export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
