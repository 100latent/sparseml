{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Sparsity via Pruning Tutorial\n",
    "\n",
    "Neural networks tend to be overparameterized for given tasks (i.e. the number of parameters far exceeds the number of training points) yet still they [generalize very well](https://arxiv.org/abs/1611.03530). This flies against conventional wisdom where overparamatarizing a model will lead to overfitting and putting theory behind this empirical evidence is a very active area of research.\n",
    "\n",
    "Additionally, [early on](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf) it was discovered that large numbers of weights in neural networks could be pruned away (set to 0) without affecting the loss and in most cases actually improving the generalization capability of the network. This work was reinvigorated with Song Han's [2015 paper](https://arxiv.org/abs/1510.00149) in pursuit of compressing model size for mobile applications. This has resulted in numerous papers coming out on the topic of weight pruning, filter pruning, channel pruning, and ultimately block pruning. [This paper](https://arxiv.org/abs/1902.09574) out of Google gives a good overview of the current state of sparsity.\n",
    "\n",
    "Given that models are very overparamatarized and large numbers of weights can be effectively pruned away, what does this leave us with? Well intuitively, then, we can think of pruning as performing an [architecture search](https://openreview.net/pdf?id=rJlnB3C5Ym) within this large, traditionally fixed weight space. What was originally important in the dense model was representing a large number of pathways for optimization. We can then effectively remove the unused pathways in the optimization space with a fine toothed comb (post training).\n",
    "\n",
    "Well what does pruning get us? We now have a model with a lot of multiplications by zero that we don't need to run. If we're smart about how we do structure this compute (a surprisingly tricky problem), we can now run the model much faster than ever thought possible! That's where the [Neural Magic](http://neuralmagic.com/) engine can help us.\n",
    "\n",
    "This tutorial provides a step by step walk through for pruning an already trained (dense) model. Specifically it is set up to work with the model trained in our [model training tutorial](model_training.ipynb), but it can be changed to support other models/datasets as needed:\n",
    "1. Dataset selection\n",
    "2. Model selection and loading\n",
    "3. Pruning setup\n",
    "4. Recalibration using pruning\n",
    "\n",
    "Reading through this notebook will be fairly quick to gain an intuition for what is happening. Rough time estimates for fully pruning the default model is given, note since we are training with the Pytorch CPU implementation it is much slower than GPU:\n",
    "- 30 minutes on a GPU (30 sec/epoch)\n",
    "- 60 hours on a laptop CPU (1 hour/epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print('Python %s on %s' % (sys.version, sys.platform))\n",
    "\n",
    "package_path = os.path.abspath(os.path.join(os.path.expanduser(os.getcwd()), os.pardir))\n",
    "print(package_path)\n",
    "\n",
    "\"\"\"\n",
    "Adding the path to the neuralmagic-pytorch extension to the path so it isn't necessary to have it installed\n",
    "\"\"\"\n",
    "sys.path.extend([package_path])\n",
    "\n",
    "print('Added current package path to sys.path')\n",
    "print('Be sure to install from requirements.txt and pytorch separately')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "\n",
    "We are using fast.ai's [Imagenette dataset](https://github.com/fastai/imagenette) provided under the [Apache License 2.0](https://github.com/fastai/imagenette/blob/master/LICENSE) as the default dataset. The original authors, much like ourselves, were interested in a dataset that has similar properties to more complicated datasets such as the Imagenet dataset but one that would allow rapid iterations. It includes 10 of the easiest classes out of the Imagenet 1000 dataset: tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute. If you are interested in visualizing the properties in this dataset see our [model training tutorial](model_training.ipynb) which also gives a more in depth breakdown for what batch size to use and the dataset splits.\n",
    "\n",
    "The dataset can easily be changed to the desired dataset in the code given.\n",
    "\n",
    "Below we will need to fill in the dataset path, train batch size, and test batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import torch\n",
    "\n",
    "print('\\nEnter the local path where the dataset can be found')\n",
    "\n",
    "dataset_text = widgets.Text(value='', placeholder='Enter local path to dataset', description='Dataset Path')\n",
    "display(dataset_text)\n",
    "\n",
    "print('\\nChoose the batch size to run through the model during train and test runs')\n",
    "print('(be sure to press enter if/after inputting manually)')\n",
    "train_batch_size_slider = widgets.IntSlider(\n",
    "    value=256, min=1, max=256, step=1, description='Train Batch Size:'\n",
    ")\n",
    "display(train_batch_size_slider)\n",
    "test_batch_size_slider = widgets.IntSlider(\n",
    "    value=256 if torch.cuda.is_available() else 1, min=1, max=256, step=1, description='Test Batch Size:'\n",
    ")\n",
    "display(test_batch_size_slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.pytorch.datasets import ImagenetteDataset, EarlyStopDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset_root = os.path.abspath(os.path.expanduser(dataset_text.value.strip()))\n",
    "print('\\nLoading dataset from {}'.format(dataset_root))\n",
    "\n",
    "if not os.path.exists(dataset_root):\n",
    "    raise Exception('Folder must exist for dataset at {}'.format(dataset_root))\n",
    "    \n",
    "train_batch_size = train_batch_size_slider.value\n",
    "test_batch_size = test_batch_size_slider.value\n",
    "\n",
    "print('\\nUsing train batch size of {} and test batch size of {}\\n'\n",
    "      .format(train_batch_size, test_batch_size))\n",
    "    \n",
    "train_dataset = ImagenetteDataset(dataset_root, train=True, rand_trans=True)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4)\n",
    "print('train dataset created: \\n{}\\n'.format(train_dataset))\n",
    "\n",
    "val_dataset = ImagenetteDataset(dataset_root, train=False, rand_trans=False)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=train_batch_size, shuffle=False, num_workers=4)\n",
    "print('validation test dataset created: \\n{}\\n'.format(val_dataset))\n",
    "\n",
    "train_test_dataset = EarlyStopDataset(ImagenetteDataset(dataset_root, train=True, rand_trans=False),\n",
    "                                      early_stop=round(0.1 * len(train_dataset)))\n",
    "train_test_data_loader = DataLoader(train_test_dataset, batch_size=train_batch_size, shuffle=False, num_workers=4)\n",
    "print('train test dataset created: \\n{}\\n'.format(train_test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Loading\n",
    "\n",
    "For this exercise we'll create the standard [ResNet50 model](https://arxiv.org/abs/1512.03385) and in addition we will load the pretrained weights from our [model training tutorial](model_training.ipynb)\n",
    "\n",
    "If you changed the dataset in the above cell, then we'll need to update the number of classes to create the model appropriately as well as loading your own pretrained weights. Additionally the model can be changed out completely to work with your specific use case.\n",
    "\n",
    "Additionally, run the code block and select the device to run on before continuing. cpu runs in the pytorch cpu framework and cuda runs on an attached GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import datetime\n",
    "from neuralmagicML.pytorch.models import resnet50, load_model\n",
    "\n",
    "num_classes = 10\n",
    "model = resnet50(num_classes=num_classes, pretrained='imagenette/dense')\n",
    "model_id = '{}-{}'.format(model.__class__.__name__,\n",
    "                          datetime.datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "                              .replace('-', '.').replace(':', '.'))\n",
    "print('Created model {}'.format(model.__class__.__name__))\n",
    "\n",
    "print('\\nChoose the device to run on')\n",
    "device_choice = widgets.ToggleButtons(\n",
    "    options=['cuda', 'cpu'] if torch.cuda.is_available() else ['cpu'],\n",
    "    description='Device'\n",
    ")\n",
    "display(device_choice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Setup\n",
    "\n",
    "Informally, sparsity is the degree in which a tensor is comprised of zeros. More formally:\n",
    "\n",
    "let $N^i$ be the total number of elements in a (e.g. weight) tensor $W_i$ and let $N^i_z$ be the number of elements which are zero-valued within that tensor.\n",
    "\n",
    "The sparsity level associated with that tensor is then defined as $s_i \\triangleq \\dfrac{N^i_z}{N^i}$ \n",
    "\n",
    "Our goal now is to maximize the sparsity of each weight tensor within the model while preserving the accuracy of the densely trained version. The most robust and effective approach to this so far has been magnitude pruning. To frame the problem, let's say our goal is to go from an intial sparsity $s^i_{init}$ to a final sparsity $s^i_{final}$. We could start with changing out our usual $L_2$ weight regularization with $L_1$ [($L_2$ vs $L_1$)](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) in our cost function. Following through, we would find that we did, in fact, introduce a few zeros into our weights. We created a direct pathway within the optimization problem such that the model is incentivized to reduce the magnitude of unimportant weights to 0 (unimportant are defined as weights that do not significantly contribute to the loss function). This is hard to balance, though. For example, how do we determine the proper weighting between the loss function and the regularization such that we reach a desired sparsity without sacrificing accuracy? Also, empirically we find that the model will settle in local minima thus failing to further reduce weights to zero.\n",
    "\n",
    "We can take this general idea to more of an extreme, though. Based on the previous thought experiment, it's reasonable to make an assumption that weights near zero are likely not important to the final loss function as well. Taking this even further, we could say that the smallest values within a given weight tensor are the ones least likely to affect our loss function. This is, of course, assuming that we've trained long enough to reach a stable point where our neural networks cost function has minimized unimportant weights. Naturally, then, we could apply a schedule where we prune away a small number of smallest weights for every $M$ training steps. This is exactly what 'magnitude pruning' does. \n",
    "\n",
    "Below, we go through a UI to set up a magnitude pruning schedule for our model. While pruning, some layers within a model will be more sensitive than others. A general rule is that the initial input layer and the final output layer are the most sensitive as they are an absolute bottleneck to the information flow. Because of this, we offer up a UI capable of creating very intricate schedules to the point that each layer could be pruned with a different schedule to a different final sparsity. In general, though, we can get by with ignoring the initial layer, pruning the final layer minimally (this is done to boost test accuracy for the pruned model), and prune all other layers equally. At Neural Magic we are actively working on automating this process to find the best possible pruning schedules to maximize performance while minimizing accuracy loss. \n",
    "\n",
    "The default for the below model will disable pruning for the initial and final layers and prune all other layers to 90% sparsity over the course of 10 epochs. The options available are described below:\n",
    "- Prune Epochs: number of epochs to apply the pruning over\n",
    "- Add New Group: add a new pruning group to the UI, used to prune selected layers to different sparsity levels at different rates\n",
    "- Delete Current Group: remove the current group from the pruning schedule\n",
    "- Tabs: the pruning groups setup so far, click to switch between\n",
    "- Sparsity: a range slider where the left value defines the sparsity level to initially start pruning at and the right value represents the final sparsity level\n",
    "- Start Epoch: the epoch to start pruning the selected layers at the initial sparsity\n",
    "- End Epoch: the epoch to finish pruning the selected layers to the final sparsity\n",
    "- Update Freq: the update frequency, in epochs, at which to prune the layers; ie 1.0 will prune every epoch\n",
    "- Selectable Layers: a dropdown of the layers available in the model that can be pruned along with their FLOPS and param counts; select the desired layers to prune using the checkboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from neuralmagicML.pytorch.notebooks import KSModifierWidgets\n",
    "\n",
    "\n",
    "device = device_choice.value\n",
    "print('running on device {}'.format(device))\n",
    "\n",
    "print('\\ncreating kernel sparsity analyzer widgets (need to execute the model, so may take some time)...')\n",
    "ks_widget, ks_modifiers = KSModifierWidgets.interactive_module(model, device, inp_dim=(1, 3, 224, 224))\n",
    "\n",
    "# add first group for all layers and remove the input and final layers\n",
    "ks_widget.add_group(\n",
    "    init_start_sparsity=0.05, init_final_sparsity=0.85, init_enabled=True,\n",
    "    init_start_epoch=0.0, init_end_epoch=30.0, init_update_frequency=1.0\n",
    ")\n",
    "ks_modifiers[0].layers.pop(0)\n",
    "ks_modifiers[0].layers.pop()\n",
    "ks_widget.update_from_modifiers()\n",
    "\n",
    "# add second group with just the final layer to set a lower sparsity for it\n",
    "ks_widget.add_group(\n",
    "    init_start_sparsity=0.05, init_final_sparsity=0.6, init_enabled=True,\n",
    "    init_start_epoch=0.0, init_end_epoch=30.0, init_update_frequency=1.0\n",
    ")\n",
    "ks_modifiers[1].layers = ks_modifiers[1].layers[-1:]\n",
    "ks_widget.update_from_modifiers()\n",
    "\n",
    "display(ks_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Selection\n",
    "\n",
    "With our magnitude pruning schedule complete, we need to define the hyperparameters for training while pruning. The most important of these is the learning rate. Too high and we will diverge from our initial dense solution. Too low and we will have to train for too many epochs. Below we run a learning rate sensitivity analysis from the [cyclic LR paper](https://arxiv.org/abs/1506.01186).\n",
    "\n",
    "To run the sensitivity analysis we will begin training the model at a very small learning rate ($10^{-7}$) where the weight updates will be lost in floating point precision errors (ie we won't learn). After each batch we exponentially increase the learning rate until we reach a very high learning rate ($10^0$) where we are guaranteed to diverge from our trained model.\n",
    "\n",
    "A flat region will be apparent in the graph starting from the left to some point at the right for a fully trained model. After this we reach a critical learning rate where the loss begins to rapidly rise. This is the critical point where we begin diverging from the local minimum in our optimization space. Using this information, we can find the optimal learning rate to use with an [SGD + nesterov momentum optimizer](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d) while pruning. Ideally we want to pick a learning rate that is a little before the divergent behavior. In this way we can guarantee fast convergence of the model after each pruning step. \n",
    "\n",
    "A good default for the learning rate is given for the model used in this notebook with training batch size of 256. Note that the learning rate is highly coupled with both the model architecture and the batch size. Increasing the batch size will correspond with an overall higher learning rate that can be used. With an increasing batch size, we are averaging over more samples so we are less likely to take a noisy step. With a decreasing batch size, more noise can creep in so we want to take more steps in general to be sure that we average out to the population mean. The reason for this trade off can be compared to [simmulated annealing](https://arxiv.org/abs/1711.00489). \n",
    "\n",
    "If changing either the default model or the batch size, you will need to update the learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from neuralmagicML.pytorch.utils import lr_analysis, lr_analysis_figure, CrossEntropyLossWrapper\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### optimizer definitions\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "###\n",
    "\n",
    "print('\\nrunning learning rate analysis...')\n",
    "batches_per_sample = round(500 / train_batch_size)  # make sure we have enough sample points per learning rate\n",
    "analysis = lr_analysis(model, device, train_data_loader, CrossEntropyLossWrapper(), batches_per_sample,\n",
    "                       init_lr=1e-7, final_lr=1e0, sgd_momentum=momentum, sgd_weight_decay=weight_decay)\n",
    "lr_analysis_figure(analysis)\n",
    "plt.show()\n",
    "\n",
    "print('\\nselect the learning rate')\n",
    "lr_slider = widgets.FloatLogSlider(\n",
    "    value=0.01, min=-7, max=1, step=0.01, description='Learning Rate:'\n",
    ")\n",
    "display(lr_slider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post Pruning Hyperparameters\n",
    "\n",
    "After pruning the model, we'll need to train for a few final epochs to recover any lost accuracy. A typical setup is to train with an [exponential decay learning rate](https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1) schedule starting from our previously found learning rate: $LR_i=LR_{init}*\\gamma^i$ where $LR_i$ is the learning rate used at epoch $i$. In doing this, we can find a local min with equivalent accuracy to the dense model provided we did not sparsify the model too much.\n",
    "\n",
    "Given this, the necessary hyperparameters to determine are:\n",
    "- Num Epochs - the number of epochs to train for in the finalizing stage\n",
    "- Final LR - the learning rate we will converge to while finalizing\n",
    "\n",
    "These parameters must be filled in below. We give defaults for the original model and dataset in this notebook. The defaults were found using previous intuition along with a few training runs. This model will recover the accuracy drop during pruning in about 30 epochs with the default pruning schedule (30 epochs for 85% all convs, no input layer, 60% fully connected). In general, training for longer after pruning as well as at a higher learning rate will help recover any lost accuracy provided the pruned model still has enough capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nselect the number of epochs to train for after pruning')\n",
    "finalize_epochs_text = widgets.IntText(value=30, description='Num Epochs')\n",
    "display(finalize_epochs_text)\n",
    "\n",
    "print('\\nselect the final learning rate')\n",
    "lr_final_slider = widgets.FloatLogSlider(\n",
    "    value=0.001, min=-7, max=1, step=0.01, description='Final LR:'\n",
    ")\n",
    "display(lr_final_slider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalibration Using Pruning\n",
    "\n",
    "With the parameters properly setup, we now create a schedule for applying the parameters. This is done using the Neural Magic ML library. Specifically we create a `ScheduledModifierManager` that controls a list of two different classes: `GradualKSModifier` which performs magnitude pruning to the weights in the given layers and `LearningRateModifier` which applies the exponential learning rate in the training epochs after pruning.\n",
    "\n",
    "We then create a `ScheduledOptimizer` giving it the model, an SGD optimizer, the `ScheduledModifierManager` we created, and the training datset size. Using all this information, the code can apply any schedule needed by capturing the `.step()` call, perform schedule updates, and then calling into the original optimizer. Additionally `epoch_start()` and `epoch_end()` should be called on the optimizer wrapper while training. We will see this use in the next code block. \n",
    "\n",
    "For the loss function we will use cross entropy as is standard for classification tasks. For additional metrics we will use the top 1 accuracy, ie did we predict the class correctly or not. We do this because there are only 10 classes available so top N accuracy is generally uninformative with so few classes. We use a custom wrapper class to organize the metrics and loss into one, callable class.\n",
    "\n",
    "Finally, beyond the usual basic screen-printouts let's use tensorboard's nice logging capabilities. We'll primarily track scalars such as the loss and accuracy throughout training in this example. We use tensorboardX to log from pytorch into tensorboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import optim\n",
    "from torch.nn import Conv2d, Linear\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "from neuralmagicML.pytorch.sparsity import (\n",
    "    LearningRateModifier, ScheduledModifierManager, ScheduledOptimizer, KSAnalyzerLayer\n",
    ")\n",
    "from neuralmagicML.pytorch.utils import TopKAccuracy, CrossEntropyLossWrapper\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "prune_epochs = max([modifier.end_epoch for modifier in ks_modifiers])\n",
    "finalizer_epochs = finalize_epochs_text.value\n",
    "lr_init = lr_slider.value\n",
    "lr_final = lr_final_slider.value\n",
    "lr_gamma = (lr_final / lr_init) ** (1 / (finalizer_epochs - 1))\n",
    "lr_modifier = LearningRateModifier(lr_class='ExponentialLR', lr_kwargs={'gamma': lr_gamma},\n",
    "                                   start_epoch=prune_epochs, end_epoch=prune_epochs + finalizer_epochs,\n",
    "                                   update_frequency=1)\n",
    "modifier_manager = ScheduledModifierManager([lr_modifier, *ks_modifiers])\n",
    "print('Created ScheduledModifierManager with exponential lr_modifier with gamma {} and {} ks modifiers'\n",
    "      .format(lr_gamma, len(ks_modifiers)))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr_slider.value, momentum=momentum,\n",
    "                      weight_decay=weight_decay, nesterov=True)\n",
    "optimizer = ScheduledOptimizer(optimizer, model, modifier_manager, steps_per_epoch=len(train_dataset))\n",
    "print('\\nCreated scheudled optimizer with initial lr: {}, momentum: {}, weight decay: {}'\n",
    "      .format(lr_slider.value, momentum, weight_decay))\n",
    "\n",
    "loss = CrossEntropyLossWrapper(extras={'top1acc': TopKAccuracy(1)})\n",
    "print('\\nCreated loss wrapper\\n{}'.format(loss))\n",
    "\n",
    "logs_dir = os.path.abspath(os.path.expanduser(os.path.join('.', 'model_training_logs', model_id)))\n",
    "\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "\n",
    "writer = SummaryWriter(logdir=logs_dir, comment='imagenette training')\n",
    "print('\\nCreated summary writer logging to \\n{}'.format(logs_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Pruning Schedule\n",
    "\n",
    "Below we go through a standard training and testing cycle in pytorch. \n",
    "\n",
    "for number of epochs required for pruning and finalizing:\n",
    "train model over full training dataset (one epoch); update weights\n",
    "test model over full validation dataset; no weight update\n",
    "test model over sampled training dataset; no weight update\n",
    "\n",
    "In the below code blocks, we create self-contained convenience functions for running the train and testing loops. These convenience functions are then called to train the model. At the end of the script we save the trained model in our current location with the date included as well as the final validation accuracy in the name.\n",
    "\n",
    "Note, we have additionally created a logs directory which, in combination with tensorboard, can be used to visualize the progress of the model. To launch tensorboard use the following command from within the notebooks directory: 'tensorboard --logdir model_training_logs --port 6006' Now you will have an interactive dashboard running on http://localhost:6006\n",
    "\n",
    "We additionally create analyzers for the kernel sparsity of all convs and kernel layers that is logged to tensorboard for visualization of the sparsity of each layer within the model. Also, we call `epoch_start()` and `epoch_end()` on the optimizer wrapper as mentioned above.\n",
    "\n",
    "Finally, we save the final result next to this notebook under the model id given earlier. Happy pruning!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from neuralmagicML.pytorch.models import save_model\n",
    "\n",
    "\n",
    "def test_epoch(model, data_loader, loss, device, epoch):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "            y_lab = y_lab.to(device)\n",
    "            x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "            batch_size = y_lab.shape[0]\n",
    "            y_pred = model(*x_feature)\n",
    "            losses = loss(x_feature, y_lab, y_pred)\n",
    "            \n",
    "            for key, val in losses.items():\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "                result = val.detach_().cpu()\n",
    "                result = result.repeat(batch_size)\n",
    "                results[key].append(result)\n",
    "                \n",
    "    return results\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, loss, device, epoch, writer):\n",
    "    model.train()\n",
    "    init_batch_size = None\n",
    "    batches_per_epoch = len(data_loader)\n",
    "    \n",
    "    for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "        y_lab = y_lab.to(device)\n",
    "        x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "        batch_size = y_lab.shape[0]\n",
    "        if init_batch_size is None:\n",
    "            init_batch_size = batch_size\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(*x_feature)\n",
    "        losses = loss(x_feature, y_lab, y_pred)\n",
    "        losses['loss'].backward()\n",
    "        optimizer.step(closure=None)\n",
    "        \n",
    "        step_count = init_batch_size * (epoch * batches_per_epoch + batch)\n",
    "        for _loss, _value in losses.items():\n",
    "            writer.add_scalar('Train/{}'.format(_loss), _value.item(), step_count)\n",
    "            writer.add_scalar('Train/Learning Rate', optimizer.learning_rate, step_count)\n",
    "            \n",
    "print('Recalibrating model for kernel sparsity...')\n",
    "\n",
    "analyzed_layers = KSAnalyzerLayer.analyze_layers(\n",
    "    model, layers=[name for name, mod in model.named_modules()\n",
    "                   if isinstance(mod, Conv2d) or isinstance(mod, Linear)]\n",
    ")\n",
    "            \n",
    "for epoch in tqdm(range(math.ceil(modifier_manager.max_epochs))):\n",
    "    print('Starting epoch {}'.format(epoch))\n",
    "    optimizer.epoch_start()\n",
    "    train_epoch(model, train_data_loader, optimizer, loss, device, epoch, writer)\n",
    "    \n",
    "    print('Completed training for epoch {}, testing validation dataset'.format(epoch))\n",
    "    val_losses = test_epoch(model, val_data_loader, loss, device, epoch)\n",
    "    for _loss, _values in val_losses.items():\n",
    "        _value = torch.mean(torch.cat(_values))\n",
    "        last_value = _value\n",
    "        writer.add_scalar('Test/Validation/{}'.format(_loss), _value, epoch)\n",
    "        print('{}: {}'.format(_loss, _value))\n",
    "        \n",
    "    print('Completed testing validation dataset for epoch {}, testing training dataset'.format(epoch))\n",
    "    train_losses = test_epoch(model, train_test_data_loader, loss, device, epoch)\n",
    "    for _loss, _values in train_losses.items():\n",
    "        _value = torch.mean(torch.cat(_values))\n",
    "        writer.add_scalar('Test/Training/{}'.format(_loss), _value, epoch)\n",
    "        print('{}: {}'.format(_loss, _value))\n",
    "        \n",
    "    optimizer.epoch_end()\n",
    "    \n",
    "    for ks_layer in analyzed_layers:\n",
    "        writer.add_scalar('Kernel Sparsity/{}'.format(ks_layer.name), ks_layer.param_sparsity.item(), epoch + 1)\n",
    "        \n",
    "    print('Completed testing training dataset for epoch {}'.format(epoch))\n",
    "    \n",
    "pruned_save_path = os.path.abspath(os.path.expanduser(os.path.join('.', '{}.pth'.format(model_id))))\n",
    "print('Finished pruning, saving model to {}'.format(pruned_save_path))\n",
    "save_model(pruned_save_path, model, optimizer, epoch)\n",
    "print('Saved model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
