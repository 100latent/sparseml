{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© 2020 Neuralmagic, Inc., Confidential // Neural Magic Evaluation License Agreement\n",
    "\n",
    "# PyTorch Model Pruning with Adam Optimizer\n",
    "\n",
    "This notebook provides a step-by-step walkthrough for pruning an already trained (dense) model to enable better performance at inference time using the Neural Magic Inference Engine. You will:\n",
    "- Set up the environment\n",
    "- Set up the model and dataset\n",
    "- Analyze loss sensitivity\n",
    "- Select hyperparameters\n",
    "- Recalibrate using pruning\n",
    "- Export to ONNX\n",
    "\n",
    "Reading through this notebook will be reasonably quick to gain an intuition for what is happening. Rough time estimates for fully pruning the default model are given. Note that training with the PyTorch CPU implementation will be much slower than a GPU:\n",
    "- 15 minutes on a GPU\n",
    "- 45 minutes on a laptop CPU\n",
    "\n",
    "## Background\n",
    "Neural networks are generally overparameterized for given tasks (i.e., the number of parameters far exceeds the number of training points), yet they still generalize well. Overparameterization is contrary to conventional ML wisdom, where overparameterizing a model would traditionally lead to overﬁtting. The overall term for this is double descent and it is a very active area of research.\n",
    "\n",
    "A side eﬀect of this overparameterization is that a large number of weights in deep learning networks can be pruned away (set to 0). This was discovered early on by Yann Lecun, but interest waned due to lack of applications at that time. Song Han's 2015 paper reinvigorated the area in pursuit of compressing model size for mobile applications. This renewed interest has resulted in numerous papers on the topic of weight pruning, ﬁlter pruning, channel pruning, and ultimately, block pruning. A Google paper gives a good overview of the current state of kernel sparsity (model pruning).\n",
    "\n",
    "While pruning to increase kernel sparsity, we iteratively go through and remove weights based on their absolute magnitude. The smallest weights are the ones pruned ﬁrst. Generally, two properties enable us to do this: the self-regularizing effect of gradient descent as well as the L1 or L2 regularization functions applied to the weights. Weights that do not help in the optimization process are quickly reduced in absolute value. In this way, pruning can be thought of as an architecture search.\n",
    "\n",
    "What does pruning get us? We now have a model with a lot of multiplications by zero that we don't need to run. If we're smart about how we structure this compute (a surprisingly tricky problem), we can run the model much faster than before! The pruned model plus the ability to run it quickly in the Neural Magic Inference Engine helps to optimize performance. Neural Magic makes it easier to apply the algorithm, giving you more information so you can apply the algorithm with better results.\n",
    "In this notebook, you prune a simple CNN on the MNIST dataset using an Adam optimizer. However, the notebook is designed to be easily extendable for your model and dataset.  Guided instructions are provided in the notebook code comments. \n",
    "\n",
    "Note that the Adam optimizer is easier to use when compared with a Stocahstic Gradient Descent (SGD) optimizer; however, SGD is the preferred method for pruning to ensure the resulting model will generalize well. See our other notebooks for pruning with SGD. \n",
    "\n",
    "## Before you begin…\n",
    "Be sure to read through the README found in the Neural Magic Recalibration Tooling (neuralmagicML) package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Setting Up the Environment\n",
    "\n",
    "In this step, Neural Magic checks your environment setup to ensure the rest of the notebook will flow smoothly.\n",
    "Before running, install the neuralmagicML package into the system using the following command:\n",
    "\n",
    "`pip install neuralmagicML-python/ `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "notebook_name = \"pruning_adam_pytorch\"\n",
    "print(\"checking setup for {}...\".format(notebook_name))\n",
    "\n",
    "# filter because of tensorboard future warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "try:\n",
    "    # make sure neuralmagicML is installed\n",
    "    import neuralmagicML\n",
    "except Exception as ex:\n",
    "    raise Exception(\n",
    "        \"please install neuralmagicML using the setup.py file before continuing\"\n",
    "    )\n",
    "    \n",
    "from neuralmagicML.utilsnb import check_notebook_setup\n",
    "check_notebook_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Setting Up the Model and Dataset\n",
    "\n",
    "By default, you will create a simple CNN to prune on the MNIST dataset. The CNN is already pretrained, and the weights download from the Neural Magic Model Repo. The MNIST dataset will auto-download as well through PyTorch.\n",
    "\n",
    "If you would like to try out your model for pruning, modify the appropriate lines for your model and dataset, speciﬁcally:\n",
    "- model = mnist_net(pretrained=True)\n",
    "- train_dataset = MNISTDataset(dataset_root, train=True) \n",
    "- val_dataset = MNISTDataset(dataset_root, train=False)\n",
    "\n",
    "Take care to keep the variable names the same, as the rest of the notebook is set up according to those.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from neuralmagicML.pytorch.datasets import MNISTDataset\n",
    "from neuralmagicML.pytorch.models import mnist_net\n",
    "from neuralmagicML.utils import clean_path\n",
    "\n",
    "#######################################################\n",
    "# Define your model below\n",
    "#######################################################\n",
    "print(\"loading model...\")\n",
    "model = mnist_net(pretrained=True)\n",
    "model_name = model.__class__.__name__\n",
    "print(model)\n",
    "\n",
    "#######################################################\n",
    "# Define your train and validation datasets below\n",
    "#######################################################\n",
    "\n",
    "print(\"\\nloading train dataset...\")\n",
    "train_dataset = MNISTDataset(train=True)\n",
    "print(train_dataset)\n",
    "\n",
    "print(\"\\nloading val dataset...\")\n",
    "val_dataset = MNISTDataset(train=False)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Analyzing Loss Sensitivity\n",
    "\n",
    "One of the hyperparameters you need to control is how sparse (percentage of zeros) to make each fully connected or convolutional layer in a network. Not all layers are created equal, so you will want to be careful about how you assign sparsity. Generally, the more parameters there are per input data, the less sensitive (and therefore more prunable) the layer will be. For example, a 3x3 convolution is much less sensitive than an equivalent channel sized 1x1 convolution. Likewise, increasing stride for convolutions will increase sensitivity.\n",
    "\n",
    "To enable more natural visibility into this, we provide a quick, one-shot approach to approximating sensitivity. Using the one_shot_ks_loss_sensitivity() function, an algorithm goes layer by layer and prunes each to different levels of sparsity without retraining. In this way, it is a reasonable approximation that is inexpensive to run because it does not require significant computer resources. For display, the piecewise integral of the sparsity versus loss curve is calculated for each layer. Therefore, higher sensitivities mean more loss for a given amount of sparsity.\n",
    "\n",
    "Note: If you changed the model and/or dataset above, you should change the loss, batch_size, and samples_per_measurement variables below. The number of samples per measurement can be relatively small (only one or a few items per class) to get a proper analysis.\n",
    "\n",
    "Finally, after running, the results will be saved to a JSON ﬁle and plotted in this notebook for easy viewing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from neuralmagicML.pytorch.utils import CrossEntropyLossWrapper\n",
    "from neuralmagicML.pytorch.recal import one_shot_ks_loss_sensitivity\n",
    "from neuralmagicML.utils import clean_path\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"running ks loss sensitivity analysis for model on {}\".format(device))\n",
    "\n",
    "#######################################################\n",
    "# Edit paramaters below\n",
    "#######################################################\n",
    "loss = CrossEntropyLossWrapper()\n",
    "batch_size = 1024\n",
    "samples_per_measurement = 1024\n",
    "\n",
    "loss_analysis = one_shot_ks_loss_sensitivity(\n",
    "    model, val_dataset, loss, device, batch_size, samples_per_measurement\n",
    ")\n",
    "\n",
    "save_path = clean_path(\n",
    "    os.path.join(\".\", notebook_name, model_name, \"ks-loss-sensitivity.json\")\n",
    ")\n",
    "loss_analysis.save_json(save_path)\n",
    "print(\"saved analysis to {}\".format(save_path))\n",
    "print(\"plotting...\")\n",
    "fig, axes = loss_analysis.plot(path=None, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Hyperparameters\n",
    "\n",
    "In addition to the sparsity per layer hyperparameter, there are a few more for pruning. The most significant are:\n",
    "- When to start pruning (stabilization period). Letting the model stabilize a bit before beginning pruning is generally a good idea. Edits to the training setup can make the initial epoch or two unstable. So, before cutting out weights, you want to make sure the model is stable.\n",
    "- How long to prune (pruning period). Pruning for more epochs is preferred up to a point. The shorter the pruning period, the less likely it is that the model has converged to a stable position before pruning again. A good rule is to prune over roughly 1/6 to 1/3 the number of epochs it took to train.\n",
    "- How long to train after pruning (fine-tuning period). Generally, the model will not have fully recovered after pruning has stopped. In this case, training should continue a bit longer until the validation loss has stabilized. A good rule is to ﬁne-tune for roughly 1/6 to 1/3 the number of epochs it took to train.\n",
    "- How often to update pruning steps while in the pruning period. The general convention is to apply pruning steps once per epoch. For diﬀerent setups, it may be beneﬁcial to prune more often (e.g., once every tenth of an epoch -- 0.1). It depends on how many weight updates have happened since the last pruning step and how stable the loss function is currently.\n",
    "\n",
    "In support of all these diﬀerent hyperparameters, a conﬁguration ﬁle is used and then loaded at training time. A simple UI is given in the cell block below to enable easy editing of the conﬁguration. The parameters mentioned above can all be adjusted. Soon, Neural Magic will replace this with a more advanced UI with more features to make this selection even easier! For now, we recommend using this notebook and the UI inside to generate the conﬁguration ﬁle. You can look at the output after the next step as it saves the conﬁguration to a ﬁle locally.\n",
    "\n",
    "Defaults are given for the MNIST network and dataset. You may need to change these to better ﬁt your application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.utilsnb import (\n",
    "    KSWidgetContainer,\n",
    "    PruningEpochWidget,\n",
    "    PruningLayersWidget,\n",
    ")\n",
    "from neuralmagicML.pytorch.utils import get_prunable_layers\n",
    "from neuralmagicML.pytorch.models import MnistNet\n",
    "\n",
    "if \"loss_analysis\" not in globals():\n",
    "    loss_analysis = None\n",
    "\n",
    "prune_layers = get_prunable_layers(model)\n",
    "not_mnist = not isinstance(model, MnistNet)\n",
    "widget_container = KSWidgetContainer(\n",
    "    PruningEpochWidget(start_epoch=2, end_epoch=20, total_epochs=25, max_epochs=100),\n",
    "    PruningLayersWidget(\n",
    "        layer_names=[layer[0] for layer in prune_layers],\n",
    "        layer_descs=[str(layer[1]) for layer in prune_layers],\n",
    "        layer_enables=None if not_mnist else [False, True, True, True, True],\n",
    "        layer_sparsities=None if not_mnist else [0.0, 0.8, 0.9, 0.9, 0.9],\n",
    "        loss_sens_analysis=loss_analysis,\n",
    "    ),\n",
    ")\n",
    "print(\"creating ui...\")\n",
    "display(widget_container.create())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Recalibrating Using Pruning\n",
    "\n",
    "Now that the hyperparameters are chosen, you will use them to recalibrate the given model and dataset. The library is designed to be easily plugged into nearly any training setup for PyTorch. In the cell block below is an example of how an integration looks. Note that only five lines are needed to be able to integrate fully.\n",
    "- Create a `ScheduledModifierManager()`. This loads the conﬁg into PyTorch objects that modify the training process.\n",
    "- Create a `ScheduledOptimizer()`. This updates the PyTorch objects that modify the training process. It wraps the original optimizer that was used to modify the training process/graph, and should be used in place of that (`optimizer.step()` must be called on ScheduledOptimizer and not the original).\n",
    "- Use `max_epochs` on the `ScheduledModifierManager` to know how many epochs are needed for training.\n",
    "- Call into the `ScheduledOptimizer` for `epoch_start()` and `epoch_end()` before training. These calls mark when an epoch has started and after training when an epoch has ended, respectively.\n",
    " \n",
    "Once the training objects are created (optimizer, loss function, etc.), a `ScheduledModifierManager` and `ScheduledOptimizer` are instantiated from the conﬁguration. Almost all logging and updates are done through TensorBoard for this notebook. The use of TensorBoard is entirely optional. Finally, regular training and testing code for PyTorch is used to go through the process.\n",
    "\n",
    "Note, for convenience a TensorBoard instance is launched in the cell below pointed at `localhost`. If you are running this notebook on a remote server, then you will need to update TensorBoard accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import auto\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from neuralmagicML.utils import create_unique_dir, clean_path\n",
    "from neuralmagicML.pytorch.utils import (\n",
    "    CrossEntropyLossWrapper,\n",
    "    TopKAccuracy,\n",
    "    ModuleTrainer,\n",
    "    ModuleTester,\n",
    "    TensorBoardLogger,\n",
    ")\n",
    "from neuralmagicML.pytorch.recal import ScheduledModifierManager, ScheduledOptimizer\n",
    "\n",
    "# save the config locally for use in this flow\n",
    "config_path = clean_path(os.path.join(\".\", notebook_name, model_name, \"config.yaml\"))\n",
    "print(\"saving config to {}\".format(config_path))\n",
    "widget_container.get_manager(\"pytorch\").save(config_path)\n",
    "\n",
    "# setup device, data loaders, loss, optimizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 1024\n",
    "train_data_loader = DataLoader(train_dataset, batch_size, shuffle=True, pin_memory=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size, shuffle=False, pin_memory=True)\n",
    "loss = CrossEntropyLossWrapper(extras={\"top1acc\": TopKAccuracy(1)})\n",
    "optim = Adam(model.parameters())\n",
    "print(\"device:{} batch_size:{} loss:{}\".format(device, batch_size, loss))\n",
    "\n",
    "tensorboard_model_path = create_unique_dir(\n",
    "    os.path.join(\".\", \"tensorboard-logs\", notebook_name, model_name)\n",
    ")\n",
    "print(\"logging at {}\".format(tensorboard_model_path))\n",
    "\n",
    "#######################################################\n",
    "# First lines required for recalibrating a model in PyTorch\n",
    "#######################################################\n",
    "loggers = [TensorBoardLogger(tensorboard_model_path)]\n",
    "steps_per_epoch = math.ceil(len(train_dataset) / batch_size)\n",
    "manager = ScheduledModifierManager.from_yaml(config_path)\n",
    "optim = ScheduledOptimizer(optim, model, manager, steps_per_epoch=steps_per_epoch, loggers=loggers)\n",
    "print(\"created manager and optimizer from config at {}\".format(config_path))\n",
    "\n",
    "# we use prewritten trainers and testers to make the code more concise\n",
    "trainer = ModuleTrainer(model, device, loss, optim, loggers=loggers)\n",
    "tester = ModuleTester(model, device, loss, loggers=loggers)\n",
    "model = model.to(device)\n",
    "\n",
    "# startup tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tensorboard-logs\n",
    "\n",
    "# run initial validation for comparison\n",
    "tester.run_epoch(val_data_loader, epoch=-1, show_progress=False)\n",
    "\n",
    "#######################################################\n",
    "# Final lines required for recalibrating a model in PyTorch\n",
    "#######################################################\n",
    "for epoch in auto.tqdm(range(manager.max_epochs), desc=\"training\"):\n",
    "    optim.epoch_start()\n",
    "\n",
    "    trainer.run_epoch(train_data_loader, epoch, show_progress=False)\n",
    "    tester.run_epoch(val_data_loader, epoch, show_progress=False)\n",
    "\n",
    "    optim.epoch_end()\n",
    "\n",
    "# delete so all modifiers are cleaned up before exporting\n",
    "del optim\n",
    "print(\"training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Exporting to ONNX\n",
    "\n",
    "Now that the model is fully recalibrated, you need to export it to an ONNX format, which is the format used by the Neural Magic Inference Engine. For PyTorch, exporting to ONNX is natively supported. In the cell block below, a convenience class, ModuleExporter(), is used to handle exporting.\n",
    "\n",
    "Once the model is saved as an ONNX ﬁle, it is ready to be used for inference with Neural Magic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.utils import clean_path\n",
    "from neuralmagicML.pytorch.utils import ModuleExporter\n",
    "\n",
    "print(\"exporting to onnx...\")\n",
    "export_path = clean_path(os.path.join(\".\", notebook_name, model_name))\n",
    "exporter = ModuleExporter(model, export_path)\n",
    "for batch in val_data_loader:\n",
    "    sample_input = batch[0]\n",
    "    break\n",
    "exporter.export_onnx(sample_input)\n",
    "print(\"exported onnx to {}\".format(export_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Run your model (ONNX file) through the Neural Magic Inference Engine. The following is an example of code that you can run in your Python console. Be sure to enter your ONNX file path and batch size.\n",
    "\n",
    "```\n",
    "from neuralmagic import create_model\n",
    "model = create_model(onnx_file_path=’some/path/to/model.onnx’, batch_size=1)\n",
    "inp = [numpy.random.rand(1, 3, 224, 224).astype(numpy.float32)]\n",
    "out = model.forward(inp)\n",
    "print(out)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
