{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Model Pruning with Adam Optimizer\n",
    "\n",
    "Neural networks are generally overparameterized for given tasks (i.e., the number of parameters far exceeds the number of training points), yet they still [generalize very well](https://arxiv.org/abs/1611.03530). Overparametrization is contrary to conventional ML wisdom, where overparameterizing a model would traditionally lead to overfitting. The overall term for this is [double descent](https://openai.com/blog/deep-double-descent/), and it is a very active area of research.\n",
    "\n",
    "A side effect of this overparameterization is that a large number of weights in deep learning networks can be pruned away (set to 0). This was discovered [early on](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf) by Yann Lecun, but interest died off due to lack of applications at that time. Song Han's [2015 paper](https://arxiv.org/abs/1510.00149) reinvigorated the area in pursuit of compressing model size for mobile applications. This renewed interest has resulted in numerous papers coming out on the topic of weight pruning, filter pruning, channel pruning, and ultimately block pruning. [This paper](https://arxiv.org/abs/1902.09574) out of Google gives a good overview of the current state of kernel sparsity (model pruning).\n",
    "\n",
    "While pruning to increase kernel sparsity, we iteratively go through and remove weights based on their absolute magnitude. The smallest weights are the ones pruned first. Generally two properties enable us to do this: the [self regularizing effect of gradient descent](https://www.nature.com/articles/s41467-020-14663-9) as well as the L1 or L2 regularization functions applied to the weights. Weights that do not help in the optimization process are quickly reduced in absolute value. In this way, pruning can be thought of as an [architecture search](https://arxiv.org/abs/1905.09717).\n",
    "\n",
    "Well, what does pruning get us? We now have a model with a lot of multiplications by zero that we don't need to run. If we're smart about how we do structure this compute (a surprisingly tricky problem), we can run the model much faster than before! That's where the [Neural Magic](http://neuralmagic.com/) engine can help us.\n",
    "\n",
    "This notebook provides a step by step walkthrough for pruning an already trained (dense) model to enable better performance at inference time using the Neural Magic Inference Engine. By default, we prune a simple [CNN](https://en.wikipedia.org/wiki/Convolutional_neural_network) on the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) using an [Adam optimizer](https://arxiv.org/abs/1412.6980); however, it is designed to be easily extendable for your model and dataset. Note that the Adam optimizer is easier to use when compared with a [Stocahstic Gradient Descent (SGD) optimizer](https://en.wikipedia.org/wiki/Stochastic_gradient_descent); however, SGD is the preferred method for pruning to ensure the resulting model will generalize well. See our other notebooks for pruning with SGD. In this notebook, we walk through the following items:\n",
    "1. Environment Setup\n",
    "2. Model and Dataset Setup\n",
    "3. Loss Sensitivity Analysis\n",
    "4. Hyperparameter Selection\n",
    "5. Recalibration Using Pruning\n",
    "6. Export to [ONNX](https://onnx.ai/)\n",
    "\n",
    "Reading through this notebook will be reasonably quick to gain an intuition for what is happening. Rough time estimates for fully pruning the default model are given, note since we are training with the PyTorch CPU implementation it will be much slower than a GPU:\n",
    "- 5 minutes on a GPU\n",
    "- 30 minutes on a laptop CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Below we try to add the project folder to the PYTHONPATH environment variable for our execution. If this does not work, we will need to install neuralmagicML into the system using `pip install ./` when you are located at the root of the folder.\n",
    "\n",
    "Additionally, please be sure to install from the requirements.txt file located at the root before running: `pip install -r ./requirements.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_name = \"pruning_adam_pytorch\"\n",
    "\n",
    "# environment setup for ease of use (puts neuralmagicML into the python package path)\n",
    "if \"WORKBOOK_DIR\" not in globals():\n",
    "    WORKBOOK_DIR = os.getcwd()\n",
    "\n",
    "package_path = os.path.abspath(\n",
    "    os.path.join(os.path.expanduser(WORKBOOK_DIR), os.pardir)\n",
    ")\n",
    "sys.path.extend([package_path])\n",
    "\n",
    "print(\"added {} to PYTHONPATH\".format(package_path))\n",
    "print(\"working out of {}\".format(WORKBOOK_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Dataset Setup\n",
    "\n",
    "By default, we create a simple CNN to prune on the MNIST dataset. The CNN is already pretrained, and the weights download from the Neural Magic Model Repo. The MNIST dataset will auto-download as well through PyTorch.\n",
    "\n",
    "If you would like to try out your model for pruning, then replace the appropriate lines for your model and dataset. Specifically,\n",
    "- `model = mnist_net(pretrained=True)`\n",
    "- `train_dataset = MNISTDataset(dataset_root, train=True)`\n",
    "- `val_dataset = MNISTDataset(dataset_root, train=False)`\n",
    "\n",
    "Take care to keep the variable names the same, as the rest of the notebook is set up according to those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from neuralmagicML.pytorch.datasets import MNISTDataset\n",
    "from neuralmagicML.pytorch.models import mnist_net\n",
    "from neuralmagicML.utils import clean_path\n",
    "\n",
    "#######################################################\n",
    "# Define your model below\n",
    "#######################################################\n",
    "print(\"loading model...\")\n",
    "model = mnist_net(pretrained=True)\n",
    "model_name = model.__class__.__name__\n",
    "print(model)\n",
    "\n",
    "#######################################################\n",
    "# Define your train and validation datasets below\n",
    "#######################################################\n",
    "dataset_root = clean_path(os.path.join(\".\", notebook_name, \"datasets\"))\n",
    "\n",
    "print(\"\\nloading train dataset...\")\n",
    "train_dataset = MNISTDataset(dataset_root, train=True)\n",
    "print(train_dataset)\n",
    "\n",
    "print(\"\\nloading val dataset...\")\n",
    "val_dataset = MNISTDataset(dataset_root, train=False)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Sensitivity Analysis\n",
    "\n",
    "One of the hyperparameters we need to control is how sparse (percentage of zeros) to make each fully connected or convolutional layer in a network. Not all layers are created equal, so we want to be careful about how we assign sparsity. Generally, the more parameters there are per input data, the less sensitive (and therefore more prunable) the layer will be. For example, a 3x3 convolution is much less sensitive than an equivalent channel sized 1x1 convolution. Likewise, increasing stride for convolutions will increase sensitivity.\n",
    "\n",
    "To enable more natural visibility into this, we provide a quick, one-shot approach to approximating sensitivity. Using the [`one_shot_ks_loss_sensitivity()`]() function, an algorithm goes layer by layer and prunes each to different levels of sparsity without retraining. In this way, it is both cheap to run and a reasonable approximation. For display, the piecewise integral of the sparsity vs. loss curve is calculated for each layer. Therefore, higher sensitivities mean more loss for a given amount of sparsity.\n",
    "\n",
    "Note, if you changed the model and/or dataset above, you likely should change the `loss`, `batch_size`, and `samples_per_measurement` variables below. The number of samples per measurement can be relatively small (only one or a few items per class) to get a proper analysis.\n",
    "\n",
    "Finally, after running, the results will be saved to a JSON file and plotted in this notebook for easy viewing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from neuralmagicML.pytorch.utils import CrossEntropyLossWrapper\n",
    "from neuralmagicML.pytorch.recal import one_shot_ks_loss_sensitivity\n",
    "from neuralmagicML.utils import clean_path\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"running ks loss sensitivity analysis for model on {}\".format(device))\n",
    "\n",
    "#######################################################\n",
    "# Edit paramaters below\n",
    "#######################################################\n",
    "loss = CrossEntropyLossWrapper()\n",
    "batch_size = 1024\n",
    "samples_per_measurement = 1024\n",
    "\n",
    "loss_analysis = one_shot_ks_loss_sensitivity(\n",
    "    model, val_dataset, loss, device, batch_size, samples_per_measurement\n",
    ")\n",
    "\n",
    "save_path = clean_path(\n",
    "    os.path.join(\".\", notebook_name, model_name, \"ks-loss-sensitivity.json\")\n",
    ")\n",
    "loss_analysis.save_json(save_path)\n",
    "print(\"saved analysis to {}\".format(save_path))\n",
    "print(\"plotting...\")\n",
    "fig, axes = loss_analysis.plot(path=None, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Selection\n",
    "\n",
    "In addition to the sparsity per layer hyperparams, there are a few more for pruning. The biggest ones are:\n",
    "- When to start pruning `(stabilization period)`. Letting the model stabilize a bit before beginning pruning is generally a good idea. Edits to the training setup can make the initial epoch or two unstable. So, before cutting out weights, we want to make sure the model is stable.\n",
    "- How long to prune `(pruning period)`; pruning for more epochs is preferred up to a point. The shorter we make our pruning period, the less likely it is that the model has converged to a stable position before pruning again. A good rule of thumb is to prune over roughly 1/6 - 1/3 of the number of epochs it took to train.\n",
    "- How long to train after pruning `(fine-tuning period)`. Generally, the model will not have fully recovered after pruning has stopped. In this case, training should continue for a bit longer until the validation loss has stabilized. A good rule of thumb is to fine-tune for roughly 1/6 - 1/3 of the number of epochs it took to train.\n",
    "- How often to update pruning steps while in the pruning period. The general convention is to apply pruning steps once per epoch. For different setups, it may be beneficial to prune more often (ex: once every tenth of an epoch -- 0.1). It depends on how many weight updates have happened since the last pruning step and how stable the loss function is currently.\n",
    "\n",
    "In support of all these different hyperparameters, a config file is used and then loaded at training time. A simple UI is given below to enable easy editing of the config. The parameters mentioned above can all be adjusted. Soon we will be replacing this with a more advanced UI with more features to make this selection even easier! For now, we recommend using this notebook and the UI inside for generating the config files. If you are curious, you can look at the output after the next step as it saves the config to a file locally.\n",
    "\n",
    "Defaults are given for the MNIST network and dataset. These will likely need to be changed to fit your application better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.nbutils import (\n",
    "    KSWidgetContainer,\n",
    "    PruningEpochWidget,\n",
    "    PruningLayersWidget,\n",
    ")\n",
    "from neuralmagicML.pytorch.utils import get_prunable_layers\n",
    "from neuralmagicML.pytorch.models import MnistNet\n",
    "\n",
    "if \"loss_analysis\" not in globals():\n",
    "    loss_analysis = None\n",
    "\n",
    "prune_layers = get_prunable_layers(model)\n",
    "not_mnist = not isinstance(model, MnistNet)\n",
    "widget_container = KSWidgetContainer(\n",
    "    PruningEpochWidget(start_epoch=2, end_epoch=20, total_epochs=25, max_epochs=100),\n",
    "    PruningLayersWidget(\n",
    "        layer_names=[layer[0] for layer in prune_layers],\n",
    "        layer_descs=[str(layer[1]) for layer in prune_layers],\n",
    "        layer_enables=None if not_mnist else [False, True, True, True, True],\n",
    "        layer_sparsities=None if not_mnist else [0.0, 0.8, 0.9, 0.9, 0.9],\n",
    "        loss_sens_analysis=loss_analysis,\n",
    "    ),\n",
    ")\n",
    "print(\"creating ui...\")\n",
    "display(widget_container.create())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recalibration Using Pruning\n",
    "\n",
    "Now that the hyperparameters are chosen, we will use them to recalibrate the given model and dataset. The library is designed to be easily plugged into nearly any training setup for PyTorch. Below we provide an example of how an integration looks. Note, only a handful of these lines are needed to be able to integrate fully.\n",
    "\n",
    "1. Create a [`ScheduledModifierManager`](); handles loading the config into PyTorch objects that modify the training process.\n",
    "2. Create a [`ScheduledOptimizer`](); handles updating the PyTorch objects that modify the training process. It wraps the original optimizer that was used to modify the training process/graph, and should be used in place of that. IE, optimizer.step() must be called on `ScheduledOptimizer` and not the original.\n",
    "3. Use `max_epochs` on the `ScheduledModifierManager` to know how many epochs are needed for training.\n",
    "4. Call into the `ScheduledOptimizer` for `epoch_start()` and `epoch_end()` before training. These calls mark when an epoch has started and after training for an epoch has ended, respectively.\n",
    "\n",
    "Once the training objects are created (optimizer, loss function, etc.), a `ScheduledModifierManager` and `ScheduledOptimizer` are instantiated from the config. Almost all logging and updates are done through `Tensorboard` for this notebook. The use of `Tensorboard` is optional, and other loggers (as well as not using a logger) are available. Finally, regular training and testing code is used to go through the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import auto\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from neuralmagicML.pytorch.utils import (\n",
    "    CrossEntropyLossWrapper,\n",
    "    TopKAccuracy,\n",
    "    ModuleTrainer,\n",
    "    ModuleTester,\n",
    "    TensorboardLogger,\n",
    ")\n",
    "from neuralmagicML.utils import create_unique_dir, clean_path\n",
    "\n",
    "# save the config locally for use in this flow\n",
    "config_path = clean_path(os.path.join(\".\", notebook_name, model_name, \"config.yaml\"))\n",
    "print(\"saving config to {}\".format(config_path))\n",
    "widget_container.get_manager(\"pytorch\").save(config_path)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# Necessary imports for running recalibration\n",
    "#######################################################\n",
    "from neuralmagicML.pytorch.recal import ScheduledModifierManager, ScheduledOptimizer\n",
    "\n",
    "\n",
    "# setup device, data loaders, loss, optimizer\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 1024\n",
    "train_data_loader = DataLoader(train_dataset, batch_size, shuffle=True, pin_memory=True)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size, shuffle=False, pin_memory=True)\n",
    "loss = CrossEntropyLossWrapper(extras={\"top1acc\": TopKAccuracy(1)})\n",
    "optim = Adam(model.parameters())\n",
    "print(\"device:{} batch_size:{} loss:{}\".format(device, batch_size, loss))\n",
    "\n",
    "tensorboard_model_path = create_unique_dir(\n",
    "    os.path.join(\".\", \"tensorboard-logs\", notebook_name, model_name)\n",
    ")\n",
    "loggers = [TensorboardLogger(tensorboard_model_path)]\n",
    "print(\"logging at {}\".format(tensorboard_model_path))\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# First lines that must be substituted in training code\n",
    "# We create a modifier manager as well as a scheduled optimizer\n",
    "# These will apply the config we created while running the training process\n",
    "# The loggers can be left out if desired\n",
    "#######################################################\n",
    "manager = ScheduledModifierManager.from_yaml(config_path)\n",
    "optim = ScheduledOptimizer(\n",
    "    optim,\n",
    "    model,\n",
    "    manager,\n",
    "    steps_per_epoch=math.ceil(len(train_dataset) / batch_size),\n",
    "    loggers=loggers,\n",
    ")\n",
    "print(\"created manager and optimizer from config at {}\".format(config_path))\n",
    "\n",
    "\n",
    "# we use prewritten trainers and testers to make the code more concise\n",
    "trainer = ModuleTrainer(model, device, loss, optim, loggers=loggers)\n",
    "tester = ModuleTester(model, device, loss, loggers=loggers)\n",
    "model = model.to(device)\n",
    "\n",
    "# startup tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tensorboard-logs\n",
    "\n",
    "# run initial validation for comparison\n",
    "tester.run_epoch(val_data_loader, epoch=-1, show_progress=False)\n",
    "\n",
    "\n",
    "#######################################################\n",
    "# Final lines that must be substituted in training code\n",
    "# We continue training for as long as requested in the config\n",
    "# Additionally, we tell the ScheduledOptimizer when each epoch\n",
    "# has started and ended\n",
    "#######################################################\n",
    "for epoch in auto.tqdm(range(manager.max_epochs), desc=\"training\"):\n",
    "    optim.epoch_start()\n",
    "\n",
    "    trainer.run_epoch(train_data_loader, epoch, show_progress=False)\n",
    "    tester.run_epoch(val_data_loader, epoch, show_progress=False)\n",
    "\n",
    "    optim.epoch_end()\n",
    "\n",
    "# delete so all modifiers are cleaned up before exporting\n",
    "del optim\n",
    "print(\"training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to ONNX\n",
    "\n",
    "Now that the model is fully recalibrated, we need to export it to an ONNX format. The ONNX format is what is used by the Neural Magic Inference Engine. For PyTorch, exporting to ONNX is natively supported. Below we use a convenience class to handle exporting, the [`ModuleExporter`](). Once the model has saved as an ONNX file, it is ready to be used for inference with Neural Magic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.utils import clean_path\n",
    "from neuralmagicML.pytorch.utils import ModuleExporter\n",
    "\n",
    "export_path = clean_path(os.path.join(\".\", notebook_name, model_name))\n",
    "exporter = ModuleExporter(model, export_path)\n",
    "for batch in val_data_loader:\n",
    "    sample_input = batch[0]\n",
    "    break\n",
    "exporter.export_onnx(sample_input)\n",
    "print(\"exported onnx to {}\".format(export_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
