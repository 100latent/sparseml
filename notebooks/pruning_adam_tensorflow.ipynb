{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<sub>&copy; 2020 Neuralmagic, Inc., Confidential // [Neural Magic Evaluation License Agreement](https://neuralmagic.com/evaluation-license-agreement/)</sub> \n",
    "\n",
    "# TensorFlow Model Pruning with Adam Optimizer\n",
    "\n",
    "This notebook provides a step-by-step walkthrough for training and pruning a model to enable better performance at inference time using the Neural Magic Inference Engine. You will:\n",
    "1. Set up the environment\n",
    "2. Set up the model and dataset\n",
    "3. Analyze loss sensitivity\n",
    "4. Select hyperparameters\n",
    "5. Recalibrate using pruning\n",
    "6. Export to [ONNX](https://onnx.ai/)\n",
    "\n",
    "Reading through this notebook will be reasonably quick to gain an intuition for what is happening. Rough time estimates for fully pruning the default model are given. Note that training with the TensorFlow CPU implementation will be much slower than a GPU:\n",
    "- 10 minutes on a GPU\n",
    "- 45 minutes on a laptop CPU\n",
    "\n",
    "## Background\n",
    "\n",
    "Neural networks are generally overparameterized for given tasks (i.e., the number of parameters far exceeds the number of training points), yet they [still generalize well](https://arxiv.org/abs/1611.03530). Overparameterization is contrary to conventional ML wisdom, where overparameterizing a model would traditionally lead to overﬁtting. The overall term for this is [double descent](https://openai.com/blog/deep-double-descent/) and it is a very active area of research.\n",
    "\n",
    "A side effect of this overparameterization is that a large number of weights in deep learning networks can be pruned away (set to 0). This was [discovered early on](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf) by Yann Lecun, but interest waned due to lack of applications at that time. [Song Han's 2015 paper](https://arxiv.org/abs/1510.00149) reinvigorated the area in pursuit of compressing model size for mobile applications. This renewed interest has resulted in numerous papers on the topic of weight pruning, ﬁlter pruning, channel pruning, and ultimately, block pruning. A [Google paper](https://arxiv.org/abs/1902.09574) gives a good overview of the current state of kernel sparsity (model pruning).\n",
    "\n",
    "While pruning to increase kernel sparsity, we iteratively go through and remove weights based on their absolute magnitude. The smallest weights are the ones pruned ﬁrst. Generally, two properties enable us to do this: the self-regularizing [effect of gradient descent](https://www.nature.com/articles/s41467-020-14663-9) as well as the L1 or L2 regularization functions applied to the weights. Weights that do not help in the optimization process are quickly reduced in absolute value. In this way, pruning can be thought of as an [architecture search](https://arxiv.org/abs/1905.09717).\n",
    "\n",
    "What does pruning get us? We now have a model with a lot of multiplications by zero that we don't need to run. If we're smart about how we structure this compute (a surprisingly tricky problem), we can run the model much faster than before! The pruned model plus the ability to run it quickly in the Neural Magic Inference Engine helps to optimize performance. [Neural Magic](http://neuralmagic.com/) makes it easier to apply the algorithm, giving you more information so you can apply the algorithm with better results.\n",
    "\n",
    "In this notebook, you prune a simple [CNN](http://yann.lecun.com/exdb/mnist/) on the [MNIST dataset](https://arxiv.org/abs/1412.6980) using an [Adam optimizer](https://arxiv.org/abs/1412.6980). However, the notebook is designed to be easily extendable for your model and dataset.  Guided instructions are provided in the notebook code comments. \n",
    "\n",
    "Note that the Adam optimizer is easier to use when compared with a [Stocahstic Gradient Descent (SGD) optimizer](https://en.wikipedia.org/wiki/Stochastic_gradient_descent); however, SGD is the preferred method for pruning to ensure the resulting model will generalize well. See our other notebooks for pruning with SGD. \n",
    "\n",
    "## Before you begin…\n",
    "Be sure to read through the README found in the Neural Magic ML Tooling (neuralmagicML) package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Setting Up the Environment\n",
    "\n",
    "In this step, Neural Magic checks your environment setup to ensure the rest of the notebook will flow smoothly.\n",
    "Before running, install the neuralmagicML package into the system using the following command:\n",
    "\n",
    "`pip install neuralmagicML-python/ `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "notebook_name = \"pruning_adam_tensorflow\"\n",
    "print(\"checking setup for {}...\".format(notebook_name))\n",
    "\n",
    "# filter because of tensorboard future warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "try:\n",
    "    # make sure neuralmagicML is installed\n",
    "    import neuralmagicML\n",
    "except Exception as ex:\n",
    "    raise Exception(\n",
    "        \"please install neuralmagicML using the setup.py file before continuing\"\n",
    "    )\n",
    "\n",
    "from neuralmagicML.utilsnb import check_tensorflow_notebook_setup\n",
    "\n",
    "check_tensorflow_notebook_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Setting Up the Model and Dataset\n",
    "\n",
    "By default, you will create a simple CNN to prune on the MNIST dataset. The CNN is already pre-trained, and the weights download from the Neural Magic Model Repo. The MNIST dataset will auto-download as well through TensorFlow.\n",
    "\n",
    "If you would like to try out your model for pruning, modify the appropriate lines for your model and dataset, speciﬁcally:\n",
    "- graph_creator():  # replace the function wiith your desired model\n",
    "- dataset_root = clean_path(os.path.join(\".\", notebook_name, \"datasets\", \"mnist\"))\n",
    "- dataset = input_data.read_data_sets(dataset_root, one_hot=True)\n",
    "\n",
    "Take care to keep the variable names the same, as the rest of the notebook is set up according to those.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from neuralmagicML.tensorflow.models import ModelRegistry\n",
    "from neuralmagicML.tensorflow.utils import tf_compat, batch_cross_entropy_loss, accuracy\n",
    "from neuralmagicML.utils import clean_path\n",
    "\n",
    "# ignore tf v1 vs v2 deprecation warnings\n",
    "tf_compat.logging.set_verbosity(tf_compat.logging.ERROR)\n",
    "\n",
    "#######################################################\n",
    "# Define your graph below\n",
    "#######################################################\n",
    "model_name = \"mnistnet\"\n",
    "\n",
    "\n",
    "def graph_creator():\n",
    "    inputs = tf_compat.placeholder(tf_compat.float32, [None, 28, 28, 1], name=\"inputs\")\n",
    "    labels = tf_compat.placeholder(tf_compat.float32, [None, 10])\n",
    "    logits = ModelRegistry.create(\"mnistnet\", inputs)\n",
    "    loss = batch_cross_entropy_loss(logits, labels)\n",
    "    acc = accuracy(logits, labels)\n",
    "\n",
    "    return inputs, labels, logits, loss, acc\n",
    "\n",
    "\n",
    "print(graph_creator)\n",
    "\n",
    "#######################################################\n",
    "# Define your dataset below\n",
    "#######################################################\n",
    "print(\"\\nloading dataset...\")\n",
    "dataset_root = clean_path(os.path.join(\".\", notebook_name, \"datasets\", \"mnist\"))\n",
    "dataset = input_data.read_data_sets(dataset_root, one_hot=True)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Analyzing Loss Sensitivity\n",
    "\n",
    "One of the hyperparameters you need to control is how sparse (percentage of zeros) to make each fully connected or convolutional layer in a network. Not all layers are created equal, so you will want to be careful about how you assign sparsity. Generally, the more parameters there are per input data, the less sensitive (and therefore more prunable) the layer will be. For example, a 3x3 convolution is much less sensitive than an equivalent channel sized 1x1 convolution. Likewise, increasing stride for convolutions will increase sensitivity.\n",
    "\n",
    "To enable more natural visibility into this, we provide a quick approach to approximating sensitivity. Using the `approx_ks_loss_sensitivity()` function, an algorithm goes layer by layer and checks the magnitude of remaining weights at various levels of sparsity. In this way, it is a reasonable approximation that is inexpensive to run because it does not require significant computer resources. For display, the piecewise integral of the sparsity versus loss curve is calculated for each layer. Therefore, higher sensitivities mean more sensitivity for a given amount of sparsity.\n",
    "\n",
    "To run the analysis, we create the graph and then pass it in to `approx_ks_loss_sensitivity()`.\n",
    "\n",
    "Finally, after running, the results will be saved to a JSON ﬁle and plotted in this notebook for easy viewing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from neuralmagicML.utils import clean_path\n",
    "from neuralmagicML.tensorflow.utils import tf_compat\n",
    "from neuralmagicML.tensorflow.recal import approx_ks_loss_sensitivity\n",
    "from neuralmagicML.tensorflow.models import ModelRegistry\n",
    "\n",
    "with tf_compat.Graph().as_default() as graph:\n",
    "    print(\"Creating graph...\")\n",
    "    inputs, labels, logits, loss, acc = graph_creator()\n",
    "    \n",
    "    with tf_compat.Session() as sess:\n",
    "        print(\"loading pre-trained weights...\")\n",
    "        ModelRegistry.load_pretrained(\"mnistnet\")\n",
    "\n",
    "        print(\"running ks loss sensitivity analysis...\")\n",
    "        loss_analysis = approx_ks_loss_sensitivity(graph)\n",
    "\n",
    "save_path = clean_path(\n",
    "    os.path.join(\".\", notebook_name, model_name, \"ks-loss-sensitivity.json\")\n",
    ")\n",
    "loss_analysis.save_json(save_path)\n",
    "print(\"saved analysis to {}\".format(save_path))\n",
    "print(\"plotting...\")\n",
    "fig, axes = loss_analysis.plot(path=None, plot_integral=True, normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Hyperparameters\n",
    "\n",
    "In addition to the sparsity per layer hyperparameter, there are a few more for pruning. The most significant are:\n",
    "- When to start pruning (stabilization period). Letting the model stabilize a bit before beginning pruning is generally a good idea. Edits to the training setup can make the initial epoch or two unstable. So, before cutting out weights, you want to make sure the model is stable.\n",
    "- How long to prune (pruning period). Pruning for more epochs is preferred up to a point. The shorter the pruning period, the less likely it is that the model has converged to a stable position before pruning again. A good rule is to prune over roughly 1/6 to 1/3 the number of epochs it took to train.\n",
    "- How long to train after pruning (fine-tuning period). Generally, the model will not have fully recovered after pruning has stopped. In this case, training should continue a bit longer until the validation loss has stabilized. A good rule is to ﬁne-tune for roughly 1/6 to 1/3 the number of epochs it took to train.\n",
    "- How often to update pruning steps while in the pruning period. The general convention is to apply pruning steps once per epoch. For different setups, it may be beneﬁcial to prune more often (e.g., once every tenth of an epoch -- 0.1). It depends on how many weight updates have happened since the last pruning step and how stable the loss function is currently.\n",
    "\n",
    "In support of all these different hyperparameters, a conﬁguration ﬁle is used and then loaded at training time. A simple UI is given in the cell block below to enable easy editing of the conﬁguration. The parameters mentioned above can all be adjusted. Soon, Neural Magic will replace this with a more advanced UI with more features to make this selection even easier! For now, we recommend using this notebook and the UI inside to generate the conﬁguration ﬁle. You can look at the output after the next step as it saves the conﬁguration to a ﬁle locally.\n",
    "\n",
    "Defaults are given for the MNIST network and dataset. You may need to change these to better ﬁt your application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.utilsnb import (\n",
    "    KSWidgetContainer,\n",
    "    PruningEpochWidget,\n",
    "    PruningParamsWidget,\n",
    ")\n",
    "from neuralmagicML.tensorflow.utils import get_ops_and_inputs_by_name_or_regex\n",
    "\n",
    "if \"loss_analysis\" not in globals():\n",
    "    loss_analysis = None\n",
    "\n",
    "with tf_compat.Graph().as_default() as graph:\n",
    "    print(\"Creating graph...\")\n",
    "    graph_creator()\n",
    "\n",
    "    # match all model weights\n",
    "    prune_ops_and_tens = get_ops_and_inputs_by_name_or_regex(\n",
    "        [\"re:.*/conv./weight\", \"re:.*/fc/weight\"]\n",
    "    )\n",
    "\n",
    "    not_mnist = model_name != \"mnistnet\"\n",
    "    widget_container = KSWidgetContainer(\n",
    "        PruningEpochWidget(\n",
    "            start_epoch=2,\n",
    "            end_epoch=20,\n",
    "            total_epochs=25,\n",
    "            max_epochs=100,\n",
    "            update_frequency=0.0,\n",
    "        ),\n",
    "        PruningParamsWidget(\n",
    "            param_names=[tens.name for _, tens in prune_ops_and_tens],\n",
    "            param_descs=[op.type for op, _ in prune_ops_and_tens],\n",
    "            param_enables=None if not_mnist else [True, True, True, True, False],\n",
    "            param_sparsities=None if not_mnist else [0.85, 0.8, 0.85, 0.85, 0.0],\n",
    "            loss_sens_analysis=loss_analysis,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "print(\"Creating ui...\")\n",
    "display(widget_container.create())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Recalibrating Using Pruning\n",
    "\n",
    "Now that the hyperparameters are chosen, you will use them to recalibrate the given model and dataset. The library is designed to be easily plugged into nearly any training setup for TensorFlow. In the cell block below is an example of how an integration looks. Note that only five lines are needed to be able to integrate fully.\n",
    "- Create a `ScheduledModifierManager()`. This loads the conﬁg into TensorFlow objects that modify the training process.\n",
    "- Invoke `manager.create_ops()` for the desired graph. This updates the TensorFlow graph with the proper operators that modify the training process.\n",
    "- Use `manager.max_epochs` to know how many epochs are needed for training.\n",
    "- Invoke `sess.run(mod_ops)` on each optimizer step. This updates the modifying operators and variables in the TensorFlow graph.\n",
    "- Invoke `manager.complete_graph()` once training has completed. This wilil cleanup the graph and set any final state for graph export and saving.\n",
    " \n",
    "Once the training objects are created (optimizer, loss function, etc.), a `ScheduledModifierManager` is instantiated from the conﬁguration. Most logging and updates are done through TensorBoard for this notebook. The use of TensorBoard is entirely optional. Finally, regular training and testing code is used to go through the process.\n",
    "\n",
    "Note, for convenience a TensorBoard instance is launched in the cell below pointed at `localhost`. If you are running this notebook on a remote server, then you will need to update TensorBoard accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import auto\n",
    "from neuralmagicML.utils import create_unique_dir, clean_path, create_dirs\n",
    "from neuralmagicML.tensorflow.utils import eval_tensor_sparsity\n",
    "from neuralmagicML.tensorflow.models import ModelRegistry\n",
    "from neuralmagicML.tensorflow.recal import ScheduledModifierManager\n",
    "\n",
    "# save the config locally for use in this flow\n",
    "config_path = clean_path(os.path.join(\".\", notebook_name, model_name, \"config.yaml\"))\n",
    "print(\"saving config to {}\".format(config_path))\n",
    "widget_container.get_manager(\"tensorflow\").save(config_path)\n",
    "\n",
    "# startup tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./tensorboard-logs\n",
    "\n",
    "\n",
    "def calc_test_metrics(acc, loss, inputs, labels):\n",
    "    test_xs = dataset.test.images.reshape([-1, 28, 28, 1])\n",
    "    test_acc, test_loss = sess.run(\n",
    "        [acc, loss], feed_dict={inputs: test_xs, labels: dataset.test.labels}\n",
    "    )\n",
    "    return test_acc, test_loss\n",
    "\n",
    "\n",
    "with tf_compat.Graph().as_default() as graph:\n",
    "    batch_size = 1024\n",
    "    steps_per_epoch = int(len(dataset.train.images) / batch_size)\n",
    "    inputs, labels, logits, loss, acc = graph_creator()\n",
    "    global_step = tf_compat.train.get_or_create_global_step()\n",
    "\n",
    "    epoch_ph = tf_compat.placeholder(dtype=tf_compat.float32, name=\"epoch\")\n",
    "    test_loss_ph = tf_compat.placeholder(dtype=tf_compat.float32, name=\"test_loss\")\n",
    "    test_acc_ph = tf_compat.placeholder(dtype=tf_compat.float32, name=\"test_acc\")\n",
    "    tf_compat.summary.scalar(\"Train/epoch\", epoch_ph)\n",
    "    tf_compat.summary.scalar(\"Train/loss\", loss)\n",
    "    tf_compat.summary.scalar(\"Train/accuracy\", acc)\n",
    "    tf_compat.summary.scalar(\"Test/loss\", test_loss_ph)\n",
    "    tf_compat.summary.scalar(\"Test/accuracy\", test_acc_ph)\n",
    "\n",
    "    #######################################################\n",
    "    # Create a manager for recalibration and create the ops\n",
    "    #######################################################\n",
    "    manager = ScheduledModifierManager.from_yaml(config_path)\n",
    "    mod_ops, mod_extras = manager.create_ops(steps_per_epoch, global_step)\n",
    "\n",
    "    train_op = tf_compat.train.AdamOptimizer(learning_rate=1e-4).minimize(\n",
    "        loss, global_step=global_step\n",
    "    )\n",
    "\n",
    "    summaries = tf_compat.summary.merge_all()\n",
    "    tensorboard_path = create_unique_dir(\n",
    "        os.path.join(\".\", \"tensorboard-logs\", notebook_name, model_name)\n",
    "    )\n",
    "    summary_writer = tf_compat.summary.FileWriter(tensorboard_path, sess.graph)\n",
    "\n",
    "    with tf_compat.Session() as sess:\n",
    "        sess.run(tf_compat.global_variables_initializer())\n",
    "        print(\"loading pre-trained weights...\")\n",
    "        ModelRegistry.load_pretrained(\"mnistnet\")\n",
    "        test_acc, test_loss = calc_test_metrics(acc, loss, inputs, labels)\n",
    "\n",
    "        for epoch in auto.tqdm(range(manager.max_epochs), desc=\"training\"):\n",
    "            for batch in range(steps_per_epoch):\n",
    "                batch_xs, batch_ys = dataset.train.next_batch(batch_size)\n",
    "                batch_xs = batch_xs.reshape([-1, 28, 28, 1])\n",
    "                sess.run(train_op, feed_dict={inputs: batch_xs, labels: batch_ys})\n",
    "                sess.run(mod_ops)\n",
    "\n",
    "                # log summaries every 5% of an epoch\n",
    "                if batch % int(steps_per_epoch * 0.05):\n",
    "                    step = sess.run(global_step)\n",
    "                    summary_str = sess.run(\n",
    "                        summaries,\n",
    "                        feed_dict={\n",
    "                            inputs: batch_xs,\n",
    "                            labels: batch_ys,\n",
    "                            test_acc_ph: test_acc,\n",
    "                            test_loss_ph: test_loss,\n",
    "                            epoch_ph: epoch,\n",
    "                        },\n",
    "                    )\n",
    "                    summary_writer.add_summary(summary_str, step)\n",
    "                    summary_writer.flush()\n",
    "\n",
    "            test_acc, test_loss = calc_test_metrics(acc, loss, inputs, labels)\n",
    "\n",
    "        manager.complete_graph()\n",
    "        print(\"final accuracy: {}\".format(test_acc))\n",
    "\n",
    "        checkpoint_path = create_unique_dir(\n",
    "            os.path.join(\".\", notebook_name, model_name, \"checkpoint\")\n",
    "        )\n",
    "        checkpoint_path = os.path.join(checkpoint_path, \"model\")\n",
    "        create_dirs(checkpoint_path)\n",
    "        saver = tf_compat.train.Saver(\n",
    "            tf_compat.get_collection(tf_compat.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        )\n",
    "        saver.save(sess, checkpoint_path)\n",
    "        print(\"saved model checkpoint to {}\".format(checkpoint_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Exporting to ONNX\n",
    "\n",
    "Now that the model is fully recalibrated, you need to export it to an ONNX format, which is the format used by the Neural Magic Inference Engine. For TensorFlow, exporting to ONNX is not natively supported. To add support, you will use the `tf2onnx` Python package. In the cell block below, a convenience class, `GraphExporter()`, is used to handle exporting. It wraps the somewhat complicated API for `tf2onnx` into an easy to use interface.\n",
    "\n",
    "Note, for some configurations, the tf2onnx code does not work properly in a Jupyter Notebook. To remedy this, you should run the `exporter.export_onnx()` function call in a Python console or script.\n",
    "\n",
    "Once the model is saved as an ONNX ﬁle, it is ready to be used for inference with Neural Magic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.utils import create_unique_dir, clean_path, create_dirs\n",
    "from neuralmagicML.tensorflow.utils import GraphExporter\n",
    "\n",
    "export_path = clean_path(os.path.join(\".\", notebook_name, model_name, \"exported\"))\n",
    "exporter = GraphExporter(export_path)\n",
    "\n",
    "with tf_compat.Graph().as_default() as graph:\n",
    "    print('Recreating graph...', flush=True)\n",
    "    inputs, labels, logits, loss, acc = graph_creator()\n",
    "    input_names = [inputs.name]\n",
    "    output_names = [logits.name]\n",
    "\n",
    "    with tf_compat.Session() as sess:\n",
    "        sess.run(tf_compat.global_variables_initializer())\n",
    "        print(\"Restoring previous weights...\", flush=True)\n",
    "        saver = tf_compat.train.Saver(\n",
    "            tf_compat.get_collection(tf_compat.GraphKeys.TRAINABLE_VARIABLES)\n",
    "        )\n",
    "        saver.restore(sess, checkpoint_path)\n",
    "        \n",
    "        print(\"Exporting to pb...\", flush=True)\n",
    "        exporter.export_pb(outputs=[logits])\n",
    "        print(\"Exported pb file to {}\".format(exporter.pb_path), flush=True)\n",
    "        \n",
    "print(\"Exporting to onnx...\", flush=True)\n",
    "exporter.export_onnx(inputs=input_names, outputs=output_names)\n",
    "print(\"Exported onnx file to {}\".format(exporter.onnx_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "Run your model (ONNX file) through the Neural Magic Inference Engine. The following is an example of code that you can run in your Python console. Be sure to enter your ONNX file path and batch size.\n",
    "\n",
    "```\n",
    "from neuralmagic import create_model\n",
    "model = create_model(onnx_file_path=’some/path/to/model.onnx’, batch_size=1)\n",
    "inp = [numpy.random.rand(1, 3, 224, 224).astype(numpy.float32)]\n",
    "out = model.forward(inp)\n",
    "print(out)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
