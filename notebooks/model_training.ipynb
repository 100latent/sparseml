{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training Tutorial\n",
    "\n",
    "Neural networks have set state of the art results across many domains such as computer vision, natural language processing, speech, etc. This tutorial focuses specifically on walking through how to train a model on a computer vision dataset for image classification.\n",
    "\n",
    "We will be training the resnet50 architecture on fast.ai's [Imagenette dataset](https://github.com/fastai/imagenette) provided under the [Apache License 2.0](https://github.com/fastai/imagenette/blob/master/LICENSE). We will walk through exploring the dataset and training the resnet50 model on it. The resulting trained network can be used with our follow up scripts on performance. This baseline for the notebook is given such that it can be run with pytorch CPU or GPU in reasonable times (hours not days). If desired, you can substitute in other model architectures or datasets to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print('Python %s on %s' % (sys.version, sys.platform))\n",
    "\n",
    "package_path = os.path.abspath(os.path.join(os.path.expanduser(os.getcwd()), os.pardir))\n",
    "print(package_path)\n",
    "\n",
    "\"\"\"\n",
    "Adding the path to the neuralmagic-pytorch extension to the path so it isn't necessary to have it installed\n",
    "\"\"\"\n",
    "sys.path.extend([package_path])\n",
    "\n",
    "print('Added current package path to sys.path')\n",
    "print('Be sure to install from requirements.txt and pytorch separately')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "\n",
    "Now we walk through setting up a dataset for training. We default to the Imagenette dataset mentioned above where the original authors, much like ourselves, were interested in a dataset that has similar properties to more complicated datasets such as the Imagenet dataset but would allow rapid iterations. It includes 10 of the easiest classes out of the Imagenet 1000 dataset: tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute.\n",
    "\n",
    "The dataset can easily be changed to the desired dataset in the code given.\n",
    "\n",
    "Below we need to fill in the dataset path which will appear as a text input after running the next block. The dataset is setup to auto download in the specified folder if not found.\n",
    "\n",
    "Additionally we will need to set the train and test batch sizes. The train batch size should generally be kept somewhere between 64 and 256 for proper convergence with stochastic gradient descent. A larger batch size allows us to average our updates over many examples to make sure we do not make a false step in the direction of noise. Too large of a batch size; however, can have a hindering effect as we end up smoothing our steps out too much effectively keeping us from exploring the optimization space. The test batch size, since we will not be updating weights in this mode, should be set to as high as possible for GPUs and does not matter much on pytorch CPU.\n",
    "\n",
    "Finally, we will create three different datasets:\n",
    "- Train dataset -- the data we iterate over and update the models weights on\n",
    "- Validation dataset -- the data we use to make sure we are not overfitting (high variance) the training dataset\n",
    "- Train Test dataset -- this one you may not be used to, we keep it here to evaluate a consistent measurement of performance metrics for the model to check for underfitting (high bias) and overfitting comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import Text, IntSlider, Label, Layout, HBox\n",
    "import torch\n",
    "\n",
    "print('\\nEnter the local path where the dataset can be found')\n",
    "\n",
    "dataset_text = Text(value='', placeholder='Enter local path to dataset', description='Dataset Path')\n",
    "display(dataset_text)\n",
    "\n",
    "print('\\nChoose the batch size to run through the model during train and test runs')\n",
    "print('(press enter if/after inputting manually)')\n",
    "train_batch_size_slider = IntSlider(value=64, min=1, max=256, step=1)\n",
    "labeled_train_batch_size_slider = HBox([Label('Train Batch Size:',layout=Layout(width='100px')),\n",
    "                                        train_batch_size_slider])\n",
    "display(labeled_train_batch_size_slider)\n",
    "\n",
    "test_batch_size_slider = IntSlider(value=64 if torch.cuda.is_available() else 1, min=1, max=256, step=1)\n",
    "labeled_test_batch_size_slider = HBox([Label('Test Batch Size:',layout=Layout(width='100px')),\n",
    "                                       test_batch_size_slider])\n",
    "\n",
    "display(labeled_test_batch_size_slider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.datasets import ImagenetteDataset, EarlyStopDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset_root = os.path.abspath(os.path.expanduser(dataset_text.value.strip()))\n",
    "print('\\nLoading dataset from {}'.format(dataset_root))\n",
    "\n",
    "if not os.path.exists(dataset_root):\n",
    "    raise Exception('Folder must exist for dataset at {}'.format(dataset_root))\n",
    "    \n",
    "train_batch_size = train_batch_size_slider.value\n",
    "test_batch_size = test_batch_size_slider.value\n",
    "\n",
    "print('\\nUsing train batch size of {} and test batch size of {}\\n'\n",
    "      .format(train_batch_size, test_batch_size))\n",
    "    \n",
    "train_dataset = ImagenetteDataset(dataset_root, train=True, rand_trans=True)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4)\n",
    "print('train dataset created: \\n{}\\n'.format(train_dataset))\n",
    "\n",
    "val_dataset = ImagenetteDataset(dataset_root, train=False, rand_trans=False)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=train_batch_size, shuffle=False, num_workers=4)\n",
    "print('validation test dataset created: \\n{}\\n'.format(val_dataset))\n",
    "\n",
    "train_test_dataset = EarlyStopDataset(ImagenetteDataset(dataset_root, train=True, rand_trans=False),\n",
    "                                      early_stop=round(0.1 * len(train_dataset)))\n",
    "train_test_data_loader = DataLoader(train_test_dataset, batch_size=train_batch_size, shuffle=False, num_workers=4)\n",
    "print('train test dataset created: \\n{}\\n'.format(train_test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Visualization\n",
    "\n",
    "It's always a good idea to first visualize and understand the dataset checking for any issues such as noise, class imbalance, etc. \n",
    "\n",
    "Specifically we check the train and validation dataset distributions. For training we want to make sure that all classes have relatively the same number of examples, if one is much less than others the most accurate path for the model will be to never predict it (why not if I can be right 80% of the time by just saying no!?). For validation we want to make sure that accuracy will be a valid measure of our performance. For unbalanced classes, and sometimes in general, we will need a substitute such as class-balanced accuracy, precision, recall, FScore, or AUC - ROC curves.\n",
    "\n",
    "Additionally we visually inspect a random sample from the dataset to make sure the images represent what we would expect - classes are correct, lack of noise, etc. This helps us make sure that our data has a learnable distribution for the model. If it does not, we'll fail to converge and will need to go back and recollect the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from collections import OrderedDict\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import utils, transforms\n",
    "\n",
    "\n",
    "def plot_tensors_as_image(tensors, num_columns=10):\n",
    "    inverse_transform = transforms.Compose([\n",
    "        transforms.Normalize(mean=[0, 0, 0], std=[1 / 0.229, 1 / 0.224, 1 / 0.225]),\n",
    "        transforms.Normalize(mean=[-1 * 0.485, -1 * 0.456, -1 * 0.406], std=[1, 1, 1])\n",
    "    ])\n",
    "    image = tensors\n",
    "    image = utils.make_grid(image, nrow=num_columns)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "    image = inverse_transform(image)\n",
    "    image = image.numpy()\n",
    "    plt.imshow(numpy.transpose(image, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "classes_map = OrderedDict({\n",
    "    0: 'tench', 1: 'english springer', 2: 'cassette player',\n",
    "    3: 'chain saw', 4: 'church', 5: 'french horn',\n",
    "    6: 'garbage truck', 7: 'gas pump', 8: 'golf ball', 9: 'parachute'\n",
    "})\n",
    "\n",
    "print('Creating stats for validation dataset...')\n",
    "val_classes_counts = OrderedDict({clazz: 0 for clazz in classes_map.values()})\n",
    "val_samples = OrderedDict({clazz: [] for clazz in classes_map.values()})\n",
    "for images, labels in val_data_loader:\n",
    "    for image, label in zip(images, labels):\n",
    "        label = label.item()\n",
    "        clazz = classes_map[label]\n",
    "        val_classes_counts[clazz] += 1\n",
    "        \n",
    "        if len(val_samples[clazz]) < 10:\n",
    "            val_samples[clazz].append(image)\n",
    "        \n",
    "print('\\nValidation dataset class counts')\n",
    "for clazz, clazz_count in val_classes_counts.items():\n",
    "    print('{}: {}'.format(clazz, clazz_count))\n",
    "\n",
    "print('\\nSample data plot')\n",
    "plot_tensors_as_image(\n",
    "    tensors=torch.stack([samp for class_samples in val_samples.values() for samp in class_samples]),\n",
    "    num_columns=10\n",
    ")\n",
    "\n",
    "print('\\nCreating stats for training dataset...')\n",
    "train_classes_counts = OrderedDict({clazz: 0 for clazz in classes_map.values()})\n",
    "for images, labels in train_data_loader:\n",
    "    for image, label in zip(images, labels):\n",
    "        label = label.item()\n",
    "        clazz = classes_map[label]\n",
    "        train_classes_counts[clazz] += 1\n",
    "        \n",
    "print('\\nTrain dataset class counts')\n",
    "for clazz, clazz_count in train_classes_counts.items():\n",
    "    print('{}: {}'.format(clazz, clazz_count))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "For this exercise we'll create the standard [ResNet50 model](https://arxiv.org/abs/1512.03385) as it is simpler to visualize / understand and has good tradeoffs between accuracy and complexity of the model. The base implementation is set up for the imagenet dataset with 1000 classes. We'll be using the 10 classes in our dataset.\n",
    "\n",
    "If you changed the dataset in the above cell then we'll need to update the number of classes to create the model  appropriately.\n",
    "\n",
    "Additionally run the code block and select the device to run on before continuing. cpu runs in the pytorch cpu framework and cuda runs on an attached GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from neuralmagicML.models import resnet50\n",
    "\n",
    "num_classes = 10\n",
    "model = resnet50(num_classes=num_classes)\n",
    "model_id = '{}-{}'.format(model.__class__.__name__,\n",
    "                          datetime.datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "                              .replace('-', '.').replace(':', '.'))\n",
    "print('Created model {}'.format(model.__class__.__name__))\n",
    "\n",
    "print('\\nChoose the device to run on')\n",
    "device_choice = widgets.ToggleButtons(\n",
    "    options=['cuda', 'cpu'] if torch.cuda.is_available() else ['cpu'],\n",
    "    description='Device'\n",
    ")\n",
    "display(device_choice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss, Metrics, Optimizer, and Logging\n",
    "\n",
    "Below we go through setting up the loss function, metrics, optimizer, and additional logging. \n",
    "\n",
    "For the loss function we will use [cross entropy](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html) as is standard for classification tasks. For additional metrics we will use the top 1 accuracy, ie did we predict the class correctly or not. We do this because there are only 10 classes available so top N accuracy is generally uninformative with so few classes. We use a custom wrapper class to organize the metrics and loss into one, callable class.\n",
    "\n",
    "For the optimizer we will use [SGD + nesterov momentum](https://towardsdatascience.com/stochastic-gradient-descent-with-momentum-a84097641a5d) with learning rate decay. The parameters for learning rate and learning rate decay were found over multiple training runs and perform well for ResNet50 and the Imagenette dataset. If working with a different dataset it may be a good idea to run a grid search to figure out the proper hyperparameters. Additionally, [Adam optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) may be substituted in at the cost of a little bit of validation accuracy.\n",
    "\n",
    "Finally, beyond the usual basic screen-printouts let's use [tensorboard's](https://www.tensorflow.org/guide/summaries_and_tensorboard) nice logging capabilities. We'll primarily track scalars such as the loss and accuracy throughout training in this example. We use [tensorboardX](https://tensorboardx.readthedocs.io/en/latest/tensorboard.html) to log from pytorch into tensorboard. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from tensorboardX import SummaryWriter\n",
    "from neuralmagicML.utils import TopKAccuracy, CrossEntropyLossWrapper\n",
    "\n",
    "\n",
    "device = device_choice.value\n",
    "model = model.to(device)\n",
    "print('\\nRunning on device {}'.format(device))\n",
    "\n",
    "### optimizer and learning rate definitions\n",
    "init_learning_rate = 0.1\n",
    "lr_decay = 0.2 # divide lr by 5 every 'lr_decay_rate' epochs\n",
    "lr_decay_rate = 20\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "###\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    lr = init_learning_rate * (lr_decay ** ((epoch + 1) // lr_decay_rate))\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "    return lr\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), init_learning_rate, momentum=momentum,\n",
    "                      weight_decay=weight_decay, nesterov=True)\n",
    "print('\\nCreated optimizer with initial lr: {}, momentum: {}, weight decay: {}'\n",
    "      .format(init_learning_rate, momentum, weight_decay))\n",
    "\n",
    "loss = CrossEntropyLossWrapper(extras={'top1acc': TopKAccuracy(1)})\n",
    "print('\\nCreated loss wrapper\\n{}'.format(loss))\n",
    "\n",
    "logs_dir = os.path.abspath(os.path.expanduser(os.path.join('.', 'model_training_logs', model_id)))\n",
    "\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "\n",
    "writer = SummaryWriter(logdir=logs_dir, comment='imagenette training')\n",
    "print('\\nCreated summary writer logging to \\n{}'.format(logs_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "Now that our dataset, model, and optimizer are all set up we can begin training! The training cycle will be broken down into the following:\n",
    "\n",
    "for number of epochs (an epoch is once through the training dataset):\n",
    "   - train model over full training dataset (one epoch); update weights\n",
    "   - test model over full validation dataset; no weight update\n",
    "   - test model over sampled training dataset; no weight update\n",
    "\n",
    "In the below code blocks, we create self-contained convenience functions for running the train and testing loops. These convenience functions are then called to train the model. At the end of the script we save the trained model in our current location with the date included as well as the final validation accuracy in the name.\n",
    "\n",
    "Note, we have additionally created a logs directory which, in combination with tensorboard, can be used to visualize the progress of the model. To launch tensorboard use the following command from within the notebooks directory:\n",
    "'tensorboard --logdir model_training_logs --port 6006'\n",
    "Now you will have an interactive dashboard running on [http://localhost:6006](http://localhost:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from neuralmagicML.models import save_model\n",
    "\n",
    "\n",
    "def test_epoch(model, data_loader, loss, device, epoch):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "            y_lab = y_lab.to(device)\n",
    "            x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "            batch_size = y_lab.shape[0]\n",
    "            y_pred = model(*x_feature)\n",
    "            losses = loss(x_feature, y_lab, y_pred)\n",
    "            \n",
    "            for key, val in losses.items():\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "                result = val.detach_().cpu()\n",
    "                result = result.repeat(batch_size)\n",
    "                results[key].append(result)\n",
    "                \n",
    "    return results\n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, loss, device, epoch, writer):\n",
    "    model.train()\n",
    "    init_batch_size = None\n",
    "    batches_per_epoch = len(data_loader)\n",
    "    \n",
    "    for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "        y_lab = y_lab.to(device)\n",
    "        x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "        batch_size = y_lab.shape[0]\n",
    "        if init_batch_size is None:\n",
    "            init_batch_size = batch_size\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(*x_feature)\n",
    "        losses = loss(x_feature, y_lab, y_pred)\n",
    "        losses['loss'].backward()\n",
    "        optimizer.step(closure=None)\n",
    "        \n",
    "        step_count = init_batch_size * (epoch * batches_per_epoch + batch)\n",
    "        for _loss, _value in losses.items():\n",
    "            writer.add_scalar('Train/{}'.format(_loss), _value.item(), step_count)\n",
    "            \n",
    "print('Training model')\n",
    "num_epochs = 90\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}'.format(epoch))\n",
    "    lr = adjust_learning_rate(optimizer, epoch)\n",
    "    writer.add_scalar('Train/Learning Rate', lr, epoch)\n",
    "    train_epoch(model, train_data_loader, optimizer, loss, device, epoch, writer)\n",
    "    \n",
    "    print('Completed training for epoch {}, testing validation dataset'.format(epoch))\n",
    "    val_losses = test_epoch(model, val_data_loader, loss, device, epoch)\n",
    "    for _loss, _values in val_losses.items():\n",
    "        _value = torch.mean(torch.cat(_values))\n",
    "        last_value = _value\n",
    "        writer.add_scalar('Test/Validation/{}'.format(_loss), _value, epoch)\n",
    "        print('{}: {}'.format(_loss, _value))\n",
    "        \n",
    "    print('Completed testing validation dataset for epoch {}, testing training dataset'.format(epoch))\n",
    "    train_losses = test_epoch(model, train_test_data_loader, loss, device, epoch)\n",
    "    for _loss, _values in train_losses.items():\n",
    "        _value = torch.mean(torch.cat(_values))\n",
    "        writer.add_scalar('Test/Training/{}'.format(_loss), _value, epoch)\n",
    "        print('{}: {}'.format(_loss, _value))\n",
    "        \n",
    "    print('Completed testing training dataset for epoch {}'.format(epoch))\n",
    "    \n",
    "scalars_json_path = os.path.join(logs_dir, 'all_scalars.json')\n",
    "writer.export_scalars_to_json(scalars_json_path)\n",
    "writer.close()\n",
    "\n",
    "save_path = os.path.abspath(os.path.expanduser(os.path.join('.', '{}-{}.pth'.format(model_id, last_value))))\n",
    "print('Finished training, saving model to {}'.format(save_path))\n",
    "save_model(save_path, model, optimizer, epoch)\n",
    "print('Saved model')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
