{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "© 2020 Neuralmagic, Inc., Confidential // Neural Magic Evaluation License Agreement\n",
    "# Post Training Quantization with an ONNX Model\n",
    "\n",
    "This notebook provides an easy step-by-step walkthrough for performing post training quantization on an ONNX model using tools in the neuralmagicML package. You will:\n",
    "- Set up the environment\n",
    "- Download an example ONNX model\n",
    "- Use `neuralmagicML.onnx.quantization` to run the post training quantization for the model\n",
    "- Quantize the model and save it to a new ONNX file\n",
    "- Validate the accuracy of your new quantized model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Setting Up the Environment\n",
    "\n",
    "In this step, Neural Magic checks your environment setup to ensure the rest of the notebook will flow smoothly.\n",
    "Before running, install the neuralmagicML package into the system using the following at the parent of the package directory:\n",
    "\n",
    "`pip install neuralmagicML-python/ `\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_name = \"quantize_post_training_onnx\"\n",
    "print(\"checking setup for {}...\".format(notebook_name))\n",
    "\n",
    "# filter because of tensorboard future warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "try:\n",
    "    # make sure neuralmagicML is installed\n",
    "    import neuralmagicML\n",
    "except Exception as ex:\n",
    "    raise Exception(\n",
    "        \"please install neuralmagicML using the setup.py file before continuing\"\n",
    "    )\n",
    "\n",
    "# torch is required for loading the MNIST dataset\n",
    "from neuralmagicML.utilsnb import check_pytorch_notebook_setup\n",
    "check_pytorch_notebook_setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Downloading a Sample ONNX Model\n",
    "To run the quantization example, you will need an ONNX model to quantize.  The cell below contains code to download an ONNX encoded mnist model from the Neural Magic Model Repo.  To quantize another model, point the code to your own ONNX file, or to another ONNX model from the Neural Magic Model Repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from neuralmagicML.utils import available_models, RepoModel, clean_path\n",
    "\n",
    "save_dir = clean_path(os.path.join(\".\", notebook_name, \"mnist\"))\n",
    "\n",
    "model = [model for model in available_models() if model.arch_display == 'MnistNet'][0]\n",
    "\n",
    "model_path = model.download_onnx_file(save_dir=save_dir)\n",
    "print(\"Model downloaded to {}\".format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Load a Calibration Dataset\n",
    "To quantize your model, you will need a dataset that is representitive of model inputs to perform calibration.  The neuralmagicML post training quantization tool takes a `DataLoader` (`neuralmagicML.onnx.utils.data.DataLoader`) that will be used to calibrate the model.\n",
    "\n",
    "The cell below downloads an Mnist validation dataset from the PyTorch datasets repo to use as your calibration dataset and loads it into a DataLoader object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from neuralmagicML.pytorch.datasets import MNISTDataset\n",
    "from neuralmagicML.onnx.utils import DataLoader\n",
    "\n",
    "mnist_dataset = MNISTDataset(root=save_dir, train=False)\n",
    "\n",
    "# Format every data point into a dictionary of input name to array\n",
    "input_dict = [{\"input\": img.numpy()} for (img, _) in mnist_dataset]\n",
    "data_loader = DataLoader(input_dict, None, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Quantize the Model\n",
    "Run the code below to use the `quantize_model_post_training` function from `neuralmagicML.onnx.quantization` to save a quantized version of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.onnx.quantization import quantize_model_post_training\n",
    "\n",
    "quantized_model_path = os.path.join(save_dir, \"model-quant.onnx\")\n",
    "\n",
    "print(\"Calibrating...\")\n",
    "\n",
    "quantize_model_post_training(\n",
    "    model_path,\n",
    "    data_loader,\n",
    "    output_model_path=quantized_model_path,\n",
    "    static=True\n",
    ")\n",
    "\n",
    "print(\"Quantized model saved to {}\".format(quantized_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Validate the Quantized Model\n",
    "After quantizing your model, it is important to check its performance.  Run the following code to test the quantized model against the MNIST validation dataset.  You should see the model still has accuracy upwards of 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from neuralmagicML.onnx.utils import ORTModelRunner\n",
    "\n",
    "print(\"Validating Quantized MNIST Model...\")\n",
    "\n",
    "onnx_inference_session = ORTModelRunner(quantized_model_path, batch_size=1)\n",
    "correct_predictions = 0\n",
    "\n",
    "# Reload data_loader\n",
    "labels = [{\"output\": np.array(label)} for _, label in mnist_dataset]\n",
    "data_loader = DataLoader(input_dict, labels, 1)\n",
    "for i, (batch, label) in enumerate(tqdm(data_loader)):\n",
    "    outputs, _ = onnx_inference_session.batch_forward(batch)\n",
    "    prediction = np.argmax(outputs[\"output_0\"])\n",
    "    if prediction == label[\"output\"]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "print(\n",
    "    \"{} / {} samples correctly labeled\".format(\n",
    "        correct_predictions, len(mnist_dataset)\n",
    "    )\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations - You have completed the Post Training Quantization Notebook\n",
    "### Next Step\n",
    "\n",
    "Run your model (ONNX file) through the Neural Magic Inference Engine. The following is an example of code that you can run in your Python console. Be sure to enter your ONNX file path and batch size.\n",
    "\n",
    "```\n",
    "from neuralmagic import create_model\n",
    "model = create_model(onnx_file_path=’some/path/to/model.onnx’, batch_size=1)\n",
    "out = model.forward(input_batch)\n",
    "print(out)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
