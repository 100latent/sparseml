{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime\n",
    "import ipywidgets as widgets\n",
    "import torch\n",
    "\n",
    "print('Python %s on %s' % (sys.version, sys.platform))\n",
    "\n",
    "package_path = os.path.abspath(os.path.join(os.path.expanduser(os.getcwd()), os.pardir))\n",
    "print(package_path)\n",
    "\n",
    "\"\"\"\n",
    "Adding the path to the neuralmagic-pytorch extension to the path so it isn't necessary to have it installed\n",
    "\"\"\"\n",
    "sys.path.extend([package_path])\n",
    "\n",
    "print('Added current package path to sys.path')\n",
    "print('Be sure to install from requirements.txt and pytorch separately')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nEnter the local path where the dataset can be found')\n",
    "\n",
    "dataset_text = widgets.Text(value='~/datasets/imagenette', placeholder='Enter local path to dataset', description='Dataset Path')\n",
    "display(dataset_text)\n",
    "\n",
    "print('\\nChoose the batch size to run through the model during train and test runs')\n",
    "print('(be sure to press enter if/after inputting manually)')\n",
    "train_batch_size_slider = widgets.IntSlider(\n",
    "    value=256, min=1, max=1024, step=1, description='Train Batch Size:'\n",
    ")\n",
    "display(train_batch_size_slider)\n",
    "test_batch_size_slider = widgets.IntSlider(\n",
    "    value=256 if torch.cuda.is_available() else 1, min=1, max=1024, step=1, description='Test Batch Size:'\n",
    ")\n",
    "display(test_batch_size_slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.datasets import ImagenetteDataset, ImageNetDataset, EarlyStopDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset_root = os.path.abspath(os.path.expanduser(dataset_text.value))\n",
    "print('\\nLoading dataset from {}'.format(dataset_root))\n",
    "\n",
    "if not os.path.exists(dataset_root):\n",
    "    raise Exception('Folder must exist for dataset at {}'.format(dataset_root))\n",
    "    \n",
    "train_batch_size = train_batch_size_slider.value\n",
    "test_batch_size = test_batch_size_slider.value\n",
    "\n",
    "print('\\nUsing train batch size of {} and test batch size of {}\\n'\n",
    "      .format(train_batch_size, test_batch_size))\n",
    "\n",
    "train_dataset = ImagenetteDataset(dataset_root, train=True, rand_trans=True)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4)\n",
    "print('train dataset created: \\n{}\\n'.format(train_dataset))\n",
    "\n",
    "val_dataset = ImagenetteDataset(dataset_root, train=False, rand_trans=False)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=test_batch_size, shuffle=False, num_workers=4)\n",
    "print('validation test dataset created: \\n{}\\n'.format(val_dataset))\n",
    "\n",
    "train_test_dataset = EarlyStopDataset(ImagenetteDataset(dataset_root, train=True, rand_trans=False),\n",
    "                                      early_stop=len(val_dataset) if len(val_dataset) > 1000 else round(0.1 * len(train_dataset)))\n",
    "train_test_data_loader = DataLoader(train_test_dataset, batch_size=test_batch_size, shuffle=False, num_workers=4)\n",
    "print('train test dataset created: \\n{}\\n'.format(train_test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.models import resnet50\n",
    "\n",
    "num_classes = 10 if isinstance(train_dataset, ImagenetteDataset) else 1000\n",
    "pretrained = 'imagenette/dense' if isinstance(train_dataset, ImagenetteDataset) else True\n",
    "model = resnet50(num_classes=num_classes, pretrained=pretrained)\n",
    "model_id = '{}-{}'.format(model.__class__.__name__,\n",
    "                          datetime.datetime.today().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "                              .replace('-', '.').replace(':', '.'))\n",
    "print('Created model {}'.format(model.__class__.__name__))\n",
    "\n",
    "print('\\nSet the model id')\n",
    "model_id_text = widgets.Text(\n",
    "    value=model_id\n",
    ")\n",
    "display(model_id_text)\n",
    "\n",
    "print('\\nChoose the device to run on')\n",
    "device_choice = widgets.ToggleButtons(\n",
    "    options=['cuda', 'cpu'] if torch.cuda.is_available() else ['cpu'],\n",
    "    description='Device'\n",
    ")\n",
    "display(device_choice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = model_id_text.value\n",
    "device = device_choice.value\n",
    "\n",
    "print('\\nchoose which tensors to regularize: the inputs or outputs to each conv layer')\n",
    "reg_tensor_choice = widgets.ToggleButtons(\n",
    "    options=['inp', 'out'],\n",
    "    description='reg tens'\n",
    ")\n",
    "display(reg_tensor_choice)\n",
    "\n",
    "print('\\nchoose which regularization function to use: l1, l2, relu for the tensors')\n",
    "reg_func_choice = widgets.ToggleButtons(\n",
    "    options=['l1', 'l2', 'relu']\n",
    ")\n",
    "display(reg_func_choice)\n",
    "\n",
    "print('\\nchoose the alpha value to use for regularization of the activation values')\n",
    "alpha_slider = widgets.FloatLogSlider(\n",
    "    value=0.0001, min=-9, max=-1, step=0.0001, description='alpha'\n",
    ")\n",
    "display(alpha_slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.utils import lr_analysis, lr_analysis_figure, CrossEntropyLossWrapper\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### optimizer definitions\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "###\n",
    "\n",
    "# print('\\nrunning learning rate analysis...')\n",
    "# batches_per_sample = round(500 / train_batch_size)  # make sure we have enough sample points per learning rate\n",
    "# analysis = lr_analysis(model, device, train_data_loader, CrossEntropyLossWrapper(), batches_per_sample,\n",
    "#                        init_lr=1e-7, final_lr=1e0, sgd_momentum=momentum, sgd_weight_decay=weight_decay)\n",
    "# lr_analysis_figure(analysis)\n",
    "# plt.show()\n",
    "\n",
    "print('\\nselect the initial learning rate')\n",
    "lr_slider = widgets.FloatLogSlider(\n",
    "    value=0.01, min=-7, max=1, step=0.0001, description='init lr'\n",
    ")\n",
    "display(lr_slider)\n",
    "\n",
    "print('\\nselect the number of epochs to train for')\n",
    "finalize_epochs_text = widgets.IntText(value=30, description='num epochs')\n",
    "display(finalize_epochs_text)\n",
    "\n",
    "print('\\nselect the final learning rate')\n",
    "lr_final_slider = widgets.FloatLogSlider(\n",
    "    value=0.001, min=-7, max=1, step=0.0001, description='final lr'\n",
    ")\n",
    "display(lr_final_slider)\n",
    "\n",
    "print('\\nselect the number of exponential updates to apply to the learning rate over the epochs')\n",
    "lr_updates_slider = widgets.IntText(value=3, description='lr updates')\n",
    "display(lr_updates_slider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.sparsity import (\n",
    "    ASAnalyzerLayer, ASAnalyzerModule, ASRegModifier, LearningRateModifier,\n",
    "    ScheduledModifierManager, ScheduledOptimizer\n",
    ")\n",
    "from neuralmagicML.utils import TopKAccuracy\n",
    "from tensorboardX import SummaryWriter\n",
    "from torch.nn.modules.conv import _ConvNd\n",
    "from torch.nn import Linear\n",
    "from torch import optim\n",
    "\n",
    "reg_tens = reg_tensor_choice.value\n",
    "reg_func = reg_func_choice.value\n",
    "alpha = alpha_slider.value\n",
    "\n",
    "print('using AS reg params of reg_tens:{} reg_func:{} and alpha:{}'\n",
    "     .format(reg_tens, reg_func, alpha))\n",
    "\n",
    "lr_init = lr_slider.value\n",
    "lr_final = lr_final_slider.value\n",
    "epochs = finalize_epochs_text.value\n",
    "lr_updates = lr_updates_slider.value\n",
    "lr_update_freq = epochs / (lr_updates + 1.0)\n",
    "lr_gamma = (lr_final / lr_init) ** (1 / lr_updates)\n",
    "print('using lr params of init:{} final:{} epochs:{} updates:{} update_freq:{} gamma:{}'\n",
    "      .format(lr_init, lr_final, epochs, lr_updates, lr_update_freq, lr_gamma))\n",
    "\n",
    "lr_modifier = LearningRateModifier(lr_class='ExponentialLR', lr_kwargs={'gamma': lr_gamma},\n",
    "                                   start_epoch=0.0, end_epoch=epochs,\n",
    "                                   update_frequency=lr_update_freq)\n",
    "modify_layers = [name for name, mod in model.named_modules() if isinstance(mod, _ConvNd)]\n",
    "\n",
    "# remove the first conv if we are working on the input to each conv\n",
    "if reg_tens == 'inp':\n",
    "    modify_layers = modify_layers[1:]\n",
    "# remove the last conv if we are working on the output from each conv\n",
    "elif reg_tensor == 'out':\n",
    "    modify_layers = modify_layers[:-1]\n",
    "\n",
    "as_reg_modifier = ASRegModifier(modify_layers, alpha, reg_func, reg_tens, start_epoch=0.0)\n",
    "\n",
    "modifier_manager = ScheduledModifierManager([lr_modifier, as_reg_modifier])\n",
    "print('\\nCreated ScheduledModifierManager with exponential lr_modifier with gamma {} and AS reg modifier'\n",
    "      .format(lr_gamma))\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr_slider.value, momentum=momentum,\n",
    "                      weight_decay=weight_decay, nesterov=True)\n",
    "optimizer = ScheduledOptimizer(optimizer, model, modifier_manager, steps_per_epoch=len(train_dataset))\n",
    "print('\\nCreated scheudled optimizer with initial lr: {}, momentum: {}, weight decay: {}'\n",
    "      .format(lr_slider.value, momentum, weight_decay))\n",
    "\n",
    "loss = CrossEntropyLossWrapper(extras={'top1acc': TopKAccuracy(1)})\n",
    "print('\\nCreated loss wrapper\\n{}'.format(loss))\n",
    "\n",
    "logs_dir = os.path.abspath(os.path.expanduser(os.path.join('.', 'model_training_logs', model_id)))\n",
    "\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "\n",
    "writer = SummaryWriter(logdir=logs_dir, comment='imagenette training')\n",
    "print('\\nCreated summary writer logging to \\n{}'.format(logs_dir))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import math\n",
    "from neuralmagicML.models import save_model\n",
    "\n",
    "\n",
    "def test_epoch(model, data_loader, loss, device, epoch):\n",
    "    model.eval()\n",
    "    results = {}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "            y_lab = y_lab.to(device)\n",
    "            x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "            batch_size = y_lab.shape[0]\n",
    "            y_pred = model(*x_feature)\n",
    "            losses = loss(x_feature, y_lab, y_pred)\n",
    "            \n",
    "            for key, val in losses.items():\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "                result = val.detach_().cpu()\n",
    "                result = result.repeat(batch_size)\n",
    "                results[key].append(result)\n",
    "                \n",
    "    return results\n",
    "\n",
    "def test_epoch_writer(model, data_loader, loss, device, epoch, writer, key):\n",
    "    losses = test_epoch(model, data_loader, loss, device, epoch)\n",
    "    \n",
    "    for loss, values in losses.items():\n",
    "        val = torch.mean(torch.cat(values))\n",
    "        writer.add_scalar(key.format(loss), val, epoch)\n",
    "        print('{}: {}'.format(loss, val))\n",
    "        \n",
    "def test_as_values(as_model, data_loader, device, epoch, writer, sample_size=1000):\n",
    "    as_model.eval()\n",
    "    as_model.clear_layers()\n",
    "    as_model.enable_layers()\n",
    "    sample_count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "            y_lab = y_lab.to(device)\n",
    "            x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "            batch_size = y_lab.shape[0]\n",
    "            y_pred = model(*x_feature)\n",
    "            sample_count += batch_size\n",
    "            \n",
    "            if sample_count >= sample_size:\n",
    "                break\n",
    "        \n",
    "    as_model.disable_layers()\n",
    "    \n",
    "    for name, layer in as_model.layers.items():\n",
    "        writer.add_scalar('Act Sparsity/{}'.format(name), layer.inputs_sparsity_mean, epoch)\n",
    "    \n",
    "    as_model.clear_layers()\n",
    "            \n",
    "\n",
    "def train_epoch(model, data_loader, optimizer, loss, device, epoch, writer):\n",
    "    model.train()\n",
    "    init_batch_size = None\n",
    "    batches_per_epoch = len(data_loader)\n",
    "    \n",
    "    for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "        y_lab = y_lab.to(device)\n",
    "        x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "        batch_size = y_lab.shape[0]\n",
    "        if init_batch_size is None:\n",
    "            init_batch_size = batch_size\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(*x_feature)\n",
    "        losses = loss(x_feature, y_lab, y_pred)\n",
    "        losses['loss'] = optimizer.loss_update(losses['loss']) # update loss with the AS modifier regularization\n",
    "        losses['loss'].backward()\n",
    "        optimizer.step(closure=None)\n",
    "        \n",
    "        step_count = init_batch_size * (epoch * batches_per_epoch + batch)\n",
    "        for _loss, _value in losses.items():\n",
    "            writer.add_scalar('Train/{}'.format(_loss), _value.item(), step_count)\n",
    "            writer.add_scalar('Train/Learning Rate', optimizer.learning_rate, step_count)\n",
    "            \n",
    "print('Training model...')\n",
    "\n",
    "analyzer_model = ASAnalyzerModule(\n",
    "    model, [ASAnalyzerLayer(name, division=0, track_inputs_sparsity=True)\n",
    "            for name, mod in model.named_modules() if isinstance(mod, _ConvNd) or isinstance(mod, Linear)]\n",
    ")\n",
    "print('\\nCreated AS analyzer module')\n",
    "\n",
    "print('Running initial validation values for later comparison')\n",
    "# test_epoch_writer(model, val_data_loader, loss, device, -1, writer, 'Test/Validation/{}')\n",
    "# test_as_values(analyzer_model, val_data_loader, device, -1, writer)\n",
    "            \n",
    "for epoch in tqdm(range(math.ceil(modifier_manager.max_epochs))):\n",
    "    print('Starting epoch {}'.format(epoch))\n",
    "    optimizer.epoch_start()\n",
    "    train_epoch(model, train_data_loader, optimizer, loss, device, epoch, writer)\n",
    "    \n",
    "    print('Completed training for epoch {}, testing validation dataset'.format(epoch))\n",
    "    test_epoch_writer(model, val_data_loader, loss, device, epoch, writer, 'Test/Validation/{}')\n",
    "    \n",
    "    print('Completed testing validation dataset for epoch {}, testing training dataset'.format(epoch))\n",
    "    test_epoch_writer(model, train_test_data_loader, loss, device, epoch, writer, 'Test/Training/{}')\n",
    "    \n",
    "    print('Completed testing validation dataset for epoch {}, testing activation sparsity'.format(epoch))\n",
    "    test_as_values(analyzer_model, val_data_loader, optimizer, loss, device, epoch, writer)\n",
    "        \n",
    "    optimizer.epoch_end()\n",
    "    \n",
    "save_path = os.path.abspath(os.path.expanduser(os.path.join('.', '{}.pth'.format(model_id))))\n",
    "print('Finished training, saving model to {}'.format(pruned_save_path))\n",
    "save_model(save_path, model, optimizer, epoch)\n",
    "print('Saved model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
