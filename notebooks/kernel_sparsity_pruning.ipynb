{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Sparsity via Pruning Tutorial\n",
    "\n",
    "Neural networks, in general, are very overparamatarized for given tasks (ie the number of parameters far exceeds the number of training points) yet still they [generalize very well](https://arxiv.org/abs/1611.03530). This flies against conventional wisdom where overparamatarizing a model will lead to overfitting and puting theory behind this empirical evidence is a very active area of research.\n",
    "\n",
    "Additionally, [early on](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf) it was discovered that large numbers of weights in neural networks could be pruned away (set to 0) without affecting the loss and in most cases actually improving the generalization capability of the network. This work was reinvigorated with Song Han's [2015 paper](https://arxiv.org/abs/1510.00149) in pursuit of compressing model size for mobile applications. This has resulted in numerous papers coming out on the topic of weight pruning, filter pruning, channel pruning, and ultimately block pruning. [This paper](https://arxiv.org/abs/1902.09574) out of Google gives a good overview of the current state of sparsity.\n",
    "\n",
    "Given that models are very overparamatarized and large numbers of weights can be effectively pruned away, what does this leave us with? Well intuitively, then, we can think of pruning as performing an [architecture search](https://openreview.net/pdf?id=rJlnB3C5Ym) within this large, traditionally fixed weight space. What was originally important in the dense model was representing a large number of pathways for optimization. We can then effectively remove the unused pathways in the optimization space with a fine toothed comb.\n",
    "\n",
    "Well what does pruning get us? We now have a model with a lot of multiplications by zero that we don't need to run. If we're smart about how we do structure this compute (a suprisingly trickly problem), we can now run the model way faster than ever thought possible! That's where the [Neural Magic](http://neuralmagic.com/) engine can help us.\n",
    "\n",
    "This tutorial provides a step by step walk through for pruning an already trained (dense) model. Specifically it is setup to work with the model trained in our [model training tutorial](model_training.ipynb), but it can be changed to support other models/datasets as needed:\n",
    "1. Dataset selection\n",
    "2. Model selection and loading\n",
    "3. Pruning setup\n",
    "4. Recalibration using pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print('Python %s on %s' % (sys.version, sys.platform))\n",
    "\n",
    "package_path = os.path.abspath(os.path.join(os.path.expanduser(os.getcwd()), os.pardir))\n",
    "print(package_path)\n",
    "\n",
    "\"\"\"\n",
    "Adding the path to the neuralmagic-pytorch extension to the path so it isn't necessary to have it installed\n",
    "\"\"\"\n",
    "sys.path.extend([package_path])\n",
    "\n",
    "print('Added current package path to sys.path')\n",
    "print('Be sure to install from requirements.txt and pytorch separately')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "\n",
    "We are using fast.ai's [Imagenette dataset](https://github.com/fastai/imagenette) provided under the [Apache License 2.0](https://github.com/fastai/imagenette/blob/master/LICENSE) as the default dataset. The original authors, much like ourselves, were interested in a dataset that has similar properties to more complicated datasets such as the Imagenet dataset but one that would allow rapid iterations. It includes 10 of the easiest classes out of the Imagenet 1000 dataset: tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute. If you are interested in visualizing the properties in this dataset see our [model training tutorial](model_training.ipynb) which also gives a more in depth breakdown for what batch size to use and the dataset splits.\n",
    "\n",
    "The dataset can easily be changed to the desired dataset in the code given.\n",
    "\n",
    "Below we will need to fill in the dataset path, train batch size, and test batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "import torch\n",
    "\n",
    "print('\\nEnter the local path where the dataset can be found')\n",
    "\n",
    "dataset_text = widgets.Text(value='', placeholder='Enter local path to dataset', description='Dataset Path')\n",
    "display(dataset_text)\n",
    "\n",
    "print('\\nChoose the batch size to run through the model during train and test runs')\n",
    "print('(press enter if/after inputting manually)')\n",
    "train_batch_size_slider = widgets.IntSlider(\n",
    "    value=64, min=1, max=256, step=1, description='Train Batch Size:'\n",
    ")\n",
    "display(train_batch_size_slider)\n",
    "test_batch_size_slider = widgets.IntSlider(\n",
    "    value=64 if torch.cuda.is_available() else 1, min=1, max=256, step=1, description='Test Batch Size:'\n",
    ")\n",
    "display(test_batch_size_slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.datasets import ImagenetteDataset, EarlyStopDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset_root = os.path.abspath(os.path.expanduser(dataset_text.value.strip()))\n",
    "print('\\nLoading dataset from {}'.format(dataset_root))\n",
    "\n",
    "if not os.path.exists(dataset_root):\n",
    "    raise Exception('Folder must exist for dataset at {}'.format(dataset_root))\n",
    "    \n",
    "train_batch_size = train_batch_size_slider.value\n",
    "test_batch_size = test_batch_size_slider.value\n",
    "\n",
    "print('\\nUsing train batch size of {} and test batch size of {}\\n'\n",
    "      .format(train_batch_size, test_batch_size))\n",
    "    \n",
    "train_dataset = ImagenetteDataset(dataset_root, train=True, rand_trans=True)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4)\n",
    "print('train dataset created: \\n{}\\n'.format(train_dataset))\n",
    "\n",
    "val_dataset = ImagenetteDataset(dataset_root, train=False, rand_trans=False)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=train_batch_size, shuffle=False, num_workers=4)\n",
    "print('validation test dataset created: \\n{}\\n'.format(val_dataset))\n",
    "\n",
    "train_test_dataset = EarlyStopDataset(ImagenetteDataset(dataset_root, train=True, rand_trans=False),\n",
    "                                      early_stop=len(val_dataset))\n",
    "train_test_data_loader = DataLoader(train_test_dataset, batch_size=train_batch_size, shuffle=False, num_workers=4)\n",
    "print('train test dataset created: \\n{}\\n'.format(train_test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Loading\n",
    "\n",
    "For this exercise we'll create the standard [ResNet50 model](https://arxiv.org/abs/1512.03385) and in addition we will load the pretrained weights from our [model training tutorial](model_training.ipynb)\n",
    "\n",
    "If you changed the dataset in the above cell, then we'll need to update the number of classes to create the model appropriately as well as loading your own pretrained weights. Additionally the model can be changed out completely to work with your specific use case.\n",
    "\n",
    "Additionally run the code block and select the device to run on before continuing. cpu runs in the pytorch cpu framework and cuda runs on an attached GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from neuralmagicML.models import resnet50, load_model\n",
    "\n",
    "num_classes = 10\n",
    "# TODO: change this to load pretrained weights from our cloud\n",
    "model = resnet50(num_classes=num_classes)\n",
    "pretrained_paths = [path for path in glob.glob('ResNet*.pth')]\n",
    "pretrained_path = pretrained_paths[0]\n",
    "load_model(pretrained_path, model)\n",
    "print('Created model {}'.format(model.__class__.__name__))\n",
    "\n",
    "print('\\nChoose the device to run on')\n",
    "device_choice = widgets.ToggleButtons(\n",
    "    options=['cuda', 'cpu'] if torch.cuda.is_available() else ['cpu'],\n",
    "    description='Device'\n",
    ")\n",
    "display(device_choice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning Setup\n",
    "\n",
    "Informally, sparsity is the degree in which a tensor is comprised of zeros. More formally:\n",
    "\n",
    "let $N^i$ be the total number of elements in a (e.g. weight) tensor $W_i$ and let $N^i_z$ be the number of elements which are zero-valued within that tensor.\n",
    "\n",
    "The sparsity level associated with that tensor is then defined as $s_i \\triangleq \\dfrac{N^i_z}{N^i}$ \n",
    "\n",
    "Our goal now is to maximize the sparsity of each weight tensor within the model while preserving the accuracy of the densely trained version. The most robust and effective approach to this so far has been magnitude pruning. To frame the problem, let's say our goal is to go from an intial sparsity $N^i_{init}$ to a final sparsity $N^i_{final}$. We could start with changing out our usual $L_2$ weight regularization with $L_1$ [($L_2$ vs $L_1$)](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c) in our cost function. Following through, we would find that we did, in fact, introduce a few zeros into our weights. We created a direct pathway within the optimization problem such that the model is incentivized to reduce the magnitude of unimportant weights to 0 (unimportant are defined as weights that do not significantly contribute to the loss function). This is hard to balance, though. For example, how do we determine the proper weighting between the loss function and the regularization such that we reach a desired sparsity without sacrificing accuracy? Also, emperically we find that the model will settle in local mins thus failing to reduce further weights to zero.\n",
    "\n",
    "We can take this general idea to more of an extreme, though. Based on the previous thought experiment, it's reasonable to make an assumption that weights near zero are likely not important to the final loss function as well. Taking this even further, we could say that the smallest values within a given weight tensor are the ones least likely to affect our loss function. This is, of course, assuming that we've trained long enough to reach a stable point where our neural networks cost function has mimimized unimportant weights. Naturally, then, we could apply a schedule where we prune away a small number of smallest weights for every $M$ training steps. This is exactly what magnitude pruning does. \n",
    "\n",
    "Below, we go through a UI to setup a magntiude pruning schedule for our model. While pruning, some layers within a model will be more sensitive than others. A general rule is that the initial input layer and the final output layer are the most sensitive as they are an absolute bottleneck to the information flow. Because of this, we offer up a UI capable of creating very intricate schedules to the point that each layer could be pruned with a different schedule to a different final sparsity. In general, though, we can get by with pruning the initial and final layers minimally and the other layers equally. At Neural Magic we are actively working on automating this process to find the best possible pruning schedules to maximize performance while minimizing accuracy loss. \n",
    "\n",
    "The default for the below model will disable pruning for the initial and final layers and prune all other layers to 90% sparsity over the course of 10 epochs. The options available are described below:\n",
    "- Add New Group: add a new pruning group to the UI, used to prune selected layers to differnt sparsity levels at different rates\n",
    "- Delete Current Group: remove the current group from the pruning schedule\n",
    "- Tabs: the pruning groups setup so far, click to switch between\n",
    "- Sparsity: a range slider where the left value defines the sparsity level to initially start pruning at and the right value represents the final sparsity level\n",
    "- Start Epoch: the epoch to start pruning the selected layers at the initial sparsity\n",
    "- End Epoch: the epoch to finish pruning the selected layers to the final sparsity\n",
    "- Update Freq: the update frequency, in epochs, at which to prune the layers; ie 1.0 will prune every epoch\n",
    "- Selectable Layers: a dropdown of the layers available in the model that can be pruned along with their FLOPS and param counts; select the desired layers to prune using the checkboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from neuralmagicML.notebooks import KSModifierWidgets\n",
    "\n",
    "\n",
    "device = device_choice.value\n",
    "print('running on device {}'.format(device))\n",
    "\n",
    "print('\\ncreating kernel sparsity analyzer widgets (need to execute the model, so may take some time)...')\n",
    "ks_widget, ks_modifiers = KSModifierWidgets.interactive_module(\n",
    "    model, device, inp_dim=(1, 3, 224, 224), init_start_sparsity=0.05, init_final_sparsity=0.9, disable_all=False\n",
    ")\n",
    "display(ks_widget)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Selection\n",
    "\n",
    "With our magnitude pruning schedule setup, we now need to define the hyperparameters to train with using magnitude pruning. The most important of these is the learning rate. Too high and we will fail to recover accuracy while pruning. Too low and best case we will have to train for many epochs, worst case we will fail to recover accuracy while pruning. Below we run a learning rate sensitivty analysis popularized in the [cyclic LR paper](https://arxiv.org/abs/1506.01186).\n",
    "\n",
    "To run the sensitivity analysis we will begin training the model at a very small learning rate ($10^{-9}$) where the weight updates are guaranteed to be lost in floating point precision errors (ie we won't learn). After each batch we exponentially increase the learning rate until we reach a very high learning rate ($10^0$) where we are guaranteed to diverge from our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from neuralmagicML.utils import lr_analysis, CrossEntropyLossWrapper\n",
    "\n",
    "\n",
    "### optimizer definitions\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "###\n",
    "\n",
    "analysis = lr_analysis(model, device, train_data_loader, CrossEntropyLossWrapper(),\n",
    "                       init_lr=1e-9, final_lr=1e0, sgd_momentum=momentum, sgd_weight_decay=weight_decay)\n",
    "# convert the loss points for each lr to the mean\n",
    "analysis = [(lr, torch.mean(loss).item()) for lr, loss in analysis]\n",
    "print(analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "print('Setting up model for kernel sparsity tracking...')\n",
    "conv_layers_names = []\n",
    "for name, mod in model.named_modules():\n",
    "    if isinstance(mod, Conv2d): #to add the FC layers: isinstance(mod, Conv2d) or isinstance(mod, Linear) \n",
    "        conv_layers_names.append(name)\n",
    "analyzed_layers = KSAnalyzerLayer.analyze_layers(model, conv_layers_names)\n",
    "\n",
    "def _record_kernel_sparsity(analyzed_layers: List[KSAnalyzerLayer], writer: SummaryWriter, epoch: int):\n",
    "#     layers_sparsities = []\n",
    "    for ks_layer in analyzed_layers:\n",
    "        tag = 'Kernel Sparsity/{}'.format(ks_layer.name)\n",
    "        writer.add_scalar(tag, ks_layer.param_sparsity.item(), epoch)\n",
    "    print('sparsity per layer [%]: '+ str([round(ks_layer.param_sparsity.item()*100.0,0) for ks_layer in analyzed_layers]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer , Loss, Logging etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import DataParallel\n",
    "from neuralmagicML.utils import CrossEntropyLossCalc, TopKAccuracy\n",
    "import os\n",
    "\n",
    "init_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "print('Creating optimizer with initial lr: {}, momentum: {}, weight_decay: {}'\n",
    "          .format(init_lr, momentum, weight_decay))\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), init_lr, momentum=momentum, weight_decay=weight_decay, nesterov=True\n",
    ")\n",
    "loss_extras = {\n",
    "    'top1acc': TopKAccuracy(1),\n",
    "    'top5acc': TopKAccuracy(5)\n",
    "}\n",
    "loss_calc = CrossEntropyLossCalc(loss_extras)\n",
    "print('Created loss calc {} with extras {}'.format(loss_calc, ', '.join(loss_extras.keys())))\n",
    "\n",
    "\n",
    "logs_dir = './logs'\n",
    "model_dir = '../pruned'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "    \n",
    "save_rate = 5\n",
    "print('Creating summary writer in {}'.format(logs_dir))\n",
    "writer = SummaryWriter(logdir=logs_dir, comment='imagenet training')\n",
    "if isinstance(model, DataParallel):\n",
    "    model = model.module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling the prunning process:\n",
    "Prunning involves two intertwined processes:\n",
    "1. sparsification - i.e. the selection of weights to zero out.\n",
    "2. re-training the model post sparsification.\n",
    "\n",
    "In practice, a gradual increase of the sparsity level allows for the recovery of accuracy by retraining (up to high levels of sparsity)\n",
    "\n",
    "In order to simplify these control of these two processes, we introduce 'Modifier' classes which manage the schedules  of the associated hyperparameters (i.e. learning_rate, sparsity per layer) thoughout the epochs. For added convinience below is a simple GUI to set these hyperparameters.\n",
    "The GUI allows for controlling the target sparsity on an individual layer basis / global / mixed fashion.\n",
    "Try setting all layers to a sparsity level of 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "############################################################\n",
    "## configuration of sparsity levels / enables per layer ####\n",
    "############################################################\n",
    "c0 = widgets.VBox([widgets.Checkbox(description=ks_layer.name, value=True) for ks_layer in analyzed_layers])\n",
    "c1 = widgets.VBox([widgets.FloatSlider(value=0.5,min=0.05,max=0.99) for _ in range(len(analyzed_layers))])\n",
    "layer_ctrl = widgets.HBox([c0,c1])\n",
    "global_ctrl = widgets.HBox([widgets.Checkbox(description='enable/disable all', value = True), \n",
    "                            widgets.FloatSlider(value=0.5,min=0.05,max=0.99,description='sparsity [%]')\n",
    "                           ])\n",
    "output2 = widgets.Output()\n",
    "\n",
    "activated_layers = [child.value for child in layer_ctrl.children[0].children]\n",
    "\n",
    "def global_enable_change(change):\n",
    "    with output2:\n",
    "        state = change['new']\n",
    "        print(state)\n",
    "        if state is not None:\n",
    "            for ckbx_child in layer_ctrl.children[0].children:\n",
    "                ckbx_child.value = state\n",
    "                \n",
    "global_ctrl.children[0].observe(global_enable_change, names='value')   \n",
    "\n",
    "def global_sparsity_set(change):\n",
    "    with output2:\n",
    "        val = change['new']\n",
    "        print(val)\n",
    "        if val is not None:\n",
    "            for ckbx_child, sldr_child in zip(layer_ctrl.children[0].children, layer_ctrl.children[1].children):\n",
    "                if ckbx_child.value:\n",
    "                    sldr_child.value = val\n",
    "    \n",
    "global_ctrl.children[1].observe(global_sparsity_set, names='value')   \n",
    "\n",
    "###############################################\n",
    "## configuration of learning rate schedule ####\n",
    "###############################################\n",
    "\n",
    "lr_class_dict = {   #TODO: read from CONSTRUCTORS in modifier_lr.py instead\n",
    "                    #to include all supported methods in the GUI\n",
    "    'ExponentialLR': {'gamma': [0.95, widgets.BoundedFloatText]}, #bound by 0.0\n",
    "    'StepLR': {'step_size': [20, widgets.BoundedIntText], #bound by 1\n",
    "              'gamma': [0.2, widgets.BoundedFloatText]}\n",
    "}\n",
    "\n",
    "lr_mod_args_field_initval = {\n",
    "    'start_epoch': 25.0,# 'start epoch:'],\n",
    "    'end_epoch': 35.0,# 'end epoch  :'],\n",
    "    'update_frequency': 1.0,# 'update freq:'],\n",
    "    'init_lr': 0.001# 'initial learning rate :']\n",
    "}\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "# lr_cfg_list = [widgets.Text(value='learning rate schedule', disabled=True)]\n",
    "lr_section_title = widgets.Text(value='learning rate schedule', disabled=True)\n",
    "lr_cfg_list =[]\n",
    "for fld, val in lr_mod_args_field_initval.items():\n",
    "    lr_cfg_list.append(widgets.BoundedFloatText(value=val, description=fld, disabled=False, min=0, style=style,))\n",
    "\n",
    "lr_slct = widgets.Dropdown(\n",
    "    options=[key for key in lr_class_dict.keys()],  \n",
    "    value=[key for key in lr_class_dict.keys()][0],\n",
    "    description='lr_class',\n",
    ")\n",
    "# lr_cfg_list.append(lr_slct)\n",
    "\n",
    "def create_lr_slct_list():\n",
    "    lr_slct_params = [] #create new widgets\n",
    "    for param, val in lr_class_dict[lr_slct.value].items():\n",
    "        lr_slct_params.append(val[1](value=val[0],description=param))\n",
    "    return lr_slct_params\n",
    "slct_param = widgets.VBox(children=create_lr_slct_list())\n",
    "# lr_cfg_list.append(slct_param)\n",
    "\n",
    "def refresh_lr_param(change):\n",
    "    if change['new']:\n",
    "        val = lr_slct.value\n",
    "        slct_param.children = create_lr_slct_list()\n",
    "\n",
    "lr_slct.observe(refresh_lr_param, names='value')   \n",
    "lr_cfg = widgets.VBox([lr_section_title, *lr_cfg_list, lr_slct, slct_param])\n",
    "\n",
    "##########################################\n",
    "## configuration of prunning schedule ####\n",
    "##########################################\n",
    "\n",
    "prunning_mod_args_field_initval = {\n",
    "    'start_epoch': 0.0,# 'start epoch:'],\n",
    "    'end_epoch': 25.0,#'end epoch  :'],\n",
    "    'update_frequency': 1.0#,'update freq:'],\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "prn_section_title = widgets.Text(value='prunning schedule', disabled=True)\n",
    "prn_cfg_list =[]\n",
    "for fld, val in prunning_mod_args_field_initval.items():\n",
    "    prn_cfg_list.append(widgets.BoundedFloatText(value=val, description=fld, disabled=False, min=0, style=style,))\n",
    "\n",
    "\n",
    "prn_cfg = widgets.VBox([prn_section_title,*prn_cfg_list])\n",
    "schd_cfg = widgets.VBox([lr_cfg, prn_cfg])#,prn_cfg_list])\n",
    "display(widgets.HBox([widgets.VBox([global_ctrl,layer_ctrl]),schd_cfg]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating learning rate schedule...')\n",
    "lr_mod_args = {}\n",
    "\n",
    "for child in lr_cfg_list: \n",
    "    lr_mod_args[child.description] = child.value\n",
    "assert(lr_slct.description == 'lr_class')\n",
    "lr_mod_args['lr_class'] = lr_slct.value\n",
    "lr_mod_args['lr_kwargs'] = {}\n",
    "for child in slct_param.children:\n",
    "    lr_mod_args['lr_kwargs'][child.description] = child.value\n",
    "\n",
    "lr_mod = LearningRateModifier(**lr_mod_args)\n",
    "\n",
    "print('Creating sparsification schedule...')\n",
    "\n",
    "def create_ks_mod_args(layer_name, final_sparsity):\n",
    "    ks_mod_args ={\n",
    "        'param': 'weight',\n",
    "        'init_sparsity': 0.05,\n",
    "        'inter_func': 'linear',\n",
    "        'layers': [layer_name],\n",
    "        'final_sparsity': final_sparsity\n",
    "    }\n",
    "    # add common fields\n",
    "    for child in prn_cfg_list:\n",
    "        ks_mod_args[child.description] = child.value\n",
    "    return ks_mod_args\n",
    "\n",
    "ks_mod_args_list = []\n",
    "for ckbx_child, sldr_child in zip(layer_ctrl.children[0].children, layer_ctrl.children[1].children):\n",
    "        if ckbx_child.value: #layer is sparsified\n",
    "            layer_name = ckbx_child.description\n",
    "            final_sparsity = sldr_child.value#\n",
    "            ks_mod_args_list.append(create_ks_mod_args(layer_name, final_sparsity))\n",
    "            \n",
    "ks_mod_list = [GradualKSModifier(**ks_mod_args) for ks_mod_args in ks_mod_args_list]\n",
    "modifiers = [lr_mod, *ks_mod_list]\n",
    "\n",
    "modifier_manager = ScheduledModifierManager(modifiers)\n",
    "optimizer = ScheduledOptimizer(optimizer, model, modifier_manager, steps_per_epoch=len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training\n",
    "The following should look very familiar - it is in fact the exact same code from our previous tutorial (NB1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Tuple, Dict\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "def _test_datasets(model, train_data_loader: DataLoader, val_data_loader: DataLoader,\n",
    "                   writer: SummaryWriter, epoch: int) -> Tuple[Dict[str, float], Dict[str, float]]:\n",
    "    val_losses , train_losses = None, None\n",
    "    if val_data_loader  is not None:\n",
    "        print('Running test for validation dataset for epoch {}'.format(epoch))\n",
    "        val = test_epoch(model, val_data_loader, loss_calc, device, epoch)\n",
    "        print('Completed test for validation dataset for epoch {}'.format(epoch))\n",
    "        val_losses = {}\n",
    "        for loss, _ in val.items():\n",
    "            val_losses[loss] = torch.mean(torch.cat(val[loss])).item()\n",
    "            val_tag = 'Test/validation/{}'.format(loss)\n",
    "            writer.add_scalar(val_tag, val_losses[loss], epoch)\n",
    "        \n",
    "        val_loss_str = 'validation set - epoch: {} '.format(epoch)\n",
    "        for loss, value in val_losses.items():\n",
    "            val_loss_str += (loss + ': {0:.2f} '.format(value))\n",
    "        print(val_loss_str)\n",
    "        \n",
    "        \n",
    "    if train_data_loader is not None:\n",
    "        print('Running test for train dataset for epoch {}'.format(epoch))\n",
    "        train = test_epoch(model, train_data_loader, loss_calc, device, epoch)\n",
    "        print('Completed test for train dataset for epoch {}'.format(epoch))\n",
    "        train_losses = {}\n",
    "\n",
    "        for loss, _ in train.items():\n",
    "            train_losses[loss] = torch.mean(torch.cat(train[loss])).item()\n",
    "            train_tag = 'Test/training/{}'.format(loss)\n",
    "            writer.add_scalar(train_tag, train_losses[loss], epoch)\n",
    "\n",
    "        \n",
    "        train_loss_str = 'training set - epoch: {} '.format(epoch)\n",
    "        for loss, value in train_losses.items():\n",
    "            train_loss_str += (loss + ': {0:.2f} '.format(value))\n",
    "        print(train_loss_str)\n",
    "\n",
    "\n",
    "    return val_losses , train_losses\n",
    "\n",
    "\n",
    "def test_epoch(model: Module, data_loader: DataLoader, loss, device, epoch: int) -> Dict:\n",
    "    model.eval()\n",
    "    results = {}#ModuleTestResults()\n",
    "    with torch.no_grad():\n",
    "        for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "            y_lab = y_lab.to(device)\n",
    "            x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "            batch_size = y_lab.shape[0]\n",
    "            \n",
    "            y_pred = model(*x_feature)\n",
    "\n",
    "            losses = loss(x_feature, y_lab, y_pred)  # type: Dict[str, Tensor]\n",
    "            for key, val in losses.items():\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "\n",
    "                result = val.detach_().cpu()\n",
    "                result = result.repeat(batch_size) #repeat tensor so that there is no dependency on batch size\n",
    "                results[key].append(result)\n",
    "#             results.append(losses, batch_size)\n",
    "    return results\n",
    "\n",
    "def train_epoch(model: Module, data_loader: DataLoader, optimizer, loss, device, data_counter: int):\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "        # copy next batch to the device we are using\n",
    "        y_lab = y_lab.to(device)\n",
    "        x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "        batch_size = y_lab.shape[0]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward \n",
    "        y_pred = model(*x_feature)\n",
    "        \n",
    "        # update losses\n",
    "        losses = loss(x_feature, y_lab, y_pred)  # type: Dict[str, Tensor]\n",
    "        \n",
    "        # backward\n",
    "        losses['loss'].backward()\n",
    "        \n",
    "        # take SGD step\n",
    "        optimizer.step(closure=None)\n",
    "        \n",
    "        # log loss and accuracy\n",
    "        data_counter += batch_size\n",
    "        for _loss, _value in losses.items():\n",
    "            writer.add_scalar('Train/{}'.format(_loss), _value.item(), data_counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prunning main loop:\n",
    "This too is very simialr to the training main loop previously introduced, the main differences are:\n",
    "1. we are tracking sparsity\n",
    "2. we are following the schedules as orchastrated by the modifiers above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print('Running baseline test...')\n",
    "epoch = -1\n",
    "_record_kernel_sparsity(analyzed_layers, writer, epoch)\n",
    "_test_datasets(model, None, val_data_loader, writer, epoch=-1)\n",
    "\n",
    "print('Training model')\n",
    "num_epochs = int(math.ceil(modifier_manager.max_epochs))\n",
    "data_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}'.format(epoch))\n",
    "    optimizer.epoch_start()\n",
    "    _record_kernel_sparsity(analyzed_layers, writer, epoch)\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch(model, train_data_loader, optimizer, loss_calc, device, data_counter)\n",
    "    optimizer.epoch_end()\n",
    "    val_losses, train_losses = _test_datasets(model, None, val_data_loader, writer, epoch)\n",
    "\n",
    "    if save_rate > 0 and epoch % save_rate == 0:\n",
    "        save_path = os.path.join(model_dir, 'resnet50-epoch={:03d}-val={:.4f}.pth'\n",
    "                                 .format(epoch, val_losses['loss']))\n",
    "        save_model(save_path, model, optimizer, epoch)\n",
    "        print('saved model checkpoint at {}'.format(save_path))\n",
    "\n",
    "_record_kernel_sparsity(analyzed_layers, writer, num_epochs)\n",
    "\n",
    "\n",
    "scalars_json_path = os.path.join(logs_dir, 'all_scalars.json')\n",
    "writer.export_scalars_to_json(scalars_json_path)\n",
    "writer.close()\n",
    "\n",
    "save_path = os.path.join(model_dir, 'resnet50-pruned.pth')\n",
    "print('Finished training, saving model to {}'.format(save_path))\n",
    "save_model(save_path, model)\n",
    "print('Saved model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifier_manager.max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
