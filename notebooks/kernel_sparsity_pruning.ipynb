{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Sparsity Pruning Tutorial\n",
    "\n",
    "Neural networks, in general, are very overparamatarized for given tasks (ie the number of parameters far exceeds the number of training points) yet still they [generalize very well](https://arxiv.org/abs/1611.03530). This flies against conventional wisdom where overparamatarizing a model will lead to overfitting and puting theory behind this empirical evidence is a very active area of research.\n",
    "\n",
    "Additionally, [early on](http://yann.lecun.com/exdb/publis/pdf/lecun-90b.pdf) it was discovered that large numbers of weights in neural networks could be pruned away (set to 0) without affecting the loss and in most cases actually improving the generalization capability of the network. This work was reinvigorated with Song Han's [2015 paper](https://arxiv.org/abs/1510.00149) in pursuit of compressing model size for mobile applications. This has resulted in numerous papers coming out on the topic of weight pruning, filter pruning, channel pruning, and ultimately block pruning. [This paper](https://arxiv.org/abs/1902.09574) out of Google gives a good overview of the current state of sparsity.\n",
    "\n",
    "Given that models are very overparamatarized and large numbers of weights can be effectively pruned away, what does this leave us with? Well intuitively, then, we can think of pruning as performing an [architecture search](https://openreview.net/pdf?id=rJlnB3C5Ym) within this large, traditionally fixed weight space. What was originally important in the dense model was representing a large number of pathways for optimization. We can then effectively remove the unused pathways in the optimization space with a fine toothed comb.\n",
    "\n",
    "Well what does pruning get us? We now have a model with a lot of multiplications by zero that we don't need to run. If we're smart about how we do structure this compute (a suprisingly trickly problem), we can now run the model way faster than ever thought possible! That's where the [Neural Magic](http://neuralmagic.com/) engine can help us.\n",
    "\n",
    "This tutorial provides a step by step walk through for pruning an already trained (dense) model. Specifically it is setup to work with the model trained in our [model training tutorial](model_training.ipynb), but it can be changed to support other models/datasets as needed:\n",
    "1. Dataset selection\n",
    "2. Model selection and loading\n",
    "3. Pruning setup\n",
    "4. Recalibration using pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.6.8 (v3.6.8:3c6b436a57, Dec 24 2018, 02:04:31) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 6.0 (clang-600.0.57)] on darwin\n",
      "/Users/markkurtz/code/neuralmagic/Shared/neuralmagicml-pytorch\n",
      "Added current package path to sys.path\n",
      "Be sure to install from requirements.txt and pytorch separately\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print('Python %s on %s' % (sys.version, sys.platform))\n",
    "\n",
    "package_path = os.path.abspath(os.path.join(os.path.expanduser(os.getcwd()), os.pardir))\n",
    "print(package_path)\n",
    "\n",
    "\"\"\"\n",
    "Adding the path to the neuralmagic-pytorch extension to the path so it isn't necessary to have it installed\n",
    "\"\"\"\n",
    "sys.path.extend([package_path])\n",
    "\n",
    "print('Added current package path to sys.path')\n",
    "print('Be sure to install from requirements.txt and pytorch separately')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "\n",
    "We are using fast.ai's [Imagenette dataset](https://github.com/fastai/imagenette) provided under the [Apache License 2.0](https://github.com/fastai/imagenette/blob/master/LICENSE) as the default dataset. The original authors, much like ourselves, were interested in a dataset that has similar properties to more complicated datasets such as the Imagenet dataset but one that would allow rapid iterations. It includes 10 of the easiest classes out of the Imagenet 1000 dataset: tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute. If you are interested in visualizing the properties in this dataset see our [model training tutorial](model_training.ipynb) which also gives a more in depth breakdown for what batch size to use and the dataset splits.\n",
    "\n",
    "The dataset can easily be changed to the desired dataset in the code given.\n",
    "\n",
    "Below we will need to fill in the dataset path, train batch size, and test batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enter the local path where the dataset can be found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2db008a7df2449c7ba9c00b1c12aee96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Dataset Path', placeholder='Enter local path to dataset')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Choose the batch size to run through the model during train and test runs\n",
      "(press enter if/after inputting manually)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1746fd00263d468587c0e2759e7f3930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=64, description='Train Batch Size:', max=256, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "108eb9692d7d4f6094326aa45d174aba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=1, description='Test Batch Size:', max=256, min=1)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "import torch\n",
    "\n",
    "print('\\nEnter the local path where the dataset can be found')\n",
    "\n",
    "dataset_text = widgets.Text(value='', placeholder='Enter local path to dataset', description='Dataset Path')\n",
    "display(dataset_text)\n",
    "\n",
    "print('\\nChoose the batch size to run through the model during train and test runs')\n",
    "print('(press enter if/after inputting manually)')\n",
    "train_batch_size_slider = widgets.IntSlider(\n",
    "    value=64, min=1, max=256, step=1, description='Train Batch Size:'\n",
    ")\n",
    "display(train_batch_size_slider)\n",
    "test_batch_size_slider = widgets.IntSlider(\n",
    "    value=64 if torch.cuda.is_available() else 1, min=1, max=256, step=1, description='Test Batch Size:'\n",
    ")\n",
    "display(test_batch_size_slider)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading dataset from /Users/markkurtz/datasets/imagenette\n",
      "\n",
      "Using train batch size of 64 and test batch size of 1\n",
      "\n",
      "already downloaded imagenette of size ImagenetteSize.s160\n",
      "train dataset created: \n",
      "Dataset ImagenetteDataset\n",
      "    Number of datapoints: 12894\n",
      "    Root location: /Users/markkurtz/datasets/imagenette/imagenette-160/train\n",
      "\n",
      "already downloaded imagenette of size ImagenetteSize.s160\n",
      "validation test dataset created: \n",
      "Dataset ImagenetteDataset\n",
      "    Number of datapoints: 500\n",
      "    Root location: /Users/markkurtz/datasets/imagenette/imagenette-160/val\n",
      "\n",
      "already downloaded imagenette of size ImagenetteSize.s160\n",
      "train test dataset created: \n",
      "Dataset ImagenetteDataset\n",
      "    Number of datapoints: 500\n",
      "    Root location: /Users/markkurtz/datasets/imagenette/imagenette-160/train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from neuralmagicML.datasets import ImagenetteDataset, EarlyStopDataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset_root = os.path.abspath(os.path.expanduser(dataset_text.value.strip()))\n",
    "print('\\nLoading dataset from {}'.format(dataset_root))\n",
    "\n",
    "if not os.path.exists(dataset_root):\n",
    "    raise Exception('Folder must exist for dataset at {}'.format(dataset_root))\n",
    "    \n",
    "train_batch_size = train_batch_size_slider.value\n",
    "test_batch_size = test_batch_size_slider.value\n",
    "\n",
    "print('\\nUsing train batch size of {} and test batch size of {}\\n'\n",
    "      .format(train_batch_size, test_batch_size))\n",
    "    \n",
    "train_dataset = ImagenetteDataset(dataset_root, train=True, rand_trans=True)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=4)\n",
    "print('train dataset created: \\n{}\\n'.format(train_dataset))\n",
    "\n",
    "val_dataset = ImagenetteDataset(dataset_root, train=False, rand_trans=False)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=train_batch_size, shuffle=False, num_workers=4)\n",
    "print('validation test dataset created: \\n{}\\n'.format(val_dataset))\n",
    "\n",
    "train_test_dataset = EarlyStopDataset(ImagenetteDataset(dataset_root, train=True, rand_trans=False),\n",
    "                                      early_stop=len(val_dataset))\n",
    "train_test_data_loader = DataLoader(train_test_dataset, batch_size=train_batch_size, shuffle=False, num_workers=4)\n",
    "print('train test dataset created: \\n{}\\n'.format(train_test_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Loading\n",
    "\n",
    "For this exercise we'll create the standard [ResNet50 model](https://arxiv.org/abs/1512.03385) and in addition we will load the pretrained weights from our [model training tutorial](model_training.ipynb)\n",
    "\n",
    "If you changed the dataset in the above cell then we'll need to update the number of classes to create the model with appropriately and load your own pretrained weights. Additionally the model can be changed out completely to work with your specific use case.\n",
    "\n",
    "Additionally run the code block and select the device to run on before continuing. cpu runs in the pytorch cpu framework and cuda runs on an attached GPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model ResNet\n",
      "\n",
      "Choose the device to run on\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfeb84662de24e9da7b39f9318f73a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ToggleButtons(description='Device', options=('cpu',), value='cpu')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "from neuralmagicML.models import resnet50, load_model\n",
    "\n",
    "num_classes = 10\n",
    "# TODO: change this to load pretrained weights from our cloud\n",
    "model = resnet50(num_classes=num_classes)\n",
    "pretrained_paths = [path for path in glob.glob('ResNet*.pth')]\n",
    "pretrained_path = pretrained_paths[0]\n",
    "load_model(pretrained_path, model)\n",
    "print('Created model {}'.format(model.__class__.__name__))\n",
    "\n",
    "print('\\nChoose the device to run on')\n",
    "device_choice = widgets.ToggleButtons(\n",
    "    options=['cuda', 'cpu'] if torch.cuda.is_available() else ['cpu'],\n",
    "    description='Device'\n",
    ")\n",
    "display(device_choice)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity, Prunning and high level motivation\n",
    "### sparsity:\n",
    "\n",
    "Informally, sparsity is the degree in which a tensor is comprised of zeros.\n",
    "\n",
    "Slightly more formally:\n",
    "\n",
    "let $N^i$ be the total number of elements in a (e.g. weight) tensor $W_i$\n",
    "\n",
    "let $N^i_z$ be the number of elements which are zero-valued within that tensor\n",
    "\n",
    "The sparsity level associated with that tensor is defined as $s_i \\triangleq \\dfrac{N^i_z}{N^i}$ \n",
    "\n",
    "\n",
    "### prunning:\n",
    "\n",
    "Prunning is the process selectively setting weights in a model to zero. The selection of how many and which weights to set to zero affects the accuracy and model required FLOPs and memory footprint. **Critically - attaining high levels of sparsity while preserving accuracy is possible, as we will demonstrate in this notebook**\n",
    "\n",
    "### Sparsity --> Less FLOPs -?-> accelerated performance:\n",
    "The fact that models can be heavily sparsified with little or no accuracy hit is well known in the research community. Intuitively, the higher the sparsity level the less theoretical FLOPs are required and hence a correspondingly large performance acceleration. However, while the first part of that intutive reasoning is true (higer sparsity --> less theoretical FLOPs), the second one is not nececcerily true (less theoretical FLOPs -/-> higher performance). The reason is that typical hardware such as GPUs is very ill-equiped to take advantage of that sparsity, and FLOPs savings in practice is very hard to come by. On CPUs, in contrast, algorithms for that very exploitation can be flexibly developed.\n",
    "\n",
    "Armed with this insight we are ready (and motivated!) to start looking at model sparsity with the aim of increasing it (via prunning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model for kernel sparsity tracking...\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "print('Setting up model for kernel sparsity tracking...')\n",
    "conv_layers_names = []\n",
    "for name, mod in model.named_modules():\n",
    "    if isinstance(mod, Conv2d): #to add the FC layers: isinstance(mod, Conv2d) or isinstance(mod, Linear) \n",
    "        conv_layers_names.append(name)\n",
    "analyzed_layers = KSAnalyzerLayer.analyze_layers(model, conv_layers_names)\n",
    "\n",
    "def _record_kernel_sparsity(analyzed_layers: List[KSAnalyzerLayer], writer: SummaryWriter, epoch: int):\n",
    "#     layers_sparsities = []\n",
    "    for ks_layer in analyzed_layers:\n",
    "        tag = 'Kernel Sparsity/{}'.format(ks_layer.name)\n",
    "        writer.add_scalar(tag, ks_layer.param_sparsity.item(), epoch)\n",
    "    print('sparsity per layer [%]: '+ str([round(ks_layer.param_sparsity.item()*100.0,0) for ks_layer in analyzed_layers]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer , Loss, Logging etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating optimizer with initial lr: 0.01, momentum: 0.9, weight_decay: 0.0001\n",
      "Created loss calc <neuralmagicML.utils.loss_calc.CrossEntropyLossCalc object at 0x7fd840247c18> with extras top1acc, top5acc\n",
      "Creating summary writer in ./logs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import DataParallel\n",
    "from neuralmagicML.utils import CrossEntropyLossCalc, TopKAccuracy\n",
    "import os\n",
    "\n",
    "init_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "print('Creating optimizer with initial lr: {}, momentum: {}, weight_decay: {}'\n",
    "          .format(init_lr, momentum, weight_decay))\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), init_lr, momentum=momentum, weight_decay=weight_decay, nesterov=True\n",
    ")\n",
    "loss_extras = {\n",
    "    'top1acc': TopKAccuracy(1),\n",
    "    'top5acc': TopKAccuracy(5)\n",
    "}\n",
    "loss_calc = CrossEntropyLossCalc(loss_extras)\n",
    "print('Created loss calc {} with extras {}'.format(loss_calc, ', '.join(loss_extras.keys())))\n",
    "\n",
    "\n",
    "logs_dir = './logs'\n",
    "model_dir = '../pruned'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "    \n",
    "save_rate = 5\n",
    "print('Creating summary writer in {}'.format(logs_dir))\n",
    "writer = SummaryWriter(logdir=logs_dir, comment='imagenet training')\n",
    "if isinstance(model, DataParallel):\n",
    "    model = model.module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling the prunning process:\n",
    "Prunning involves two intertwined processes:\n",
    "1. sparsification - i.e. the selection of weights to zero out.\n",
    "2. re-training the model post sparsification.\n",
    "\n",
    "In practice, a gradual increase of the sparsity level allows for the recovery of accuracy by retraining (up to high levels of sparsity)\n",
    "\n",
    "In order to simplify these control of these two processes, we introduce 'Modifier' classes which manage the schedules  of the associated hyperparameters (i.e. learning_rate, sparsity per layer) thoughout the epochs. For added convinience below is a simple GUI to set these hyperparameters.\n",
    "The GUI allows for controlling the target sparsity on an individual layer basis / global / mixed fashion.\n",
    "Try setting all layers to a sparsity level of 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a40898d300fc4b05a015b2c310271591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='enable/disable all'), FloatSlid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "############################################################\n",
    "## configuration of sparsity levels / enables per layer ####\n",
    "############################################################\n",
    "c0 = widgets.VBox([widgets.Checkbox(description=ks_layer.name, value=True) for ks_layer in analyzed_layers])\n",
    "c1 = widgets.VBox([widgets.FloatSlider(value=0.5,min=0.05,max=0.99) for _ in range(len(analyzed_layers))])\n",
    "layer_ctrl = widgets.HBox([c0,c1])\n",
    "global_ctrl = widgets.HBox([widgets.Checkbox(description='enable/disable all', value = True), \n",
    "                            widgets.FloatSlider(value=0.5,min=0.05,max=0.99,description='sparsity [%]')\n",
    "                           ])\n",
    "output2 = widgets.Output()\n",
    "\n",
    "activated_layers = [child.value for child in layer_ctrl.children[0].children]\n",
    "\n",
    "def global_enable_change(change):\n",
    "    with output2:\n",
    "        state = change['new']\n",
    "        print(state)\n",
    "        if state is not None:\n",
    "            for ckbx_child in layer_ctrl.children[0].children:\n",
    "                ckbx_child.value = state\n",
    "                \n",
    "global_ctrl.children[0].observe(global_enable_change, names='value')   \n",
    "\n",
    "def global_sparsity_set(change):\n",
    "    with output2:\n",
    "        val = change['new']\n",
    "        print(val)\n",
    "        if val is not None:\n",
    "            for ckbx_child, sldr_child in zip(layer_ctrl.children[0].children, layer_ctrl.children[1].children):\n",
    "                if ckbx_child.value:\n",
    "                    sldr_child.value = val\n",
    "    \n",
    "global_ctrl.children[1].observe(global_sparsity_set, names='value')   \n",
    "\n",
    "###############################################\n",
    "## configuration of learning rate schedule ####\n",
    "###############################################\n",
    "\n",
    "lr_class_dict = {   #TODO: read from CONSTRUCTORS in modifier_lr.py instead\n",
    "                    #to include all supported methods in the GUI\n",
    "    'ExponentialLR': {'gamma': [0.95, widgets.BoundedFloatText]}, #bound by 0.0\n",
    "    'StepLR': {'step_size': [20, widgets.BoundedIntText], #bound by 1\n",
    "              'gamma': [0.2, widgets.BoundedFloatText]}\n",
    "}\n",
    "\n",
    "lr_mod_args_field_initval = {\n",
    "    'start_epoch': 25.0,# 'start epoch:'],\n",
    "    'end_epoch': 35.0,# 'end epoch  :'],\n",
    "    'update_frequency': 1.0,# 'update freq:'],\n",
    "    'init_lr': 0.001# 'initial learning rate :']\n",
    "}\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "# lr_cfg_list = [widgets.Text(value='learning rate schedule', disabled=True)]\n",
    "lr_section_title = widgets.Text(value='learning rate schedule', disabled=True)\n",
    "lr_cfg_list =[]\n",
    "for fld, val in lr_mod_args_field_initval.items():\n",
    "    lr_cfg_list.append(widgets.BoundedFloatText(value=val, description=fld, disabled=False, min=0, style=style,))\n",
    "\n",
    "lr_slct = widgets.Dropdown(\n",
    "    options=[key for key in lr_class_dict.keys()],  \n",
    "    value=[key for key in lr_class_dict.keys()][0],\n",
    "    description='lr_class',\n",
    ")\n",
    "# lr_cfg_list.append(lr_slct)\n",
    "\n",
    "def create_lr_slct_list():\n",
    "    lr_slct_params = [] #create new widgets\n",
    "    for param, val in lr_class_dict[lr_slct.value].items():\n",
    "        lr_slct_params.append(val[1](value=val[0],description=param))\n",
    "    return lr_slct_params\n",
    "slct_param = widgets.VBox(children=create_lr_slct_list())\n",
    "# lr_cfg_list.append(slct_param)\n",
    "\n",
    "def refresh_lr_param(change):\n",
    "    if change['new']:\n",
    "        val = lr_slct.value\n",
    "        slct_param.children = create_lr_slct_list()\n",
    "\n",
    "lr_slct.observe(refresh_lr_param, names='value')   \n",
    "lr_cfg = widgets.VBox([lr_section_title, *lr_cfg_list, lr_slct, slct_param])\n",
    "\n",
    "##########################################\n",
    "## configuration of prunning schedule ####\n",
    "##########################################\n",
    "\n",
    "prunning_mod_args_field_initval = {\n",
    "    'start_epoch': 0.0,# 'start epoch:'],\n",
    "    'end_epoch': 25.0,#'end epoch  :'],\n",
    "    'update_frequency': 1.0#,'update freq:'],\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "prn_section_title = widgets.Text(value='prunning schedule', disabled=True)\n",
    "prn_cfg_list =[]\n",
    "for fld, val in prunning_mod_args_field_initval.items():\n",
    "    prn_cfg_list.append(widgets.BoundedFloatText(value=val, description=fld, disabled=False, min=0, style=style,))\n",
    "\n",
    "\n",
    "prn_cfg = widgets.VBox([prn_section_title,*prn_cfg_list])\n",
    "schd_cfg = widgets.VBox([lr_cfg, prn_cfg])#,prn_cfg_list])\n",
    "display(widgets.HBox([widgets.VBox([global_ctrl,layer_ctrl]),schd_cfg]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating learning rate schedule...\n",
      "Creating sparsification schedule...\n"
     ]
    }
   ],
   "source": [
    "print('Creating learning rate schedule...')\n",
    "lr_mod_args = {}\n",
    "\n",
    "for child in lr_cfg_list: \n",
    "    lr_mod_args[child.description] = child.value\n",
    "assert(lr_slct.description == 'lr_class')\n",
    "lr_mod_args['lr_class'] = lr_slct.value\n",
    "lr_mod_args['lr_kwargs'] = {}\n",
    "for child in slct_param.children:\n",
    "    lr_mod_args['lr_kwargs'][child.description] = child.value\n",
    "\n",
    "lr_mod = LearningRateModifier(**lr_mod_args)\n",
    "\n",
    "print('Creating sparsification schedule...')\n",
    "\n",
    "def create_ks_mod_args(layer_name, final_sparsity):\n",
    "    ks_mod_args ={\n",
    "        'param': 'weight',\n",
    "        'init_sparsity': 0.05,\n",
    "        'inter_func': 'linear',\n",
    "        'layers': [layer_name],\n",
    "        'final_sparsity': final_sparsity\n",
    "    }\n",
    "    # add common fields\n",
    "    for child in prn_cfg_list:\n",
    "        ks_mod_args[child.description] = child.value\n",
    "    return ks_mod_args\n",
    "\n",
    "ks_mod_args_list = []\n",
    "for ckbx_child, sldr_child in zip(layer_ctrl.children[0].children, layer_ctrl.children[1].children):\n",
    "        if ckbx_child.value: #layer is sparsified\n",
    "            layer_name = ckbx_child.description\n",
    "            final_sparsity = sldr_child.value#\n",
    "            ks_mod_args_list.append(create_ks_mod_args(layer_name, final_sparsity))\n",
    "            \n",
    "ks_mod_list = [GradualKSModifier(**ks_mod_args) for ks_mod_args in ks_mod_args_list]\n",
    "modifiers = [lr_mod, *ks_mod_list]\n",
    "\n",
    "modifier_manager = ScheduledModifierManager(modifiers)\n",
    "optimizer = ScheduledOptimizer(optimizer, model, modifier_manager, steps_per_epoch=len(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training\n",
    "The following should look very familiar - it is in fact the exact same code from our previous tutorial (NB1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Tuple, Dict\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "def _test_datasets(model, train_data_loader: DataLoader, val_data_loader: DataLoader,\n",
    "                   writer: SummaryWriter, epoch: int) -> Tuple[Dict[str, float], Dict[str, float]]:\n",
    "    val_losses , train_losses = None, None\n",
    "    if val_data_loader  is not None:\n",
    "        print('Running test for validation dataset for epoch {}'.format(epoch))\n",
    "        val = test_epoch(model, val_data_loader, loss_calc, device, epoch)\n",
    "        print('Completed test for validation dataset for epoch {}'.format(epoch))\n",
    "        val_losses = {}\n",
    "        for loss, _ in val.items():\n",
    "            val_losses[loss] = torch.mean(torch.cat(val[loss])).item()\n",
    "            val_tag = 'Test/validation/{}'.format(loss)\n",
    "            writer.add_scalar(val_tag, val_losses[loss], epoch)\n",
    "        \n",
    "        val_loss_str = 'validation set - epoch: {} '.format(epoch)\n",
    "        for loss, value in val_losses.items():\n",
    "            val_loss_str += (loss + ': {0:.2f} '.format(value))\n",
    "        print(val_loss_str)\n",
    "        \n",
    "        \n",
    "    if train_data_loader is not None:\n",
    "        print('Running test for train dataset for epoch {}'.format(epoch))\n",
    "        train = test_epoch(model, train_data_loader, loss_calc, device, epoch)\n",
    "        print('Completed test for train dataset for epoch {}'.format(epoch))\n",
    "        train_losses = {}\n",
    "\n",
    "        for loss, _ in train.items():\n",
    "            train_losses[loss] = torch.mean(torch.cat(train[loss])).item()\n",
    "            train_tag = 'Test/training/{}'.format(loss)\n",
    "            writer.add_scalar(train_tag, train_losses[loss], epoch)\n",
    "\n",
    "        \n",
    "        train_loss_str = 'training set - epoch: {} '.format(epoch)\n",
    "        for loss, value in train_losses.items():\n",
    "            train_loss_str += (loss + ': {0:.2f} '.format(value))\n",
    "        print(train_loss_str)\n",
    "\n",
    "\n",
    "    return val_losses , train_losses\n",
    "\n",
    "\n",
    "def test_epoch(model: Module, data_loader: DataLoader, loss, device, epoch: int) -> Dict:\n",
    "    model.eval()\n",
    "    results = {}#ModuleTestResults()\n",
    "    with torch.no_grad():\n",
    "        for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "            y_lab = y_lab.to(device)\n",
    "            x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "            batch_size = y_lab.shape[0]\n",
    "            \n",
    "            y_pred = model(*x_feature)\n",
    "\n",
    "            losses = loss(x_feature, y_lab, y_pred)  # type: Dict[str, Tensor]\n",
    "            for key, val in losses.items():\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "\n",
    "                result = val.detach_().cpu()\n",
    "                result = result.repeat(batch_size) #repeat tensor so that there is no dependency on batch size\n",
    "                results[key].append(result)\n",
    "#             results.append(losses, batch_size)\n",
    "    return results\n",
    "\n",
    "def train_epoch(model: Module, data_loader: DataLoader, optimizer, loss, device, data_counter: int):\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "        # copy next batch to the device we are using\n",
    "        y_lab = y_lab.to(device)\n",
    "        x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "        batch_size = y_lab.shape[0]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward \n",
    "        y_pred = model(*x_feature)\n",
    "        \n",
    "        # update losses\n",
    "        losses = loss(x_feature, y_lab, y_pred)  # type: Dict[str, Tensor]\n",
    "        \n",
    "        # backward\n",
    "        losses['loss'].backward()\n",
    "        \n",
    "        # take SGD step\n",
    "        optimizer.step(closure=None)\n",
    "        \n",
    "        # log loss and accuracy\n",
    "        data_counter += batch_size\n",
    "        for _loss, _value in losses.items():\n",
    "            writer.add_scalar('Train/{}'.format(_loss), _value.item(), data_counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prunning main loop:\n",
    "This too is very simialr to the training main loop previously introduced, the main differences are:\n",
    "1. we are tracking sparsity\n",
    "2. we are following the schedules as orchastrated by the modifiers above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline test...\n",
      "sparsity per layer [%]: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Running test for validation dataset for epoch -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch -1\n",
      "validation set - epoch: -1 loss: 0.40 top1acc: 87.20 top5acc: 98.60 \n",
      "Training model\n",
      "Starting epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 0\n",
      "validation set - epoch: 0 loss: 0.39 top1acc: 87.60 top5acc: 98.80 \n",
      "saved model checkpoint at ../pruned/resnet50-epoch=000-val=0.3917.pth\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.76it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 1\n",
      "validation set - epoch: 1 loss: 0.37 top1acc: 87.80 top5acc: 98.80 \n",
      "Starting epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0, 9.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 2\n",
      "validation set - epoch: 2 loss: 0.39 top1acc: 88.00 top5acc: 99.00 \n",
      "Starting epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0, 10.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.74it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 3\n",
      "validation set - epoch: 3 loss: 0.35 top1acc: 88.40 top5acc: 99.00 \n",
      "Starting epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0, 12.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.76it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 4\n",
      "validation set - epoch: 4 loss: 0.38 top1acc: 89.00 top5acc: 98.80 \n",
      "Starting epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0, 14.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.76it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 5\n",
      "validation set - epoch: 5 loss: 0.35 top1acc: 89.40 top5acc: 98.60 \n",
      "saved model checkpoint at ../pruned/resnet50-epoch=005-val=0.3498.pth\n",
      "Starting epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0, 16.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 6\n",
      "validation set - epoch: 6 loss: 0.37 top1acc: 89.20 top5acc: 98.40 \n",
      "Starting epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0, 18.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 7\n",
      "validation set - epoch: 7 loss: 0.40 top1acc: 86.20 top5acc: 99.00 \n",
      "Starting epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0, 19.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.74it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 8\n",
      "validation set - epoch: 8 loss: 0.36 top1acc: 88.00 top5acc: 98.80 \n",
      "Starting epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0, 21.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 9\n",
      "validation set - epoch: 9 loss: 0.36 top1acc: 89.20 top5acc: 99.00 \n",
      "Starting epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0, 23.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.75it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 10\n",
      "validation set - epoch: 10 loss: 0.37 top1acc: 87.40 top5acc: 98.40 \n",
      "saved model checkpoint at ../pruned/resnet50-epoch=010-val=0.3740.pth\n",
      "Starting epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0, 25.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.76it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 11\n",
      "validation set - epoch: 11 loss: 0.37 top1acc: 87.80 top5acc: 98.80 \n",
      "Starting epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0, 27.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.76it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 12\n",
      "validation set - epoch: 12 loss: 0.38 top1acc: 87.60 top5acc: 98.80 \n",
      "Starting epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0, 28.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.76it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 13\n",
      "validation set - epoch: 13 loss: 0.37 top1acc: 87.60 top5acc: 98.80 \n",
      "Starting epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0, 30.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 14\n",
      "validation set - epoch: 14 loss: 0.35 top1acc: 89.00 top5acc: 98.80 \n",
      "Starting epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0, 32.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.76it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 15\n",
      "validation set - epoch: 15 loss: 0.38 top1acc: 89.00 top5acc: 98.40 \n",
      "saved model checkpoint at ../pruned/resnet50-epoch=015-val=0.3849.pth\n",
      "Starting epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0, 34.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 16\n",
      "validation set - epoch: 16 loss: 0.37 top1acc: 89.00 top5acc: 98.40 \n",
      "Starting epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0, 36.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.76it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 17\n",
      "validation set - epoch: 17 loss: 0.36 top1acc: 89.60 top5acc: 98.80 \n",
      "Starting epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0, 37.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 18\n",
      "validation set - epoch: 18 loss: 0.39 top1acc: 87.80 top5acc: 99.00 \n",
      "Starting epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0, 39.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 19\n",
      "validation set - epoch: 19 loss: 0.37 top1acc: 89.00 top5acc: 98.80 \n",
      "Starting epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0, 41.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 20\n",
      "validation set - epoch: 20 loss: 0.36 top1acc: 89.60 top5acc: 98.80 \n",
      "saved model checkpoint at ../pruned/resnet50-epoch=020-val=0.3554.pth\n",
      "Starting epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0, 43.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 21\n",
      "validation set - epoch: 21 loss: 0.35 top1acc: 88.80 top5acc: 98.40 \n",
      "Starting epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0, 45.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 22\n",
      "validation set - epoch: 22 loss: 0.37 top1acc: 89.20 top5acc: 99.00 \n",
      "Starting epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0, 46.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 23\n",
      "validation set - epoch: 23 loss: 0.41 top1acc: 87.80 top5acc: 98.00 \n",
      "Starting epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0, 48.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 24\n",
      "validation set - epoch: 24 loss: 0.39 top1acc: 88.00 top5acc: 98.00 \n",
      "Starting epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.78it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.06it/s]\n",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 25\n",
      "validation set - epoch: 25 loss: 0.38 top1acc: 89.60 top5acc: 98.60 \n",
      "saved model checkpoint at ../pruned/resnet50-epoch=025-val=0.3797.pth\n",
      "Starting epoch 26\n",
      "sparsity per layer [%]: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.11it/s]\n",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 26\n",
      "validation set - epoch: 26 loss: 0.37 top1acc: 89.00 top5acc: 98.40 \n",
      "Starting epoch 27\n",
      "sparsity per layer [%]: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 27\n",
      "validation set - epoch: 27 loss: 0.37 top1acc: 88.80 top5acc: 98.40 \n",
      "Starting epoch 28\n",
      "sparsity per layer [%]: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 28\n",
      "validation set - epoch: 28 loss: 0.38 top1acc: 88.80 top5acc: 98.20 \n",
      "Starting epoch 29\n",
      "sparsity per layer [%]: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 29\n",
      "validation set - epoch: 29 loss: 0.38 top1acc: 89.00 top5acc: 98.40 \n",
      "Starting epoch 30\n",
      "sparsity per layer [%]: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.75it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 30\n",
      "validation set - epoch: 30 loss: 0.38 top1acc: 89.20 top5acc: 98.20 \n",
      "saved model checkpoint at ../pruned/resnet50-epoch=030-val=0.3755.pth\n",
      "Starting epoch 31\n",
      "sparsity per layer [%]: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.78it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.12it/s]\n",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 31\n",
      "validation set - epoch: 31 loss: 0.38 top1acc: 89.00 top5acc: 98.40 \n",
      "Starting epoch 32\n",
      "sparsity per layer [%]: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.76it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.10it/s]\n",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 32\n",
      "validation set - epoch: 32 loss: 0.38 top1acc: 88.60 top5acc: 97.80 \n",
      "Starting epoch 33\n",
      "sparsity per layer [%]: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.75it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.07it/s]\n",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 33\n",
      "validation set - epoch: 33 loss: 0.38 top1acc: 88.80 top5acc: 98.00 \n",
      "Starting epoch 34\n",
      "sparsity per layer [%]: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:23<00:00,  4.77it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 34\n",
      "validation set - epoch: 34 loss: 0.38 top1acc: 88.60 top5acc: 98.20 \n",
      "sparsity per layer [%]: [50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n",
      "Finished training, saving model to ../pruned/resnet50-pruned.pth\n",
      "Saved model\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print('Running baseline test...')\n",
    "epoch = -1\n",
    "_record_kernel_sparsity(analyzed_layers, writer, epoch)\n",
    "_test_datasets(model, None, val_data_loader, writer, epoch=-1)\n",
    "\n",
    "print('Training model')\n",
    "num_epochs = int(math.ceil(modifier_manager.max_epochs))\n",
    "data_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}'.format(epoch))\n",
    "    optimizer.epoch_start()\n",
    "    _record_kernel_sparsity(analyzed_layers, writer, epoch)\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch(model, train_data_loader, optimizer, loss_calc, device, data_counter)\n",
    "    optimizer.epoch_end()\n",
    "    val_losses, train_losses = _test_datasets(model, None, val_data_loader, writer, epoch)\n",
    "\n",
    "    if save_rate > 0 and epoch % save_rate == 0:\n",
    "        save_path = os.path.join(model_dir, 'resnet50-epoch={:03d}-val={:.4f}.pth'\n",
    "                                 .format(epoch, val_losses['loss']))\n",
    "        save_model(save_path, model, optimizer, epoch)\n",
    "        print('saved model checkpoint at {}'.format(save_path))\n",
    "\n",
    "_record_kernel_sparsity(analyzed_layers, writer, num_epochs)\n",
    "\n",
    "\n",
    "scalars_json_path = os.path.join(logs_dir, 'all_scalars.json')\n",
    "writer.export_scalars_to_json(scalars_json_path)\n",
    "writer.close()\n",
    "\n",
    "save_path = os.path.join(model_dir, 'resnet50-pruned.pth')\n",
    "print('Finished training, saving model to {}'.format(save_path))\n",
    "save_model(save_path, model)\n",
    "print('Saved model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modifier_manager.max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
