{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Activation Sparsity Analyzer\n",
    "\n",
    "[ReLUs](https://www.kaggle.com/dansbecker/rectified-linear-units-relu-in-deep-learning) started becoming popular with the [AlexNet model](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf) back in 2012 and added two main advantages: it gave a significant speed improvement while training over [tanh or sigmoid](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0) and it gave a partial solution to the [vanishing gradient problem](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484). All of this comes from the fact that the identity function is applied to any positive values thus making them unbounded while all negative values are set to 0 adding a non-linear component and an inexpensive operation. Since then, ReLUs have appeared in most [major models](https://blog.xrds.acm.org/2016/06/convolutional-neural-networks-cnns-illustrated-explanation/) across the deep learning field. These models, therefore, have a lot of naturally occurring zeros in their computation graph. For example, the output from each ReLU in a randomly initialized network is about 50% zeros or in more common terms 50% sparse.\n",
    "\n",
    "For a trained network, it is empirically found that ReLUs can introduce upwards of 80% sparsity for some of the [layers in most models](https://arxiv.org/abs/1705.01626) and on average more than [50% for most modern architectures](http://www.eecg.toronto.edu/~enright/albericio-isca2016.pdf) (this can vary upwards by quite a bit based on the architecture). The intuitive reasons for this are a few: ReLUs give the training process a way to remove uncorrelated information or noise from the decision paths, sparse representations in high-dimensional spaces are more easily separated by decision boundaries, and neural networks are overall very redundant in their connections. Another empirical finding is that the first layers are less sparse than the final layers. The general intuition is that early layers contain general filters that are applicable to most input/output mappings where as the later layers are much more specialized to a specific output or class.\n",
    "\n",
    "This notebook will walk you through a quick way to visualize the natural sparsity induced from ReLU activations within a selected network for a given dataset. One note is that random data cannot be used to properly view the activations. The degree to which an output from a convolutional layer is positive or negative has a strong prior of the input distribution. With random data, in general, we'll see higher activation sparsity at the beginning of the network as filters fail to find features / correlate with each other and lower activation sparsity at the end of the network as noise compounds / propagates. This shouldn't deter you, though, the activation sparsity is very stable for the full range of train and test input distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print('Python %s on %s' % (sys.version, sys.platform))\n",
    "\n",
    "package_path = os.path.abspath(os.path.join(os.path.expanduser(os.getcwd()), os.pardir))\n",
    "print(package_path)\n",
    "\n",
    "\"\"\"\n",
    "Adding the path to the neuralmagic-pytorch extension to the path so it isn't necessary to have it installed\n",
    "\"\"\"\n",
    "sys.path.extend([package_path])\n",
    "\n",
    "print('Added current package path to sys.path')\n",
    "print('Be sure to install from requirements.txt and pytorch separately')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Below you'll find the code for setting up a model for analyzing activation sparsity. Change out the model for your desired model. Note, it should work with any model coded in pytorch. By default we load a [ResNet50 model](https://arxiv.org/abs/1512.03385) pretrained on the imagenet dataset.\n",
    "\n",
    "Additionally run the code block and select the device to run on before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from neuralmagicML.models import resnet50\n",
    "import ipywidgets as widgets\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "change the desired model to run here\n",
    "\"\"\"\n",
    "model = resnet50(pretrained=True)\n",
    "\n",
    "print('Created model {}'.format(model.__class__.__name__))\n",
    "model = model.eval()\n",
    "\n",
    "print('\\nChoose the device to run on')\n",
    "device_choice = widgets.ToggleButtons(\n",
    "    options=['cuda', 'cpu'] if torch.cuda.is_available() else ['cpu'],\n",
    "    description='Device'\n",
    ")\n",
    "display(device_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzer Selection\n",
    "\n",
    "The below code block will now go through your selected model and offer up a UI for choosing which layers to track. It will execute the model in inference mode once to provide information on how many FLOPS and params are associated with each layer. It may take a few seconds to generate. Each layer in the model will be grouped by type as well: Convs are convolution, BNs are batch normalization, Acts are any activations such as ReLU, Pools are in pooling layers such as max pool, and FCs are any fully connected / linear layers.\n",
    "\n",
    "Input sparsity will track the sparsity within the input tensor for the given layer. Likewise, output sparsity will track the sparsity within the output tensor for the given layer. Input distribution will sample the input tensor (1000 samples per element from the dataset) for histogram visualization later. Likewise, output distribution will sample the output tensor (1000 samples per element from the dataset) for histogram visualization later.\n",
    "\n",
    "In general what we care about is the input sparsity to the convolutions and fully connected layers in our model. This is because we can take advantage of the input sparsity in a convolution or fully connected such that we do not run multiplications for the 0's. This can significantly reduce our compute especially for layers that have upwards of 50% sparsity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from neuralmagicML.notebooks import ASAnalyzerWidgets\n",
    "\n",
    "device = device_choice.value\n",
    "\n",
    "print('running on device {}'.format(device))\n",
    "print('creating activation sparsity analyzer widgets (need to execute the model, so may take some time)...')\n",
    "wid, analyzer_layers = ASAnalyzerWidgets.interactive_module(model, device, inp_dim=(1, 3, 224, 224))\n",
    "display(wid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "\n",
    "Given below is the code for setting up a dataset. It is defaulted to the ImageNet dataset. This must be changed to whatever dataset the model you selected above was trained on. It supports any general supervised dataset.\n",
    "\n",
    "Additionally after selecting the dataset and hitting run, use the sliders to select the number of samples and batch size. The number of samples are the number of individual elements from the dataset that will be run through the model in inference mode for calculating activation sparsity. The batch size is the number of individual elements that will be run through the model at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nEnter the local path where the dataset can be found')\n",
    "\n",
    "dataset_text = widgets.Text(value='', placeholder='Enter local path to dataset', description='Dataset Path')\n",
    "display(dataset_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.datasets import ImageNetDataset, EarlyStopDataset\n",
    "from neuralmagicML.sparsity import ASAnalyzerModule\n",
    "\n",
    "\"\"\"\n",
    "change the desired dataset to run on and the path to it here\n",
    "\"\"\"\n",
    "dataset_root = os.path.abspath(os.path.expanduser(dataset_text.value.strip()))\n",
    "print('Loading dataset from {}'.format(dataset_root))\n",
    "\n",
    "if not os.path.exists(dataset_root):\n",
    "    raise Exception('Folder must exist for dataset at {}'.format(dataset_root))\n",
    "\n",
    "dataset = ImageNetDataset(dataset_root, train=False, download=True)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "print('\\nChoose the number of samples and batch size to run through the model for calculating AS')\n",
    "print('(press enter if/after inputting manually)')\n",
    "sample_size_slider = widgets.IntSlider(\n",
    "    value=100, min=1, max=len(dataset), step=1, description='Num samples:'\n",
    ")\n",
    "display(sample_size_slider)\n",
    "batch_size_slider = widgets.IntSlider(\n",
    "    value=64 if torch.cuda.is_available() else 1, min=1, max=256, step=1, description='Batch Size:'\n",
    ")\n",
    "display(batch_size_slider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Section\n",
    "\n",
    "Below we'll setup the proper variables for use and then run the desired amount of samples from the given dataset through the given model for inference. The given settings for tracking activation statistics will apply. \n",
    "\n",
    "This may take some time depending on your settings, so wait until 'Complete' appears before continuing. A progress bar is given to keep track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import progressbar\n",
    "\n",
    "\n",
    "analyzer_module = ASAnalyzerModule(model, analyzer_layers)\n",
    "print('created analyzer module for {} with {} analyzed layers\\n'\n",
    "      .format(model.__class__.__name__, len([lay for lay in analyzer_layers if lay.enabled])))\n",
    "\n",
    "sample_size = sample_size_slider.value\n",
    "batch_size = batch_size_slider.value\n",
    "\n",
    "print('running model for {} samples from the dataset at batch size {}'\n",
    "      .format(sample_size, batch_size))\n",
    "\n",
    "data_loader = DataLoader(EarlyStopDataset(dataset, sample_size), batch_size=batch_size, num_workers=4)\n",
    "bar = progressbar.ProgressBar(\n",
    "    widgets=['Running samples: ', progressbar.SimpleProgress(),\n",
    "             ' [', progressbar.Percentage(), ']\\t', \n",
    "             '\\t(', progressbar.Timer(), '\\t', progressbar.ETA(), ') '],\n",
    "    maxval=sample_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (*x_feature, y_lab) in enumerate(data_loader):\n",
    "        y_lab = y_lab.to(device)\n",
    "        x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "        y_pred = model(*x_feature)\n",
    "        bar.update(batch * batch_size if batch * batch_size < sample_size else sample_size - 1)\n",
    "        \n",
    "bar.finish()\n",
    "print('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Section\n",
    "\n",
    "Below we visualize the results through multiple possible plots. Each plot will only be shown if there is at least one layer that requested it. If you would like to save any of the data for other use simply print out a json string of the 'data' var after it has been populated. It contains information of the mean and standard deviation of the sparsity.\n",
    "\n",
    "If you selected `input sparsity` for any layers in the UI, a chart will be printed out showing the input sparsity per selected layer. The y axis will be the layer name and the x axis will be the sparsity in percentage: ie 0% is no sparsity and 100% is completely sparse. This is beneficial for any convolutions or fully connected layers.\n",
    "\n",
    "If you selected `output sparsity` for any layers in the UI, a chart will be printed out showing the output sparsity per selected layer. The y axis will be the layer name and the x axis will be the sparsity in percentage: ie 0% is no sparsity and 100% is completely sparse. This is beneficial for any ReLU layers to see how much sparsity they are introducing to the following layer.\n",
    "\n",
    "If you selected `input distribution` for any layers in the UI, a separate chart will be printed out showing the histogram of the input values for each layer. The y axis in these charts will be the count of each bucketed value and the x axis will be the value itself.\n",
    "\n",
    "If you selected `output distribution` for any layers in the UI, a separate chart will be printed out showing the histogram of the output values for each layer. The y axis in these charts will be the count of each bucketed value and the y axis will be the value itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "\n",
    "# plot input sparsities\n",
    "input_sparsity_analyzers = [analyz for analyz in analyzer_layers if analyz.track_inputs_sparsity]\n",
    "\n",
    "if len(input_sparsity_analyzers) > 0:\n",
    "    print('Creating inputs sparsity chart...')\n",
    "    data = {}\n",
    "    \n",
    "    for index, analyz in enumerate(input_sparsity_analyzers):\n",
    "        data[analyz.name] = {'mean': analyz.inputs_sparsity_mean.item(),\n",
    "                             'stddev': analyz.inputs_sparsity_std.item(),\n",
    "                             'ordered_key': '{:04d}  {}'.format(index, analyz.name)}\n",
    "    \n",
    "    height = round(len(data) / 4) + 3\n",
    "    fig = plt.figure(figsize=(10, height))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('Input Sparsity')\n",
    "    ax.set_xlabel('Sparsity')\n",
    "    ax.set_ylabel('Layer')\n",
    "    plt.subplots_adjust(left=0.3, bottom=0.1, right=0.95, top=0.9)\n",
    "    frame = pandas.DataFrame(data={'model': {val['ordered_key']: val['mean'] * 100.0\n",
    "                                             for val in data.values()}})\n",
    "    frame.plot.barh(ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "# plot output sparsities\n",
    "output_sparsity_analyzers = [analyz for analyz in analyzer_layers if analyz.track_outputs_sparsity]\n",
    "\n",
    "if len(output_sparsity_analyzers) > 0:\n",
    "    print('Creating outputs sparsity chart...')\n",
    "    data = {}\n",
    "    \n",
    "    for index, analyz in enumerate(output_sparsity_analyzers):\n",
    "        data[analyz.name] = {'mean': analyz.outputs_sparsity_mean.item(),\n",
    "                             'stddev': analyz.outputs_sparsity_std.item(),\n",
    "                             'ordered_key': '{:04d}  {}'.format(index, analyz.name)}\n",
    "    \n",
    "    height = round(len(data) / 4) + 3\n",
    "    fig = plt.figure(figsize=(10, height))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('Output Sparsity')\n",
    "    ax.set_xlabel('Sparsity')\n",
    "    ax.set_ylabel('Layer')\n",
    "    plt.subplots_adjust(left=0.3, bottom=0.1, right=0.95, top=0.9)\n",
    "    frame = pandas.DataFrame(data={'model': {val['ordered_key']: val['mean'] * 100.0\n",
    "                                             for val in data.values()}})\n",
    "    frame.plot.barh(ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "# plot input distributions\n",
    "input_distribution_analyzers = [analyz for analyz in analyzer_layers if analyz.inputs_sample_size > 0]\n",
    "\n",
    "if len(input_distribution_analyzers) > 0:\n",
    "    print('Creating input distribution charts...')\n",
    "    \n",
    "    for analyz in input_distribution_analyzers:\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        title = '{} Input Distribution'.format(analyz.name)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Count')\n",
    "        frame = pandas.DataFrame(data={title: torch.cat(analyz.inputs_sample).view(-1).tolist()})\n",
    "        frame.hist(column=title, bins=256, ax=ax)\n",
    "        plt.show()\n",
    "\n",
    "# plot output distributions\n",
    "output_distribution_analyzers = [analyz for analyz in analyzer_layers if analyz.outputs_sample_size > 0]\n",
    "\n",
    "if len(output_distribution_analyzers) > 0:\n",
    "    print('Creating output distribution charts...')\n",
    "    \n",
    "    for analyz in output_distribution_analyzers:\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        title = '{} Output Distribution'.format(analyz.name)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Count')\n",
    "        frame = pandas.DataFrame(data={title: torch.cat(analyz.outputs_sample).view(-1).tolist()})\n",
    "        frame.hist(column=title, bins=256, ax=ax)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
