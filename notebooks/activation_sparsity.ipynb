{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Activation Sparsity Analyzer\n",
    "\n",
    "ReLU's started becoming popular with the AlexNet model back in 2012 and added two main advantages: it gave a significant speed improvement while training over tanh or sigmoid and it gave a partial solution to the vanishing gradient problem. All of this comes from the fact that the identity function is applied to any positive values thus making them unbounded while all negative values are set to 0 adding a non-linear component and an inexpensive operation. Since then, ReLU's have appeared in most major models across the deep learning field. These models, therefore, have a lot of naturally occuring zeros in their computation graph. For example, the output from each ReLU in a randomly initialized network is about 50% zeros or in more common terms 50% sparse.\n",
    "\n",
    "For a trained network, it is empirically found that ReLU's can introduce upwards of 80% sparsity for some of the layers in most models and on average more than 60% for most modern architectures (this can vary upwards by quite a bit based on the architecture). The intuitive reasons for this are a few: ReLUs give the training process a way to remove uncorrelated information or noise from the decision paths, sparse representations in high-dimensional spaces are more easily separated by decision boundaries, and neural networks are overall very redundant in their connections. Another empirical finding is that the first layers are less sparse than the final layers. The general intuition is that early layers contain general filters that are applicable to most input/output mappings where as the later layers are much more specialized to a specific output or class.\n",
    "\n",
    "This notebook will walk you through a quick way to visualize the natural sparsity induced from ReLU activations within a selected network for a given dataset. One note is that random data cannot be used to properly view the activations. The degree to which an output from a convolutional layer is positive or negative has a strong prior of the input distribution. With random data, in general, we'll see higher activation sparsity at the beginning of the network as filters fail to find features / correlate with each other and lower activation sparsity at the end of the network as noise compounds / propagates. This shouldn't deter you, though, the activation sparsity is very stable for the full range of train and test input distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print('Python %s on %s' % (sys.version, sys.platform))\n",
    "\n",
    "package_path = os.path.abspath(os.path.join(os.path.expanduser(os.getcwd()), os.pardir))\n",
    "print(package_path)\n",
    "\n",
    "\"\"\"\n",
    "Adding the path to the neuralmagic-pytorch extension to the path so it isn't necessary to have it installed\n",
    "\"\"\"\n",
    "sys.path.extend([package_path])\n",
    "\n",
    "print('Added current package path to sys.path')\n",
    "print('Be sure to install from requirements.txt and pytorch separately')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "Below you'll find the code for setting up a model for analyzing activation sparsity. Change out the model for your desired model. Note, it should work with any model coded in pytorch.\n",
    "\n",
    "Additionally run the code block and select the device to run on before continuing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.models import resnet50\n",
    "import ipywidgets as widgets\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "change the desired model to run here\n",
    "\"\"\"\n",
    "model = resnet50()\n",
    "\n",
    "print('Created model {}'.format(model.__class__.__name__))\n",
    "model = model.eval()\n",
    "\n",
    "print('\\nChoose the device to run on')\n",
    "device_choice = widgets.ToggleButtons(\n",
    "    options=['cuda', 'cpu'] if torch.cuda.is_available() else ['cpu'],\n",
    "    description='Device'\n",
    ")\n",
    "display(device_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzer Selection\n",
    "\n",
    "The below code block will now go through your selected model and offer up a UI for choosing which layers to track. It will execute the model in inference mode once to provide information on how many FLOPS and params for each layer. It may take a few seconds to generate.\n",
    "\n",
    "Input sparsity will track the sparsity within the input tensor for the given layer. Likewise, output sparsity will track the sparsity within the output tensor for the given layer. Input distribution will sample the input tensor (1000 samples per element from the dataset) for histogram visualization later. Likewise, output distribution will sample the output tensor (1000 samples per element from the dataset) for histogram visualization later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from neuralmagicML.notebooks import ASAnalyzerWidgets\n",
    "\n",
    "device = device_choice.value\n",
    "\n",
    "print('running on device {}'.format(device))\n",
    "print('creating activation sparsity analyzer widgets (need to execute the model, so may take some time)...')\n",
    "wid, analyzer_layers = ASAnalyzerWidgets.interactive_module(model, device, inp_dim=(1, 3, 224, 224))\n",
    "display(wid)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Selection\n",
    "\n",
    "Below the code for setting up a dataset is given and defaulted to the ImageNet dataset. This must be changed to whatever dataset the model you selected above was trained on. It supports any general supervised dataset. **The dataset_root variable must be changed to wherever you keep your dataset on the local system.**\n",
    "\n",
    "Additionally after selecting the dataset and hitting run, use the sliders to select the number of samples and batch size. The number of samples are the number of individual elements from the dataset that will be run through the model in inference mode for calculating activation sparsity. The batch size is the number of individual elements that will be run through the model at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nEnter the local path where the dataset can be found')\n",
    "\n",
    "dataset_text = widgets.Text(value='', placeholder='Enter local path to dataset', description='Dataset Path')\n",
    "display(dataset_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neuralmagicML.datasets import ImageNetDataset, EarlyStopDataSet\n",
    "from neuralmagicML.sparsity import ASAnalyzerModule\n",
    "\n",
    "\"\"\"\n",
    "change the desired dataset to run on and the path to it here\n",
    "\"\"\"\n",
    "dataset_root = os.path.abspath(os.path.expanduser(dataset_text.value.strip()))\n",
    "print('Loading dataset from {}'.format(dataset_root))\n",
    "\n",
    "if not os.path.exists(dataset_root):\n",
    "    raise Exception('Folder must exist for dataset at {}'.format(dataset_root))\n",
    "\n",
    "dataset = ImageNetDataset(dataset_root, train=False, download=True)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "print('\\nChoose the number of samples and batch size to run through the model for calculating AS')\n",
    "print('(press enter if/after inputting manually)')\n",
    "sample_size_slider = widgets.IntSlider(\n",
    "    value=100, min=1, max=len(dataset), step=1, description='Num samples:'\n",
    ")\n",
    "display(sample_size_slider)\n",
    "batch_size_slider = widgets.IntSlider(\n",
    "    value=64 if torch.cuda.is_available() else 1, min=1, max=256, step=1, description='Batch Size:'\n",
    ")\n",
    "display(batch_size_slider)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Section\n",
    "\n",
    "Below we'll setup the proper variables for use and then run the desired amount of samples from the given dataset through the given model for inference. The given settings for tracking activation statistics will apply. \n",
    "\n",
    "This may take some time depending on your settings, so wait until 'Complete' appears before continuing. A progress bar is given to keep track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import progressbar\n",
    "\n",
    "\n",
    "analyzer_module = ASAnalyzerModule(model, analyzer_layers)\n",
    "print('created analyzer module for {} with {} analyzed layers\\n'\n",
    "      .format(model.__class__.__name__, len([lay for lay in analyzer_layers if lay.enabled])))\n",
    "\n",
    "sample_size = sample_size_slider.value\n",
    "batch_size = batch_size_slider.value\n",
    "\n",
    "print('running model for {} samples from the dataset at batch size {}'\n",
    "      .format(sample_size, batch_size))\n",
    "\n",
    "data_loader = DataLoader(EarlyStopDataSet(dataset, sample_size), batch_size=batch_size, num_workers=4)\n",
    "bar = progressbar.ProgressBar(\n",
    "    widgets=['Running samples: ', progressbar.SimpleProgress(),\n",
    "             ' [', progressbar.Percentage(), ']\\t', \n",
    "             '\\t(', progressbar.Timer(), '\\t', progressbar.ETA(), ') '],\n",
    "    maxval=sample_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch, (*x_feature, y_lab) in enumerate(data_loader):\n",
    "        y_lab = y_lab.to(device)\n",
    "        x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "        y_pred = model(*x_feature)\n",
    "        bar.update(batch * batch_size if batch * batch_size < sample_size else sample_size - 1)\n",
    "        \n",
    "bar.finish()\n",
    "print('Completed')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Section\n",
    "\n",
    "Below we visualize the results through multiple possible plots. Each plot will only be shown if there is at least one layer that requested it. If you would like to save any of the data for other use simply print out a json string of the 'data' var after it has been populated. It contains information of the mean and standard of deviation of the sparsity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "\n",
    "\n",
    "# plot input sparsities\n",
    "input_sparsity_analyzers = [analyz for analyz in analyzer_layers if analyz.track_inputs_sparsity]\n",
    "\n",
    "if len(input_sparsity_analyzers) > 0:\n",
    "    print('Creating inputs sparsity chart...')\n",
    "    data = {}\n",
    "    \n",
    "    for index, analyz in enumerate(input_sparsity_analyzers):\n",
    "        data[analyz.name] = {'mean': analyz.inputs_sparsity_mean.item(),\n",
    "                             'stddev': analyz.inputs_sparsity_std.item(),\n",
    "                             'ordered_key': '{:04d}  {}'.format(index, analyz.name)}\n",
    "    \n",
    "    height = round(len(data) / 4) + 3\n",
    "    fig = plt.figure(figsize=(10, height))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('Input Sparsity')\n",
    "    ax.set_xlabel('Sparsity')\n",
    "    ax.set_ylabel('Layer')\n",
    "    plt.subplots_adjust(left=0.3, bottom=0.1, right=0.95, top=0.9)\n",
    "    frame = pandas.DataFrame(data={'model': {val['ordered_key']: val['mean'] * 100.0\n",
    "                                             for val in data.values()}})\n",
    "    frame.plot.barh(ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "# plot output sparsities\n",
    "output_sparsity_analyzers = [analyz for analyz in analyzer_layers if analyz.track_outputs_sparsity]\n",
    "\n",
    "if len(output_sparsity_analyzers) > 0:\n",
    "    print('Creating outputs sparsity chart...')\n",
    "    data = {}\n",
    "    \n",
    "    for index, analyz in enumerate(output_sparsity_analyzers):\n",
    "        data[analyz.name] = {'mean': analyz.outputs_sparsity_mean.item(),\n",
    "                             'stddev': analyz.outputs_sparsity_std.item(),\n",
    "                             'ordered_key': '{:04d}  {}'.format(index, analyz.name)}\n",
    "    \n",
    "    height = round(len(plt_data) / 4) + 3\n",
    "    fig = plt.figure(figsize=(10, height))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_title('Output Sparsity')\n",
    "    ax.set_xlabel('Sparsity')\n",
    "    ax.set_ylabel('Layer')\n",
    "    plt.subplots_adjust(left=0.3, bottom=0.1, right=0.95, top=0.9)\n",
    "    frame = pandas.DataFrame(data={'model': {val['ordered_key']: val['mean'] * 100.0\n",
    "                                             for val in data.values()}})\n",
    "    frame.plot.barh(ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "# plot input distributions\n",
    "input_distribution_analyzers = [analyz for analyz in analyzer_layers if analyz.inputs_sample_size > 0]\n",
    "\n",
    "if len(input_distribution_analyzers) > 0:\n",
    "    print('Creating input distribution charts...')\n",
    "    \n",
    "    for analyz in input_distribution_analyzers:\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        title = '{} Input Distribution'.format(analyz.name)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Count')\n",
    "        frame = pandas.DataFrame(data={title: torch.cat(analyz.inputs_sample).view(-1).tolist()})\n",
    "        frame.hist(column=title, bins=256, ax=ax)\n",
    "        plt.show()\n",
    "\n",
    "# plot output distributions\n",
    "output_distribution_analyzers = [analyz for analyz in analyzer_layers if analyz.outputs_sample_size > 0]\n",
    "\n",
    "if len(output_distribution_analyzers) > 0:\n",
    "    print('Creating output distribution charts...')\n",
    "    \n",
    "    for analyz in output_distribution_analyzers:\n",
    "        fig = plt.figure(figsize=(5, 5))\n",
    "        ax = fig.add_subplot(111)\n",
    "        title = '{} Output Distribution'.format(analyz.name)\n",
    "        ax.set_title(title)\n",
    "        ax.set_xlabel('Value')\n",
    "        ax.set_ylabel('Count')\n",
    "        frame = pandas.DataFrame(data={title: torch.cat(analyz.outputs_sample).view(-1).tolist()})\n",
    "        frame.hist(column=title, bins=256, ax=ax)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
