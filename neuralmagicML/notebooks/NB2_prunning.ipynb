{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook #2 - Model prunning\n",
    "This tutorial provides a walk through of model prunning assuming we already have a trained (dense) model:\n",
    "\n",
    "1. prepare and load data\n",
    "2. load model\n",
    "3. sparsity: why/how to prune a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load Data\n",
    "This example uses the same dataset examplified in notebook #1: \n",
    "\\\n",
    "imagenette/imagewoof datasets from fast.ai provided under the Apache License 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already downloaded imagenette of size ImagenetteSize.s160\n",
      "already downloaded imagenette of size ImagenetteSize.s160\n",
      "already downloaded imagenette of size ImagenetteSize.s160\n"
     ]
    }
   ],
   "source": [
    "from neuralmagicML.datasets import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset_type = 'imagenette'\n",
    "dataset_path = '../data/imagenette-160/'\n",
    "device = 'cuda:2'\n",
    " #device to run on: 'cpu' / 'cuda:0'\n",
    "    \n",
    "train_batch_size = 128\n",
    "test_batch_size = 256\n",
    "dataset_early_stop = -1\n",
    "\n",
    "train_dataset = ImagenetteDataset(dataset_path, train=True, rand_trans=True)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "val_dataset = ImagenetteDataset(dataset_path, train=False, rand_trans=False)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=test_batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "train_test_dataset = ImagenetteDataset(dataset_path, train=True, rand_trans=False)\n",
    "train_test_data_loader = DataLoader(train_test_dataset, batch_size=test_batch_size, shuffle=True, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "Let us load the ResNet-50 Model we have trained in the previous notebook example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model...\n"
     ]
    }
   ],
   "source": [
    "from neuralmagicML.models import *\n",
    "from neuralmagicML.sparsity import *\n",
    "from torch.nn import Conv2d\n",
    "\n",
    "model_path = '../checkpoints/resnet50-epoch=030-val=0.3995.pth'\n",
    "print('initializing model...')\n",
    "model = resnet50(num_classes = 10)\n",
    "model = model.to(device)\n",
    "\n",
    "print('loading model...')\n",
    "load_model(model_path, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity, Prunning and high level motivation\n",
    "### sparsity:\n",
    "\n",
    "Informally, sparsity is the degree in which a tensor is comprised of zeros.\n",
    "\n",
    "Slightly more formally:\n",
    "\n",
    "let $N^i$ be the total number of elements in a (e.g. weight) tensor $W_i$\n",
    "\n",
    "let $N^i_z$ be the number of elements which are zero-valued within that tensor\n",
    "\n",
    "The sparsity level associated with that tensor is defined as $s_i \\triangleq \\dfrac{N^i_z}{N^i}$ \n",
    "\n",
    "\n",
    "### prunning:\n",
    "\n",
    "Prunning is the process selectively setting weights in a model to zero. The selection of how many and which weights to set to zero affects the accuracy and model required FLOPs and memory footprint. **Critically - attaining high levels of sparsity while preserving accuracy is possible, as we will demonstrate in this notebook**\n",
    "\n",
    "### Sparsity --> Less FLOPs -?-> accelerated performance:\n",
    "The fact that models can be heavily sparsified with little or no accuracy hit is well known in the research community. Intuitively, the higher the sparsity level the less theoretical FLOPs are required and hence a correspondingly large performance acceleration. However, while the first part of that intutive reasoning is true (higer sparsity --> less theoretical FLOPs), the second one is not nececcerily true (less theoretical FLOPs -/-> higher performance). The reason is that typical hardware such as GPUs is very ill-equiped to take advantage of that sparsity, and FLOPs savings in practice is very hard to come by. On CPUs, in contrast, algorithms for that very exploitation can be flexibly developed.\n",
    "\n",
    "Armed with this insight we are ready (and motivated!) to start looking at model sparsity with the aim of increasing it (via prunning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "print('Setting up model for kernel sparsity tracking...')\n",
    "conv_layers_names = []\n",
    "for name, mod in model.named_modules():\n",
    "    if isinstance(mod, Conv2d): #to add the FC layers: isinstance(mod, Conv2d) or isinstance(mod, Linear) \n",
    "        conv_layers_names.append(name)\n",
    "analyzed_layers = KSAnalyzerLayer.analyze_layers(model, conv_layers_names)\n",
    "\n",
    "def _record_kernel_sparsity(analyzed_layers: List[KSAnalyzerLayer], writer: SummaryWriter, epoch: int):\n",
    "#     layers_sparsities = []\n",
    "    for ks_layer in analyzed_layers:\n",
    "        tag = 'Kernel Sparsity/{}'.format(ks_layer.name)\n",
    "        writer.add_scalar(tag, ks_layer.param_sparsity.item(), epoch)\n",
    "    print('sparsity per layer [%]: '+ str([int(ks_layer.param_sparsity.item()*100.0)} for ks_layer in analyzed_layers]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer , Loss, Logging etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import DataParallel\n",
    "from neuralmagicML.utils import CrossEntropyLossCalc, TopKAccuracy\n",
    "import os\n",
    "\n",
    "init_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "print('Creating optimizer with initial lr: {}, momentum: {}, weight_decay: {}'\n",
    "          .format(init_lr, momentum, weight_decay))\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), init_lr, momentum=momentum, weight_decay=weight_decay, nesterov=True\n",
    ")\n",
    "loss_extras = {\n",
    "    'top1acc': TopKAccuracy(1),\n",
    "    'top5acc': TopKAccuracy(5)\n",
    "}\n",
    "loss_calc = CrossEntropyLossCalc(loss_extras)\n",
    "print('Created loss calc {} with extras {}'.format(loss_calc, ', '.join(loss_extras.keys())))\n",
    "\n",
    "\n",
    "logs_dir = './logs'\n",
    "model_dir = '../pruned'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "    \n",
    "save_rate = 5\n",
    "print('Creating summary writer in {}'.format(logs_dir))\n",
    "writer = SummaryWriter(logdir=logs_dir, comment='imagenet training')\n",
    "if isinstance(model, DataParallel):\n",
    "    model = model.module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling the prunning process:\n",
    "Prunning involves two intertwined processes:\n",
    "1. sparsification - i.e. the selection of weights to zero out.\n",
    "2. re-training the model post sparsification.\n",
    "\n",
    "In practice, a gradual increase of the sparsity level allows for the recovery of accuracy by retraining (up to high levels of sparsity)\n",
    "\n",
    "In order to simplify these control of these two processes, we introduce 'Modifier' classes which manage the schedules  of the associated hyperparameters (i.e. learning_rate, sparsity per layer) thoughout the epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating learning rate schedule...')\n",
    "lr_mod_args = {\n",
    "    'start_epoch': 40.0,\n",
    "    'end_epoch': 100.0,\n",
    "    'update_frequency': 1.0,\n",
    "    'lr_class': 'ExponentialLR',\n",
    "    'lr_kwargs':\n",
    "      {'gamma': 0.95},\n",
    "    'init_lr': 0.001\n",
    "}\n",
    "lr_mod = LearningRateModifier(**lr_mod_args)\n",
    "\n",
    "print('Creating sparsification schedule...')\n",
    "ks_mod_args ={\n",
    "    'start_epoch': 0.0,\n",
    "    'end_epoch': 35.0,\n",
    "    'update_frequency': 1.0,\n",
    "    'param': 'weight',\n",
    "    'init_sparsity': 0.05,\n",
    "    'final_sparsity': 0.8,\n",
    "    'inter_func': 'cubic',\n",
    "    'layers': conv_layers_names\n",
    "}\n",
    "ks_mod = GradualKSModifier(**ks_mod_args)\n",
    "modifiers = [lr_mod, ks_mod]\n",
    "\n",
    "modifier_manager = ScheduledModifierManager(modifiers)\n",
    "optimizer = ScheduledOptimizer(optimizer, model, modifier_manager, steps_per_epoch=len(train_dataset))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training\n",
    "The following should look very familiar - it is in fact the exact same code from our previous tutorial (NB1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Tuple, Dict\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "def _test_datasets(model, train_data_loader: DataLoader, val_data_loader: DataLoader,\n",
    "                   writer: SummaryWriter, epoch: int) -> Tuple[Dict[str, float], Dict[str, float]]:\n",
    "    print('Running test for validation dataset for epoch {}'.format(epoch))\n",
    "    val = test_epoch(model, val_data_loader, loss_calc, device, epoch)\n",
    "    print('Completed test for validation dataset for epoch {}'.format(epoch))\n",
    "\n",
    "    print('Running test for train dataset for epoch {}'.format(epoch))\n",
    "    train = test_epoch(model, train_data_loader, loss_calc, device, epoch)\n",
    "    print('Completed test for train dataset for epoch {}'.format(epoch))\n",
    "\n",
    "    val_losses = {}\n",
    "    train_losses = {}\n",
    "\n",
    "    for loss, _ in val.items():\n",
    "        val_losses[loss] = torch.mean(torch.cat(val[loss])).item()\n",
    "        val_tag = 'Test/validation/{}'.format(loss)\n",
    "        writer.add_scalar(val_tag, val_losses[loss], epoch)\n",
    "        train_losses[loss] = torch.mean(torch.cat(train[loss])).item()\n",
    "        train_tag = 'Test/training/{}'.format(loss)\n",
    "        writer.add_scalar(train_tag, train_losses[loss], epoch)\n",
    "    val_loss_str = 'validation set - epoch: {} '.format(epoch)\n",
    "    for loss, value in val_losses.items():\n",
    "        val_loss_str += (loss + ': {0:.2f} '.format(value))\n",
    "    print(val_loss_str)\n",
    "    train_loss_str = 'training set - epoch: {} '.format(epoch)\n",
    "    for loss, value in train_losses.items():\n",
    "        train_loss_str += (loss + ': {0:.2f} '.format(value))\n",
    "    print(train_loss_str)\n",
    "\n",
    "\n",
    "    return val_losses, train_losses\n",
    "\n",
    "\n",
    "def test_epoch(model: Module, data_loader: DataLoader, loss, device, epoch: int) -> Dict:\n",
    "    model.eval()\n",
    "    results = {}#ModuleTestResults()\n",
    "    with torch.no_grad():\n",
    "        for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "            y_lab = y_lab.to(device)\n",
    "            x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "            batch_size = y_lab.shape[0]\n",
    "            \n",
    "            y_pred = model(*x_feature)\n",
    "\n",
    "            losses = loss(x_feature, y_lab, y_pred)  # type: Dict[str, Tensor]\n",
    "            for key, val in losses.items():\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "\n",
    "                result = val.detach_().cpu()\n",
    "                result = result.repeat(batch_size) #repeat tensor so that there is no dependency on batch size\n",
    "                results[key].append(result)\n",
    "#             results.append(losses, batch_size)\n",
    "    return results\n",
    "\n",
    "def train_epoch(model: Module, data_loader: DataLoader, optimizer, loss, device, data_counter: int):\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "        # copy next batch to the device we are using\n",
    "        y_lab = y_lab.to(device)\n",
    "        x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "        batch_size = y_lab.shape[0]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward \n",
    "        y_pred = model(*x_feature)\n",
    "        \n",
    "        # update losses\n",
    "        losses = loss(x_feature, y_lab, y_pred)  # type: Dict[str, Tensor]\n",
    "        \n",
    "        # backward\n",
    "        losses['loss'].backward()\n",
    "        \n",
    "        # take SGD step\n",
    "        optimizer.step(closure=None)\n",
    "        \n",
    "        # log loss and accuracy\n",
    "        data_counter += batch_size\n",
    "        for _loss, _value in losses.items():\n",
    "            writer.add_scalar('Train/{}'.format(_loss), _value.item(), data_counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prunning main loop:\n",
    "This too is very simialr to the training main loop previously introduced, the main differences are:\n",
    "1. we are tracking sparsity\n",
    "2. we are following the schedules as orchastrated by the modifiers above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "print('Running baseline test...')\n",
    "epoch = -1\n",
    "_record_kernel_sparsity(analyzed_layers, writer, epoch)\n",
    "_test_datasets(model, train_test_data_loader, val_data_loader, writer, epoch=-1)\n",
    "\n",
    "print('Training model')\n",
    "num_epochs = int(math.ceil(modifier_manager.max_epochs))\n",
    "data_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}'.format(epoch))\n",
    "    optimizer.epoch_start()\n",
    "    _record_kernel_sparsity(analyzed_layers, writer, epoch)\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch(model, train_data_loader, optimizer, loss_calc, device, data_counter)\n",
    "    optimizer.epoch_end()\n",
    "    val_losses, train_losses = _test_datasets(model, train_test_data_loader, val_data_loader, writer, epoch)\n",
    "\n",
    "    if save_rate > 0 and epoch % save_rate == 0:\n",
    "        save_path = os.path.join(model_dir, 'resnet50-epoch={:03d}-val={:.4f}.pth'\n",
    "                                 .format(epoch, val_losses['loss']))\n",
    "        save_model(save_path, model, optimizer, epoch)\n",
    "        print('saved model checkpoint at {}'.format(save_path))\n",
    "\n",
    "_record_kernel_sparsity(analyzed_layers, writer, num_epochs)\n",
    "\n",
    "\n",
    "scalars_json_path = os.path.join(logs_dir, 'all_scalars.json')\n",
    "writer.export_scalars_to_json(scalars_json_path)\n",
    "writer.close()\n",
    "\n",
    "save_path = os.path.join(model_dir, 'resnet50-pruned.pth')\n",
    "print('Finished training, saving model to {}'.format(save_path))\n",
    "save_model(save_path, model)\n",
    "print('Saved model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifier_manager.max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
