{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook #2 - Model prunning\n",
    "This tutorial provides a walk through of model prunning assuming we already have a trained (dense) model:\n",
    "\n",
    "1. prepare and load data\n",
    "2. load model\n",
    "3. sparsity: why/how to prune a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load Data\n",
    "This example uses the same dataset examplified in notebook #1: \n",
    "\\\n",
    "imagenette/imagewoof datasets from fast.ai provided under the Apache License 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "already downloaded imagenette of size ImagenetteSize.s160\n",
      "already downloaded imagenette of size ImagenetteSize.s160\n",
      "already downloaded imagenette of size ImagenetteSize.s160\n"
     ]
    }
   ],
   "source": [
    "from neuralmagicML.datasets import *\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "dataset_type = 'imagenette'\n",
    "dataset_path = '../data/imagenette-160/'\n",
    "device = 'cuda:0'\n",
    " #device to run on: 'cpu' / 'cuda:0'\n",
    "    \n",
    "train_batch_size = 128\n",
    "test_batch_size = 256\n",
    "dataset_early_stop = -1\n",
    "\n",
    "train_dataset = ImagenetteDataset(dataset_path, train=True, rand_trans=True)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "val_dataset = ImagenetteDataset(dataset_path, train=False, rand_trans=False)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=test_batch_size, shuffle=True, num_workers=8)\n",
    "\n",
    "train_test_dataset = ImagenetteDataset(dataset_path, train=True, rand_trans=False)\n",
    "train_test_data_loader = DataLoader(train_test_dataset, batch_size=test_batch_size, shuffle=True, num_workers=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model\n",
    "Let us load the ResNet-50 Model we have trained in the previous notebook example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing model...\n",
      "loading model...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from neuralmagicML.models import *\n",
    "from neuralmagicML.sparsity import *\n",
    "from torch.nn import Conv2d\n",
    "\n",
    "model_path = '../checkpoints/resnet50-epoch=030-val=0.3995.pth'\n",
    "print('initializing model...')\n",
    "model = resnet50(num_classes = 10)\n",
    "model = model.to(device)\n",
    "\n",
    "print('loading model...')\n",
    "load_model(model_path, model)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparsity, Prunning and high level motivation\n",
    "### sparsity:\n",
    "\n",
    "Informally, sparsity is the degree in which a tensor is comprised of zeros.\n",
    "\n",
    "Slightly more formally:\n",
    "\n",
    "let $N^i$ be the total number of elements in a (e.g. weight) tensor $W_i$\n",
    "\n",
    "let $N^i_z$ be the number of elements which are zero-valued within that tensor\n",
    "\n",
    "The sparsity level associated with that tensor is defined as $s_i \\triangleq \\dfrac{N^i_z}{N^i}$ \n",
    "\n",
    "\n",
    "### prunning:\n",
    "\n",
    "Prunning is the process selectively setting weights in a model to zero. The selection of how many and which weights to set to zero affects the accuracy and model required FLOPs and memory footprint. **Critically - attaining high levels of sparsity while preserving accuracy is possible, as we will demonstrate in this notebook**\n",
    "\n",
    "### Sparsity --> Less FLOPs -?-> accelerated performance:\n",
    "The fact that models can be heavily sparsified with little or no accuracy hit is well known in the research community. Intuitively, the higher the sparsity level the less theoretical FLOPs are required and hence a correspondingly large performance acceleration. However, while the first part of that intutive reasoning is true (higer sparsity --> less theoretical FLOPs), the second one is not nececcerily true (less theoretical FLOPs -/-> higher performance). The reason is that typical hardware such as GPUs is very ill-equiped to take advantage of that sparsity, and FLOPs savings in practice is very hard to come by. On CPUs, in contrast, algorithms for that very exploitation can be flexibly developed.\n",
    "\n",
    "Armed with this insight we are ready (and motivated!) to start looking at model sparsity with the aim of increasing it (via prunning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up model for kernel sparsity tracking...\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "\n",
    "print('Setting up model for kernel sparsity tracking...')\n",
    "conv_layers_names = []\n",
    "for name, mod in model.named_modules():\n",
    "    if isinstance(mod, Conv2d): #to add the FC layers: isinstance(mod, Conv2d) or isinstance(mod, Linear) \n",
    "        conv_layers_names.append(name)\n",
    "analyzed_layers = KSAnalyzerLayer.analyze_layers(model, conv_layers_names)\n",
    "\n",
    "def _record_kernel_sparsity(analyzed_layers: List[KSAnalyzerLayer], writer: SummaryWriter, epoch: int):\n",
    "#     layers_sparsities = []\n",
    "    for ks_layer in analyzed_layers:\n",
    "        tag = 'Kernel Sparsity/{}'.format(ks_layer.name)\n",
    "        writer.add_scalar(tag, ks_layer.param_sparsity.item(), epoch)\n",
    "    print('sparsity per layer [%]: '+ str([round(ks_layer.param_sparsity.item()*100.0,0) for ks_layer in analyzed_layers]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer , Loss, Logging etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating optimizer with initial lr: 0.01, momentum: 0.9, weight_decay: 0.0001\n",
      "Created loss calc <neuralmagicML.utils.loss_calc.CrossEntropyLossCalc object at 0x7f6f127c4a58> with extras top1acc, top5acc\n",
      "Creating summary writer in ./logs\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import DataParallel\n",
    "from neuralmagicML.utils import CrossEntropyLossCalc, TopKAccuracy\n",
    "import os\n",
    "\n",
    "init_lr = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 1e-4\n",
    "\n",
    "print('Creating optimizer with initial lr: {}, momentum: {}, weight_decay: {}'\n",
    "          .format(init_lr, momentum, weight_decay))\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), init_lr, momentum=momentum, weight_decay=weight_decay, nesterov=True\n",
    ")\n",
    "loss_extras = {\n",
    "    'top1acc': TopKAccuracy(1),\n",
    "    'top5acc': TopKAccuracy(5)\n",
    "}\n",
    "loss_calc = CrossEntropyLossCalc(loss_extras)\n",
    "print('Created loss calc {} with extras {}'.format(loss_calc, ', '.join(loss_extras.keys())))\n",
    "\n",
    "\n",
    "logs_dir = './logs'\n",
    "model_dir = '../pruned'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "    \n",
    "save_rate = 5\n",
    "print('Creating summary writer in {}'.format(logs_dir))\n",
    "writer = SummaryWriter(logdir=logs_dir, comment='imagenet training')\n",
    "if isinstance(model, DataParallel):\n",
    "    model = model.module\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheduling the prunning process:\n",
    "Prunning involves two intertwined processes:\n",
    "1. sparsification - i.e. the selection of weights to zero out.\n",
    "2. re-training the model post sparsification.\n",
    "\n",
    "In practice, a gradual increase of the sparsity level allows for the recovery of accuracy by retraining (up to high levels of sparsity)\n",
    "\n",
    "In order to simplify these control of these two processes, we introduce 'Modifier' classes which manage the schedules  of the associated hyperparameters (i.e. learning_rate, sparsity per layer) thoughout the epochs. For added convinience below is a simple GUI to set these hyperparameters.\n",
    "The GUI allows for controlling the target sparsity on an individual layer basis / global / mixed fashion.\n",
    "Try setting all layers to a sparsity level of 80%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480b30f5c84644ad891204101da8c98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(HBox(children=(Checkbox(value=True, description='enable/disable all'), FloatSlid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "############################################################\n",
    "## configuration of sparsity levels / enables per layer ####\n",
    "############################################################\n",
    "c0 = widgets.VBox([widgets.Checkbox(description=ks_layer.name, value=True) for ks_layer in analyzed_layers])\n",
    "c1 = widgets.VBox([widgets.FloatSlider(value=0.5,min=0.05,max=0.99) for _ in range(len(analyzed_layers))])\n",
    "layer_ctrl = widgets.HBox([c0,c1])\n",
    "global_ctrl = widgets.HBox([widgets.Checkbox(description='enable/disable all', value = True), \n",
    "                            widgets.FloatSlider(value=0.5,min=0.05,max=0.99,description='sparsity [%]')\n",
    "                           ])\n",
    "output2 = widgets.Output()\n",
    "\n",
    "activated_layers = [child.value for child in layer_ctrl.children[0].children]\n",
    "\n",
    "def global_enable_change(change):\n",
    "    with output2:\n",
    "        state = change['new']\n",
    "        print(state)\n",
    "        if state is not None:\n",
    "            for ckbx_child in layer_ctrl.children[0].children:\n",
    "                ckbx_child.value = state\n",
    "                \n",
    "global_ctrl.children[0].observe(global_enable_change, names='value')   \n",
    "\n",
    "def global_sparsity_set(change):\n",
    "    with output2:\n",
    "        val = change['new']\n",
    "        print(val)\n",
    "        if val is not None:\n",
    "            for ckbx_child, sldr_child in zip(layer_ctrl.children[0].children, layer_ctrl.children[1].children):\n",
    "                if ckbx_child.value:\n",
    "                    sldr_child.value = val\n",
    "    \n",
    "global_ctrl.children[1].observe(global_sparsity_set, names='value')   \n",
    "\n",
    "###############################################\n",
    "## configuration of learning rate schedule ####\n",
    "###############################################\n",
    "\n",
    "lr_class_dict = {   #TODO: read from CONSTRUCTORS in modifier_lr.py instead\n",
    "                    #to include all supported methods in the GUI\n",
    "    'ExponentialLR': {'gamma': [0.95, widgets.BoundedFloatText]}, #bound by 0.0\n",
    "    'StepLR': {'step_size': [20, widgets.BoundedIntText], #bound by 1\n",
    "              'gamma': [0.2, widgets.BoundedFloatText]}\n",
    "}\n",
    "\n",
    "lr_mod_args_field_initval = {\n",
    "    'start_epoch': 25.0,# 'start epoch:'],\n",
    "    'end_epoch': 35.0,# 'end epoch  :'],\n",
    "    'update_frequency': 1.0,# 'update freq:'],\n",
    "    'init_lr': 0.001# 'initial learning rate :']\n",
    "}\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "# lr_cfg_list = [widgets.Text(value='learning rate schedule', disabled=True)]\n",
    "lr_section_title = widgets.Text(value='learning rate schedule', disabled=True)\n",
    "lr_cfg_list =[]\n",
    "for fld, val in lr_mod_args_field_initval.items():\n",
    "    lr_cfg_list.append(widgets.BoundedFloatText(value=val, description=fld, disabled=False, min=0, style=style,))\n",
    "\n",
    "lr_slct = widgets.Dropdown(\n",
    "    options=[key for key in lr_class_dict.keys()],  \n",
    "    value=[key for key in lr_class_dict.keys()][0],\n",
    "    description='lr_class',\n",
    ")\n",
    "# lr_cfg_list.append(lr_slct)\n",
    "\n",
    "def create_lr_slct_list():\n",
    "    lr_slct_params = [] #create new widgets\n",
    "    for param, val in lr_class_dict[lr_slct.value].items():\n",
    "        lr_slct_params.append(val[1](value=val[0],description=param))\n",
    "    return lr_slct_params\n",
    "slct_param = widgets.VBox(children=create_lr_slct_list())\n",
    "# lr_cfg_list.append(slct_param)\n",
    "\n",
    "def refresh_lr_param(change):\n",
    "    if change['new']:\n",
    "        val = lr_slct.value\n",
    "        slct_param.children = create_lr_slct_list()\n",
    "\n",
    "lr_slct.observe(refresh_lr_param, names='value')   \n",
    "lr_cfg = widgets.VBox([lr_section_title, *lr_cfg_list, lr_slct, slct_param])\n",
    "\n",
    "##########################################\n",
    "## configuration of prunning schedule ####\n",
    "##########################################\n",
    "\n",
    "prunning_mod_args_field_initval = {\n",
    "    'start_epoch': 0.0,# 'start epoch:'],\n",
    "    'end_epoch': 25.0,#'end epoch  :'],\n",
    "    'update_frequency': 1.0#,'update freq:'],\n",
    "\n",
    "\n",
    "}\n",
    "\n",
    "style = {'description_width': 'initial'}\n",
    "prn_section_title = widgets.Text(value='prunning schedule', disabled=True)\n",
    "prn_cfg_list =[]\n",
    "for fld, val in prunning_mod_args_field_initval.items():\n",
    "    prn_cfg_list.append(widgets.BoundedFloatText(value=val, description=fld, disabled=False, min=0, style=style,))\n",
    "\n",
    "\n",
    "prn_cfg = widgets.VBox([prn_section_title,*prn_cfg_list])\n",
    "schd_cfg = widgets.VBox([lr_cfg, prn_cfg])#,prn_cfg_list])\n",
    "display(widgets.HBox([widgets.VBox([global_ctrl,layer_ctrl]),schd_cfg]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating learning rate schedule...\n",
      "Creating sparsification schedule...\n"
     ]
    }
   ],
   "source": [
    "print('Creating learning rate schedule...')\n",
    "lr_mod_args = {}\n",
    "\n",
    "for child in lr_cfg_list: \n",
    "    lr_mod_args[child.description] = child.value\n",
    "assert(lr_slct.description == 'lr_class')\n",
    "lr_mod_args['lr_class'] = lr_slct.value\n",
    "lr_mod_args['lr_kwargs'] = {}\n",
    "for child in slct_param.children:\n",
    "    lr_mod_args['lr_kwargs'][child.description] = child.value\n",
    "\n",
    "lr_mod = LearningRateModifier(**lr_mod_args)\n",
    "\n",
    "print('Creating sparsification schedule...')\n",
    "\n",
    "def create_ks_mod_args(layer_name, final_sparsity):\n",
    "    ks_mod_args ={\n",
    "        'param': 'weight',\n",
    "        'init_sparsity': 0.05,\n",
    "        'inter_func': 'linear',\n",
    "        'layers': [layer_name],\n",
    "        'final_sparsity': final_sparsity\n",
    "    }\n",
    "    # add common fields\n",
    "    for child in prn_cfg_list:\n",
    "        ks_mod_args[child.description] = child.value\n",
    "    return ks_mod_args\n",
    "\n",
    "ks_mod_args_list = []\n",
    "for ckbx_child, sldr_child in zip(layer_ctrl.children[0].children, layer_ctrl.children[1].children):\n",
    "        if ckbx_child.value: #layer is sparsified\n",
    "            layer_name = ckbx_child.description\n",
    "            final_sparsity = sldr_child.value#\n",
    "            ks_mod_args_list.append(create_ks_mod_args(layer_name, final_sparsity))\n",
    "            \n",
    "ks_mod_list = [GradualKSModifier(**ks_mod_args) for ks_mod_args in ks_mod_args_list]\n",
    "modifiers = [lr_mod, *ks_mod_list]\n",
    "\n",
    "modifier_manager = ScheduledModifierManager(modifiers)\n",
    "optimizer = ScheduledOptimizer(optimizer, model, modifier_manager, steps_per_epoch=len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['input.conv'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.0.0.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.0.0.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.0.0.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.0.0.identity.conv'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.0.1.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.0.1.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.0.1.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.0.2.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.0.2.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.0.2.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.0.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.0.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.0.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.0.identity.conv'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.1.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.1.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.1.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.2.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.2.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.2.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.3.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.3.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.1.3.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.0.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.0.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.0.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.0.identity.conv'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.1.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.1.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.1.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.2.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.2.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.2.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.3.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.3.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.3.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.4.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.4.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.4.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.5.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.5.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.2.5.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.3.0.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.3.0.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.3.0.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.3.0.identity.conv'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.3.1.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.3.1.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.3.1.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.3.2.conv1'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.3.2.conv2'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0},\n",
       " {'param': 'weight',\n",
       "  'init_sparsity': 0.05,\n",
       "  'inter_func': 'linear',\n",
       "  'layers': ['sections.3.2.conv3'],\n",
       "  'final_sparsity': 0.5,\n",
       "  'start_epoch': 0.0,\n",
       "  'end_epoch': 25.0,\n",
       "  'update_frequency': 1.0}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ks_mod_args_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up training\n",
    "The following should look very familiar - it is in fact the exact same code from our previous tutorial (NB1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from typing import Tuple, Dict\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torch.nn import Module\n",
    "\n",
    "\n",
    "def _test_datasets(model, train_data_loader: DataLoader, val_data_loader: DataLoader,\n",
    "                   writer: SummaryWriter, epoch: int) -> Tuple[Dict[str, float], Dict[str, float]]:\n",
    "    val_losses , train_losses = None, None\n",
    "    if val_data_loader  is not None:\n",
    "        print('Running test for validation dataset for epoch {}'.format(epoch))\n",
    "        val = test_epoch(model, val_data_loader, loss_calc, device, epoch)\n",
    "        print('Completed test for validation dataset for epoch {}'.format(epoch))\n",
    "        val_losses = {}\n",
    "        for loss, _ in val.items():\n",
    "            val_losses[loss] = torch.mean(torch.cat(val[loss])).item()\n",
    "            val_tag = 'Test/validation/{}'.format(loss)\n",
    "            writer.add_scalar(val_tag, val_losses[loss], epoch)\n",
    "        \n",
    "        val_loss_str = 'validation set - epoch: {} '.format(epoch)\n",
    "        for loss, value in val_losses.items():\n",
    "            val_loss_str += (loss + ': {0:.2f} '.format(value))\n",
    "        print(val_loss_str)\n",
    "        \n",
    "        \n",
    "    if train_data_loader is not None:\n",
    "        print('Running test for train dataset for epoch {}'.format(epoch))\n",
    "        train = test_epoch(model, train_data_loader, loss_calc, device, epoch)\n",
    "        print('Completed test for train dataset for epoch {}'.format(epoch))\n",
    "        train_losses = {}\n",
    "\n",
    "        for loss, _ in train.items():\n",
    "            train_losses[loss] = torch.mean(torch.cat(train[loss])).item()\n",
    "            train_tag = 'Test/training/{}'.format(loss)\n",
    "            writer.add_scalar(train_tag, train_losses[loss], epoch)\n",
    "\n",
    "        \n",
    "        train_loss_str = 'training set - epoch: {} '.format(epoch)\n",
    "        for loss, value in train_losses.items():\n",
    "            train_loss_str += (loss + ': {0:.2f} '.format(value))\n",
    "        print(train_loss_str)\n",
    "\n",
    "\n",
    "    return val_losses , train_losses\n",
    "\n",
    "\n",
    "def test_epoch(model: Module, data_loader: DataLoader, loss, device, epoch: int) -> Dict:\n",
    "    model.eval()\n",
    "    results = {}#ModuleTestResults()\n",
    "    with torch.no_grad():\n",
    "        for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "            y_lab = y_lab.to(device)\n",
    "            x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "            batch_size = y_lab.shape[0]\n",
    "            \n",
    "            y_pred = model(*x_feature)\n",
    "\n",
    "            losses = loss(x_feature, y_lab, y_pred)  # type: Dict[str, Tensor]\n",
    "            for key, val in losses.items():\n",
    "                if key not in results:\n",
    "                    results[key] = []\n",
    "\n",
    "                result = val.detach_().cpu()\n",
    "                result = result.repeat(batch_size) #repeat tensor so that there is no dependency on batch size\n",
    "                results[key].append(result)\n",
    "#             results.append(losses, batch_size)\n",
    "    return results\n",
    "\n",
    "def train_epoch(model: Module, data_loader: DataLoader, optimizer, loss, device, data_counter: int):\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (*x_feature, y_lab) in enumerate(tqdm(data_loader)):\n",
    "        # copy next batch to the device we are using\n",
    "        y_lab = y_lab.to(device)\n",
    "        x_feature = tuple([dat.to(device) for dat in x_feature])\n",
    "        batch_size = y_lab.shape[0]\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward \n",
    "        y_pred = model(*x_feature)\n",
    "        \n",
    "        # update losses\n",
    "        losses = loss(x_feature, y_lab, y_pred)  # type: Dict[str, Tensor]\n",
    "        \n",
    "        # backward\n",
    "        losses['loss'].backward()\n",
    "        \n",
    "        # take SGD step\n",
    "        optimizer.step(closure=None)\n",
    "        \n",
    "        # log loss and accuracy\n",
    "        data_counter += batch_size\n",
    "        for _loss, _value in losses.items():\n",
    "            writer.add_scalar('Train/{}'.format(_loss), _value.item(), data_counter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prunning main loop:\n",
    "This too is very simialr to the training main loop previously introduced, the main differences are:\n",
    "1. we are tracking sparsity\n",
    "2. we are following the schedules as orchastrated by the modifiers above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "Running test for validation dataset for epoch -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch -1\n",
      "validation set - epoch: -1 loss: 0.40 top1acc: 87.20 top5acc: 98.60 \n",
      "Training model\n",
      "Starting epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0, 5.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [02:09<00:00,  1.63it/s]\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for validation dataset for epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed test for validation dataset for epoch 0\n",
      "validation set - epoch: 0 loss: 0.37 top1acc: 88.80 top5acc: 98.80 \n",
      "saved model checkpoint at ../pruned/resnet50-epoch=000-val=0.3695.pth\n",
      "Starting epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/101 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity per layer [%]: [7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0, 7.0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/101 [00:08<01:01,  1.44it/s]"
     ]
    }
   ],
   "source": [
    "import math\n",
    "print('Running baseline test...')\n",
    "epoch = -1\n",
    "_record_kernel_sparsity(analyzed_layers, writer, epoch)\n",
    "_test_datasets(model, None, val_data_loader, writer, epoch=-1)\n",
    "\n",
    "print('Training model')\n",
    "num_epochs = int(math.ceil(modifier_manager.max_epochs))\n",
    "data_counter = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Starting epoch {}'.format(epoch))\n",
    "    optimizer.epoch_start()\n",
    "    _record_kernel_sparsity(analyzed_layers, writer, epoch)\n",
    "\n",
    "\n",
    "\n",
    "    train_epoch(model, train_data_loader, optimizer, loss_calc, device, data_counter)\n",
    "    optimizer.epoch_end()\n",
    "    val_losses, train_losses = _test_datasets(model, None, val_data_loader, writer, epoch)\n",
    "\n",
    "    if save_rate > 0 and epoch % save_rate == 0:\n",
    "        save_path = os.path.join(model_dir, 'resnet50-epoch={:03d}-val={:.4f}.pth'\n",
    "                                 .format(epoch, val_losses['loss']))\n",
    "        save_model(save_path, model, optimizer, epoch)\n",
    "        print('saved model checkpoint at {}'.format(save_path))\n",
    "\n",
    "_record_kernel_sparsity(analyzed_layers, writer, num_epochs)\n",
    "\n",
    "\n",
    "scalars_json_path = os.path.join(logs_dir, 'all_scalars.json')\n",
    "writer.export_scalars_to_json(scalars_json_path)\n",
    "writer.close()\n",
    "\n",
    "save_path = os.path.join(model_dir, 'resnet50-pruned.pth')\n",
    "print('Finished training, saving model to {}'.format(save_path))\n",
    "save_model(save_path, model)\n",
    "print('Saved model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modifier_manager.max_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
